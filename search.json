[{"title":"IntelliJ IDEA Maven编译错误：Fatal error compiling: 无效的目标发行版: 1.8","url":"/2018/02/IntelliJ-IDEA-Maven%E7%BC%96%E8%AF%91%E9%94%99%E8%AF%AF%EF%BC%9AFatal-error-compiling-%E6%97%A0%E6%95%88%E7%9A%84%E7%9B%AE%E6%A0%87%E5%8F%91%E8%A1%8C%E7%89%88-1-8/","content":"使用IntelliJ IDEA编译工程时报错，信息如下：\n\n[ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:3.1:compile (default-compile) on project [project name]: Fatal error compiling: 无效的目标发行版: 1.8 -&gt; [Help 1]\n\nFile -&gt; Other Settings -&gt; Default Settings | Default Project Structure\n\n\n\n","categories":["Tools","IntelliJ IDEA"]},{"title":"如何使用MySQL-Proxy使MySQL实例可以通过外网访问？","url":"/2018/02/%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8MySQL-Proxy%E4%BD%BFMySQL%E5%AE%9E%E4%BE%8B%E5%8F%AF%E4%BB%A5%E9%80%9A%E8%BF%87%E5%A4%96%E7%BD%91%E8%AE%BF%E9%97%AE%EF%BC%9F/","content":"产品在demo阶段，我们往往把各种服务都装在一台主机上面，如web服务，数据库服务等。但在生产环境上运行应用时，我们还是得把web服务和数据库服务分开。我们在使用Ucloud或阿里云提供的云数据库时，往往是不能直接在外网通过数据库管理工具（如：MysqlWorkBench)来管理数据库的，因为云数据库服务只提供给我们一个内网的ip。\n鉴于MySQL实例不能通过外网IP直接访问，可使用MySQL-Proxy将MySQL实例跳转至云主机（UHost）的端口进行访问。今天我们先不谈MySQL-Proxy的读写分离的问题。\n在云主机（UHost）安装MySQL-Proxy：\nyum install mysql-proxy\n\n也可以从官网下载解压后直接运行。地址：http://dev.mysql.com/downloads/mysql-proxy/\n安装结束后，可通过如下命令查看相关信息：\nmysql-proxy -V\n\n查看MySQL-Proxy帮助选项：\nmysql-proxy -help -all\n\nMySQL-Proxy默认端口为4040，通过访问4040端口就可以访问3306端口。\n使用命令行开启MySQL-Proxy，步骤如下：\ntouch /etc/mysql-proxy.cnfvim /etc/mysql-proxy.cnf\n输入如下内容：\n[mysql-proxy]admin-username=root          # admin用户名(可以不填）admin-password=password      # admin密码（可以不填）admin-lua-script=/usr/lib64/mysql-proxy/lua/admin.lua    #lua位置，参见上面的版本信息（可以不填）daemon=true        # mysql-proxy以守护进程方式运行（这项填了以后会报错，目前没找到原因，可以在运行时通过nohup在后台运行）keepalive=true        # 保持连接启动进程会有2个， 一号进程用来监视二号进程proxy-address=:4044   # 监听端口，默认为4040proxy-backend-addresses=10.6.X.XX  # 目标地址，udb内网地址，默认端口3306log-file=/var/log/mysql-proxy.loglog-level=debug\n\n配置文件保存后需要改变权限：\nchmod 0660 /etc/mysql-proxy.cnf\n\n启动：\nmysql-proxy --defaults-file=/etc/mysql-proxy.cnf\n\n使用kill命令可以将程序终止。\n在外网环境测试：\nmysql -h$uhost_ip -P4040 -u$User -p$Password\n\n$uhost_ip为UHost的外网IP。\n注意：\n需要在UCloud管理控制台中，打开云主机（UHost）的4040端口。\n","categories":["Database"]},{"title":"工厂模式","url":"/2018/02/%E5%B7%A5%E5%8E%82%E6%A8%A1%E5%BC%8F/","content":"工厂模式（Factory Pattern）是 Java 中最常用的设计模式之一。这种类型的设计模式属于创建型模式，它提供了一种创建对象的最佳方式。\n在工厂模式中，我们在创建对象时不会对客户端暴露创建逻辑，并且是通过使用一个共同的接口来指向新创建的对象。\n介绍意图：定义一个创建对象的接口，让其子类自己决定实例化哪一个工厂类，工厂模式使其创建过程延迟到子类进行。\n主要解决：主要解决接口选择的问题。\n如何解决：让其子类实现工厂接口，返回的也是一个抽象的产品。\n关键代码：创建过程在其子类执行。\n应用实例：\n\n您需要一辆汽车，可以直接从工厂里面提货，而不用去管这辆汽车是怎么做出来的，以及这个汽车里面的具体实现。\nHibernate 换数据库只需换方言和驱动就可以。\n\n优点：\n\n一个调用者想创建一个对象，只要知道其名称就可以了。\n扩展性高，如果想增加一个产品，只要扩展一个工厂类就可以。\n屏蔽产品的具体实现，调用者只关心产品的接口。\n\n缺点：每次增加一个产品时，都需要增加一个具体类和对象实现工厂，使得系统中类的个数成倍增加，在一定程度上增加了系统的复杂度，同时也增加了系统具体类的依赖。这并不是什么好事。\n使用场景：\n\n日志记录器：记录可能记录到本地硬盘、系统事件、远程服务器等，用户可以选择记录日志到什么地方。\n数据库访问，当用户不知道最后系统采用哪一类数据库，以及数据库可能有变化时。\n设计一个连接服务器的框架，需要三个协议，”POP3”、”IMAP”、”HTTP”，可以把这三个作为产品类，共同实现一个接口。\n\n注意事项：作为一种创建类模式，在任何需要生成复杂对象的地方，都可以使用工厂方法模式。有一点需要注意的地方就是复杂对象适合使用工厂模式，而简单对象，特别是只需要通过 new 就可以完成创建的对象，无需使用工厂模式。如果使用工厂模式，就需要引入一个工厂类，会增加系统的复杂度。\n实现我们将创建一个 Shape 接口和实现 Shape 接口的实体类。下一步是定义工厂类 ShapeFactory。FactoryPatternDemo，我们的演示类使用 ShapeFactory 来获取 Shape 对象。它将向 ShapeFactory 传递信息（CIRCLE &#x2F; RECTANGLE &#x2F; SQUARE），以便获取它所需对象的类型。\n\n\n步骤 1创建一个接口。\nShape.java\npublic interface Shape &#123;   void draw();&#125; \n\n步骤 2创建实现接口的实体类。\nRectangle.java\npublic class Rectangle implements Shape &#123;   @Override   public void draw() &#123;      System.out.println(&quot;Inside Rectangle::draw() method.&quot;);   &#125;&#125;\n\nSquare.java\npublic class Square implements Shape &#123;   @Override   public void draw() &#123;      System.out.println(&quot;Inside Square::draw() method.&quot;);   &#125;&#125;\n\nCircle.java\npublic class Circle implements Shape &#123;   @Override   public void draw() &#123;      System.out.println(&quot;Inside Circle::draw() method.&quot;);   &#125;&#125;\n\n步骤 3创建一个工厂，生成基于给定信息的实体类的对象。\nShapeFactory.java\npublic class ShapeFactory &#123;       //使用 getShape 方法获取形状类型的对象   public Shape getShape(String shapeType)&#123;      if(shapeType == null)&#123;         return null;      &#125;              if(shapeType.equalsIgnoreCase(&quot;CIRCLE&quot;))&#123;         return new Circle();      &#125; else if(shapeType.equalsIgnoreCase(&quot;RECTANGLE&quot;))&#123;         return new Rectangle();      &#125; else if(shapeType.equalsIgnoreCase(&quot;SQUARE&quot;))&#123;         return new Square();      &#125;      return null;   &#125;&#125;\n\n步骤 4使用该工厂，通过传递类型信息来获取实体类的对象。\nFactoryPatternDemo.java\npublic class FactoryPatternDemo &#123;   public static void main(String[] args) &#123;      ShapeFactory shapeFactory = new ShapeFactory();      //获取 Circle 的对象，并调用它的 draw 方法      Shape shape1 = shapeFactory.getShape(&quot;CIRCLE&quot;);      //调用 Circle 的 draw 方法      shape1.draw();      //获取 Rectangle 的对象，并调用它的 draw 方法      Shape shape2 = shapeFactory.getShape(&quot;RECTANGLE&quot;);      //调用 Rectangle 的 draw 方法      shape2.draw();      //获取 Square 的对象，并调用它的 draw 方法      Shape shape3 = shapeFactory.getShape(&quot;SQUARE&quot;);      //调用 Square 的 draw 方法      shape3.draw();   &#125;&#125;\n\n步骤 5验证输出。\nInside Circle::draw() method.Inside Rectangle::draw() method.Inside Square::draw() method.\n\n其他改进每增加一种对象（Shape)时，都需要修改ShapeFactory.getShape()的逻辑（修改if…else逻辑），如果使用反射机制，则可不用修改ShapeFactory.java代码，可以解决上述问题。\npublic class ShapeFactory &#123;    public static Object getClass(Class&lt;?extends Shape&gt; clazz) &#123;        Object obj = null;        try &#123;            obj = Class.forName(clazz.getName()).newInstance();        &#125; catch (ClassNotFoundException e) &#123;            e.printStackTrace();        &#125; catch (InstantiationException e) &#123;            e.printStackTrace();        &#125; catch (IllegalAccessException e) &#123;            e.printStackTrace();        &#125;        return obj;    &#125;&#125;\n调用\nObject object = ShapeFactory.getClass(Circle.class);((Shape) object).draw();\n\n调用时需要类型强制转换，所以再次改进，如下：\npublic class ShapeFactory &#123;    public static &lt;T&gt; T getClass(Class&lt;? extends T&gt; clazz) &#123;        T obj = null;        try &#123;            obj = (T) Class.forName(clazz.getName()).newInstance();        &#125; catch (ClassNotFoundException e) &#123;            e.printStackTrace();        &#125; catch (InstantiationException e) &#123;            e.printStackTrace();        &#125; catch (IllegalAccessException e) &#123;            e.printStackTrace();        &#125;        return obj;    &#125;&#125;\n\n省略类型强制转换，支持多态\nRectangle rect = ShapeFactory.getClass(Rectangle.class);rect.draw();Shape square = ShapeFactory.getClass(Square.class);square.draw();","categories":["设计模式"]},{"title":"抽象工厂模式","url":"/2018/02/%E6%8A%BD%E8%B1%A1%E5%B7%A5%E5%8E%82%E6%A8%A1%E5%BC%8F/","content":"抽象工厂模式（Abstract Factory Pattern）是围绕一个超级工厂创建其他工厂。该超级工厂又称为其他工厂的工厂。这种类型的设计模式属于创建型模式，它提供了一种创建对象的最佳方式。\n在抽象工厂模式中，接口是负责创建一个相关对象的工厂，不需要显式指定它们的类。每个生成的工厂都能按照工厂模式提供对象。\n介绍意图：提供一个创建一系列相关或相互依赖对象(如Shape和Color)的接口，而无需指定它们具体的类。主要解决：主要解决接口选择的问题。何时使用：系统的产品有多于一个的产品族，而系统只消费其中某一族的产品。如何解决：在一个产品族里面，定义多个产品。关键代码：在一个工厂里聚合多个同类产品。应用实例：工作了，为了参加一些聚会，肯定有两套或多套衣服吧，比如说有商务装（成套，一系列具体产品）、时尚装（成套，一系列具体产品），甚至对于一个家庭来说，可能有商务女装、商务男装、时尚女装、时尚男装，这些也都是成套的，即一系列具体产品。假设一种情况（现实中是不存在的，但有利于说明抽象工厂模式），在您的家中，某一个衣柜（具体工厂）只能存放某一种这样的衣服（成套，一系列具体产品），每次拿这种成套的衣服时也自然要从这个衣柜中取出了。用 OO 的思想去理解，所有的衣柜（具体工厂）都是衣柜类（抽象工厂）的某一个，而每一套成套的衣服（如商务装、时尚装）又包括具体的上衣（某一具体产品），裤子（某一具体产品），这些具体的上衣其实也都是上衣（抽象产品），具体的裤子也都是裤子（另一个抽象产品）。优点：当一个产品族中的多个对象被设计成一起工作时，它能保证客户端始终只使用同一个产品族中的对象。缺点：产品族扩展非常困难，要增加一个系列的某一产品，既要在抽象的 Creator 里加代码，又要在具体的里面加代码。使用场景： 1、QQ 换皮肤，一整套一起换。 2、生成不同操作系统的程序。注意事项：产品族难扩展，产品等级易扩展。\n实现我们将创建 Shape 和 Color 接口和实现这些接口的实体类。下一步是创建抽象工厂类 AbstractFactory。接着定义工厂类 ShapeFactory 和 ColorFactory，这两个工厂类都是扩展了 AbstractFactory。然后创建一个工厂创造器&#x2F;生成器类 FactoryProducer。\nAbstractFactoryPatternDemo，我们的演示类使用 FactoryProducer 来获取 AbstractFactory 对象。它将向 AbstractFactory 传递形状信息 Shape（CIRCLE &#x2F; RECTANGLE &#x2F; SQUARE），以便获取它所需对象的类型。同时它还向 AbstractFactory 传递颜色信息 Color（RED &#x2F; GREEN &#x2F; BLUE），以便获取它所需对象的类型。\n\n\n步骤 1为形状创建一个接口。\nShape.java\npublic interface Shape &#123;   void draw();&#125;\n\n步骤 2创建实现接口的实体类。\nRectangle.java\npublic class Rectangle implements Shape &#123;   @Override   public void draw() &#123;      System.out.println(&quot;Inside Rectangle::draw() method.&quot;);   &#125;&#125;\n\nSquare.java\npublic class Square implements Shape &#123;   @Override   public void draw() &#123;      System.out.println(&quot;Inside Square::draw() method.&quot;);   &#125;&#125;\n\nCircle.java\npublic class Circle implements Shape &#123;   @Override   public void draw() &#123;      System.out.println(&quot;Inside Circle::draw() method.&quot;);   &#125;&#125;\n\n步骤 3为颜色创建一个接口。\nColor.java\npublic interface Color &#123;   void fill();&#125;\n\n步骤4创建实现接口的实体类。\nRed.java\npublic class Red implements Color &#123;   @Override   public void fill() &#123;      System.out.println(&quot;Inside Red::fill() method.&quot;);   &#125;&#125;\n\nGreen.java\npublic class Green implements Color &#123;   @Override   public void fill() &#123;      System.out.println(&quot;Inside Green::fill() method.&quot;);   &#125;&#125;\n\nBlue.java\npublic class Blue implements Color &#123;   @Override   public void fill() &#123;      System.out.println(&quot;Inside Blue::fill() method.&quot;);   &#125;&#125;\n\n步骤 5为 Color 和 Shape 对象创建抽象类来获取工厂。\nAbstractFactory.java\npublic abstract class AbstractFactory &#123;   abstract Color getColor(String color);   abstract Shape getShape(String shape);&#125;\n\n步骤 6创建扩展了 AbstractFactory 的工厂类，基于给定的信息生成实体类的对象。\nShapeFactory.java\npublic class ShapeFactory extends AbstractFactory &#123;       @Override   public Shape getShape(String shapeType)&#123;      if(shapeType == null)&#123;         return null;      &#125;              if(shapeType.equalsIgnoreCase(&quot;CIRCLE&quot;))&#123;         return new Circle();      &#125; else if(shapeType.equalsIgnoreCase(&quot;RECTANGLE&quot;))&#123;         return new Rectangle();      &#125; else if(shapeType.equalsIgnoreCase(&quot;SQUARE&quot;))&#123;         return new Square();      &#125;      return null;   &#125;      @Override   Color getColor(String color) &#123;      return null;   &#125;&#125;\n\nColorFactory.java\npublic class ColorFactory extends AbstractFactory &#123;       @Override   public Shape getShape(String shapeType)&#123;      return null;   &#125;      @Override   Color getColor(String color) &#123;      if(color == null)&#123;         return null;      &#125;              if(color.equalsIgnoreCase(&quot;RED&quot;))&#123;         return new Red();      &#125; else if(color.equalsIgnoreCase(&quot;GREEN&quot;))&#123;         return new Green();      &#125; else if(color.equalsIgnoreCase(&quot;BLUE&quot;))&#123;         return new Blue();      &#125;      return null;   &#125;&#125;\n\n步骤 7创建一个工厂创造器&#x2F;生成器类，通过传递形状或颜色信息来获取工厂。\nFactoryProducer.java\npublic class FactoryProducer &#123;   public static AbstractFactory getFactory(String choice)&#123;      if(choice.equalsIgnoreCase(&quot;SHAPE&quot;))&#123;         return new ShapeFactory();      &#125; else if(choice.equalsIgnoreCase(&quot;COLOR&quot;))&#123;         return new ColorFactory();      &#125;      return null;   &#125;&#125;\n\n步骤 8客户端将使用 FactoryProducer的静态方法getFactory(String choice) 来获取 AbstractFactory，通过传递类型信息来获取实体类的对象。\nAbstractFactoryPatternDemo.java\npublic class AbstractFactoryPatternDemo &#123;   public static void main(String[] args) &#123;      //获取形状工厂      AbstractFactory shapeFactory = FactoryProducer.getFactory(&quot;SHAPE&quot;);      //获取形状为 Circle 的对象      Shape shape1 = shapeFactory.getShape(&quot;CIRCLE&quot;);      //调用 Circle 的 draw 方法      shape1.draw();      //获取形状为 Rectangle 的对象      Shape shape2 = shapeFactory.getShape(&quot;RECTANGLE&quot;);      //调用 Rectangle 的 draw 方法      shape2.draw();            //获取形状为 Square 的对象      Shape shape3 = shapeFactory.getShape(&quot;SQUARE&quot;);      //调用 Square 的 draw 方法      shape3.draw();      //获取颜色工厂      AbstractFactory colorFactory = FactoryProducer.getFactory(&quot;COLOR&quot;);      //获取颜色为 Red 的对象      Color color1 = colorFactory.getColor(&quot;RED&quot;);      //调用 Red 的 fill 方法      color1.fill();      //获取颜色为 Green 的对象      Color color2 = colorFactory.getColor(&quot;Green&quot;);      //调用 Green 的 fill 方法      color2.fill();      //获取颜色为 Blue 的对象      Color color3 = colorFactory.getColor(&quot;BLUE&quot;);      //调用 Blue 的 fill 方法      color3.fill();   &#125;&#125;\n\n步骤 9验证输出\nInside Circle::draw() method.Inside Rectangle::draw() method.Inside Square::draw() method.Inside Red::fill() method.Inside Green::fill() method.Inside Blue::fill() method."},{"title":"设计模式简介","url":"/2018/02/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E7%AE%80%E4%BB%8B/","content":"设计模式（Design pattern）代表了最佳的实践，通常被有经验的面向对象的软件开发人员所采用。设计模式是软件开发人员在软件开发过程中面临的一般问题的解决方案。这些解决方案是众多软件开发人员经过相当长的一段时间的试验和错误总结出来的。\n设计模式是一套被反复使用的、多数人知晓的、经过分类编目的、代码设计经验的总结。使用设计模式是为了重用代码、让代码更容易被他人理解、保证代码可靠性。 毫无疑问，设计模式于己于他人于系统都是多赢的，设计模式使代码编制真正工程化，设计模式是软件工程的基石，如同大厦的一块块砖石一样。项目中合理地运用设计模式可以完美地解决很多问题，每种模式在现实中都有相应的原理来与之对应，每种模式都描述了一个在我们周围不断重复发生的问题，以及该问题的核心解决方案，这也是设计模式能被广泛应用的原因。\nGOF(Gang of Four)在 1994 年，由 Erich Gamma、Richard Helm、Ralph Johnson 和 John Vlissides 四人合著出版了一本名为 Design Patterns - Elements of Reusable Object-Oriented Software（中文译名：设计模式 - 可复用的面向对象软件元素） 的书，该书首次提到了软件开发中设计模式的概念。\n四位作者合称 GOF（Gang of Four）。他们所提出的设计模式主要是基于以下的面向对象设计原则。\n\n对接口编程而不是对实现编程。\n优先使用对象组合而不是继承。\n\n设计模式的使用设计模式在软件开发中的两个主要用途。\n开发人员的共同平台设计模式提供了一个标准的术语系统，且具体到特定的情景。例如，单例设计模式意味着使用单个对象，这样所有熟悉单例设计模式的开发人员都能使用单个对象，并且可以通过这种方式告诉对方，程序使用的是单例模式。\n最佳的实践设计模式已经经历了很长一段时间的发展，它们提供了软件开发过程中面临的一般问题的最佳解决方案。学习这些模式有助于经验不足的开发人员通过一种简单快捷的方式来学习软件设计。\n设计模式的类型根据设计模式的参考书 Design Patterns - Elements of Reusable Object-Oriented Software（中文译名：设计模式 - 可复用的面向对象软件元素） 中所提到的，总共有 23 种设计模式。这些模式可以分为三大类：创建型模式（Creational Patterns）、结构型模式（Structural Patterns）、行为型模式（Behavioral Patterns）。当然，我们还会讨论另一类设计模式：J2EE 设计模式。\n创建式模型 - 对象怎么来这些设计模式提供了一种在创建对象的同时隐藏创建逻辑的方式，而不是使用 new 运算符直接实例化对象。这使得程序在判断针对某个给定实例需要创建哪些对象时更加灵活。\n\n工厂模式（Factory Pattern）\n抽象工厂模式（Abstract Factory Pattern）\n单例模式（Singleton Pattern）\n建造者模式（Builder Pattern）\n原型模式（Prototype Pattern）\n\n结构型模式 - 对象和谁有关这些设计模式关注类和对象的组合。继承的概念被用来组合接口和定义组合对象获得新功能的方式。\n\n适配器模式（Adapter Pattern）\n桥接模式（Bridge Pattern）\n过滤器模式（Filter、Criteria Pattern）\n组合模式（Composite Pattern）\n装饰器模式（Decorator Pattern）\n外观模式（Facade Pattern）\n享元模式（Flyweight Pattern）\n代理模式（Proxy Pattern）\n\n行为型模式 - 对象与对象在干嘛这些设计模式特别关注对象之间的通信。\n\n责任链模式（Chain of Responsibility Pattern）\n命令模式（Command Pattern）\n解释器模式（Interpreter Pattern）\n迭代器模式（Iterator Pattern）\n中介者模式（Mediator Pattern）\n备忘录模式（Memento Pattern）\n观察者模式（Observer Pattern）\n状态模式（State Pattern）\n空对象模式（Null Object Pattern）\n策略模式（Strategy Pattern）\n模板模式（Template Pattern）\n访问者模式（Visitor Pattern）\n\nJ2EE 模式 - 对象合起来要干嘛这些设计模式特别关注表示层。这些模式是由 Sun Java Center 鉴定的。\n\nMVC 模式（MVC Pattern）\n业务代表模式（Business Delegate Pattern）\n组合实体模式（Composite Entity Pattern）\n数据访问对象模式（Data Access Object Pattern）\n前端控制器模式（Front Controller Pattern）\n拦截过滤器模式（Intercepting Filter Pattern）\n服务定位器模式（Service Locator Pattern）\n传输对象模式（Transfer Object Pattern）\n\n下面用一个图片来整体描述一下设计模式之间的关系：\n\n\n设计模式的六大原则1、开闭原则（Open Close Principle）开闭原则的意思是：对扩展开放，对修改关闭。在程序需要进行拓展的时候，不能去修改原有的代码，实现一个热插拔的效果。简言之，是为了使程序的扩展性好，易于维护和升级。想要达到这样的效果，我们需要使用接口和抽象类，后面的具体设计中我们会提到这点。\n2、里氏代换原则（Liskov Substitution Principle）里氏代换原则是面向对象设计的基本原则之一。 里氏代换原则中说，任何基类可以出现的地方，子类一定可以出现。LSP 是继承复用的基石，只有当派生类可以替换掉基类，且软件单位的功能不受到影响时，基类才能真正被复用，而派生类也能够在基类的基础上增加新的行为。里氏代换原则是对开闭原则的补充。实现开闭原则的关键步骤就是抽象化，而基类与子类的继承关系就是抽象化的具体实现，所以里氏代换原则是对实现抽象化的具体步骤的规范。\n3、依赖倒转原则（Dependence Inversion Principle）这个原则是开闭原则的基础，具体内容：针对接口编程，依赖于抽象而不依赖于具体。\n4、接口隔离原则（Interface Segregation Principle）这个原则的意思是：使用多个隔离的接口，比使用单个接口要好。它还有另外一个意思是：降低类之间的耦合度。由此可见，其实设计模式就是从大型软件架构出发、便于升级和维护的软件设计思想，它强调降低依赖，降低耦合。\n5、迪米特法则，又称最少知道原则（Demeter Principle）最少知道原则是指：一个实体应当尽量少地与其他实体之间发生相互作用，使得系统功能模块相对独立。\n6、合成复用原则（Composite Reuse Principle）合成复用原则是指：尽量使用合成&#x2F;聚合的方式，而不是使用继承。\n","categories":["设计模式"]},{"title":"Git版本如何回退","url":"/2018/03/Git%E7%89%88%E6%9C%AC%E5%A6%82%E4%BD%95%E5%9B%9E%E9%80%80/","content":"现在，我们已经学会了修改文件，然后把修改提交到Git版本库，现在，再练习一次，修改readme.txt文件如下：\n增加distributed under the GPL。\nGit is a distributed version control system.Git is free software distributed under the GPL.\n\n当修改完文件以后，git status显示：\n$ git statusOn branch masterChanges not staged for commit:  (use &quot;git add &lt;file&gt;...&quot; to update what will be committed)  (use &quot;git checkout -- &lt;file&gt;...&quot; to discard changes in working directory)\tmodified:   readme.txtno changes added to commit (use &quot;git add&quot; and/or &quot;git commit -a&quot;)\n\n此时在SourceTree里面会显示有Uncommitted Changes。并且，在Unstaged files框里面，readme.txt会被显示出来。因为没有执行git add readme.txt命令，所以并不会显示在Staged files里面。\n\n\n然后尝试提交：\n$ git add readme.txt$ git commit -m &quot;append GPL&quot;\n\n每当你觉得文件修改到一定程度的时候，就可以“保存一个快照”，这个快照在Git中被称为commit。一旦你把文件改乱了，或者误删了文件，还可以从最近的一个commit恢复，然后继续工作，而不是把几个月的工作成果全部丢失。\n现在，我们回顾一下readme.txt文件一共有几个版本被提交到Git仓库里了。当然了，在实际工作中，我们脑子里怎么可能记得一个几千行的文件每次都改了什么内容，不然要版本控制系统干什么。版本控制系统肯定有某个命令可以告诉我们历史记录，在Git中，我们用git log命令查看：\n$ git logcommit a22668ed4e61ccdd46b5008d29b9e0bea08460c6Author: Simon &lt;&gt;Date:   Wed Mar 14 13:55:40 2018 +0800    append GPLcommit 2a06975dc36f95797186e346b53c298a80344ca7Author: Simon &lt;&gt;Date:   Wed Mar 14 11:26:53 2018 +0800    add distributedcommit 7e08ce29bcebcd637eb03ea3568da4b4b0b87914Author: Simon &lt;&gt;Date:   Wed Mar 14 10:50:40 2018 +0800    wrote a readme file\n\n如果嫌输出信息太多，看得眼花缭乱的，可以试试加上–pretty&#x3D;oneline参数：\n$ git log --pretty=onelinea22668ed4e61ccdd46b5008d29b9e0bea08460c6 append GPL2a06975dc36f95797186e346b53c298a80344ca7 add distributed7e08ce29bcebcd637eb03ea3568da4b4b0b87914 wrote a readme file\n\n好了，现在我们启动时光穿梭机，准备把readme.txt回退到上一个版本，也就是“add distributed”的那个版本，怎么做呢？\n首先，Git必须知道当前版本是哪个版本，在Git中，用HEAD表示当前版本，也就是最新的提交a22668ed4e61ccdd46b5008d29b9e0bea08460c6（注意我的提交ID和你的肯定不一样），上一个版本就是HEAD^，上上一个版本就是HEAD^^，当然往上100个版本写100个^比较容易数不过来，所以写成HEAD~100。\n现在，我们要把当前版本“append GPL”回退到上一个版本“add distributed”，就可以使用git reset命令：\n$ git reset --hard HEAD^\n\n现在，你回退到了某个版本，关掉了电脑，第二天早上就后悔了，想恢复到新版本怎么办？找不到新版本的commit id怎么办？\nGit提供了一个命令git reflog用来记录你的每一次命令：\n$ git reflog\n\n现在总结一下：\n\nHEAD指向的版本就是当前版本，因此，Git允许我们在版本的历史之间穿梭，使用命令git reset --hard commit_id。\n\n穿梭前，用git log可以查看提交历史，以便确定要回退到哪个版本。\n\n要重返未来，用git reflog查看命令历史，以便确定要回到未来的哪个版本。\n\n\n","categories":["Tools","Git"],"tags":["git","github","gitlab","reset","commit"]},{"title":"Git撤销修改","url":"/2018/03/Git%E6%92%A4%E9%94%80%E4%BF%AE%E6%94%B9/","content":"总结一下，文件在三种状态下面的撤销修改\n\n当在工作区修改了文件内容，想直接丢弃掉工作区的修改，用命令git checkout -- &lt;file path&gt;\n\n当在工作区修改了文件内容，并已经执行了命令git add &lt;file path&gt;，也即是，文件已经被添加到了暂存区，想丢弃修改，分两步，第一步执行命令git reset HEAD &lt;file path&gt;，文件回到工作区，第二步，git checkout -- &lt;file path&gt;\n\n当已经提交了不合适的修改到版本库时，也即是执行了git commit -m &quot;some comment&quot;时，想要撤销本次提交，那就需要执行git reset --hard commit_id。此前提是没有推送到远程库。\n\n\n","categories":["Tools","Git"],"tags":["git","github","gitlab","reset","commit"]},{"title":"Git的工作区和暂存区","url":"/2018/03/Git%E7%9A%84%E5%B7%A5%E4%BD%9C%E5%8C%BA%E5%92%8C%E6%9A%82%E5%AD%98%E5%8C%BA/","content":"Git和其他版本控制系统如SVN的一个不同之处就是有暂存区的概念。\n先来看名词解释。\n工作区（Working Directory）就是你在电脑里能看到的目录，比如我的&#x2F;Users&#x2F;simon&#x2F;learngit文件夹就是一个工作区。\n版本库（Repository)工作区有一个隐藏目录.git，这个不算工作区，而是Git的版本库。\nGit的版本库里存了很多东西，其中最重要的就是称为stage（或者叫index）的暂存区，还有Git为我们自动创建的第一个分支master，以及指向master的一个指针叫HEAD。\n分支和HEAD的概念我们以后再讲。\n前面讲了我们把文件往Git版本库里添加的时候，是分两步执行的：\n第一步是用git add把文件添加进去，实际上就是把文件修改添加到暂存区；\n第二步是用git commit提交更改，实际上就是把暂存区的所有内容提交到当前分支。\n因为我们创建Git版本库时，Git自动为我们创建了唯一一个master分支，所以，现在，git commit就是往master分支上提交更改。\n你可以简单理解为，需要提交的文件修改通通放到暂存区，然后，一次性提交暂存区的所有修改。\n俗话说，实践出真知。现在，我们再练习一遍，先对readme.txt做个修改，比如加上一行内容：\nGit is a distributed version control system.Git is free software distributed under the GPL.Git has a mutable index called stage.\n\n然后，在工作区新增一个LICENSE文本文件（内容随便写）。\n先用git status查看一下状态：\n$ git statusOn branch masterChanges not staged for commit:  (use &quot;git add &lt;file&gt;...&quot; to update what will be committed)  (use &quot;git checkout -- &lt;file&gt;...&quot; to discard changes in working directory)\tmodified:   readme.txtUntracked files:  (use &quot;git add &lt;file&gt;...&quot; to include in what will be committed)\tLICENSEno changes added to commit (use &quot;git add&quot; and/or &quot;git commit -a&quot;)\n\nGit非常清楚地告诉我们，readme.txt被修改了，而LICENSE还从来没有被添加过，所以它的状态是Untracked。\n现在，使用两次命令git add，把readme.txt和LICENSE都添加后，用git status再查看一下：\n$ git statusOn branch masterChanges to be committed:  (use &quot;git reset HEAD &lt;file&gt;...&quot; to unstage)\tnew file:   LICENSE\tmodified:   readme.txt\n\n所以，git add命令实际上就是把要提交的所有修改放到暂存区（Stage），然后，执行git commit就可以一次性把暂存区的所有修改提交到分支。\n$ git commit -m &quot;understand how stage works&quot;[master ae9ae11] understand how stage works 2 files changed, 1 insertion(+) create mode 100644 LICENSE\n\n一旦提交后，如果你又没有对工作区做任何修改，那么工作区就是“干净”的：\n$ git statusOn branch masternothing to commit, working tree clean\n\n现在版本库变成了这样，暂存区就没有任何内容了。\n","categories":["Tools","Git"],"tags":["git","github","gitlab","reset","commit"]},{"title":"CentOS安装Mysql（MariaDb）及配置","url":"/2018/03/CentOS%E5%AE%89%E8%A3%85Mysql%EF%BC%88MariaDb%EF%BC%89%E5%8F%8A%E9%85%8D%E7%BD%AE/","content":"阿里云的空间没有续费，资源被释放了，代老师又要重启应用，这下就只有重新装系统装软件了。在这篇文章中，记录一下关于数据库Mysql的安装及配置过程。\n安装Mariadb\n安装命令\n\nyum -y install mariadb mariadb-server\n\n启动\n\nsystemctl start mariadb\n同时补充其他几个命令\nsystemctl stop mariadb # 关闭服务systemctl restart mariadb # 重启服务systemctl status mariadb # 查看状态\n\n\n进行Mariadb相关的简单初始化配置\n\nmysql_secure_installation\n输入root密码，当然第一次密码为空，直接回车\nEnter current password for root (enter for none):\n设置密码\nSet root password? [Y/n] &lt;– 是否设置root用户密码，输入y并回车或直接回车New password: &lt;– 设置root用户的密码Re-enter new password: &lt;– 再输入一次你设置的密码\n其他配置\nRemove anonymous users? [Y/n] &lt;– 是否删除匿名用户，回车Disallow root login remotely? [Y/n] &lt;–是否禁止root远程登录,回车,Remove test database and access to it? [Y/n] &lt;– 是否删除test数据库，回车Reload privilege tables now? [Y/n] &lt;– 是否重新加载权限表，回车\n\n配置Mariadb的字符集登录数据库\n$ mysql -u root -p\n\n查看当前字符集\nMariaDB [(none)]&gt; show variables like &quot;%character%&quot;;+--------------------------+----------------------------+| Variable_name            | Value                      |+--------------------------+----------------------------+| character_set_client     | utf8                       || character_set_connection | utf8                       || character_set_database   | latin1                     || character_set_filesystem | binary                     || character_set_results    | utf8                       || character_set_server     | latin1                     || character_set_system     | utf8                       || character_sets_dir       | /usr/share/mysql/charsets/ |+--------------------------+----------------------------+8 rows in set (0.00 sec)MariaDB [(none)]&gt; show variables like &#x27;%collation%&#x27;;+----------------------+-------------------+| Variable_name        | Value             |+----------------------+-------------------+| collation_connection | utf8_general_ci   || collation_database   | latin1_swedish_ci || collation_server     | latin1_swedish_ci |+----------------------+-------------------+3 rows in set (0.00 sec)\n\n修改字符集为utf-8\nvi /etc/my.cnf\n在[mysqld]后添加\ncharacter_set_server=utf8\n再次查看字符集\nMariaDB [(none)]&gt; show variables like &#x27;%character%&#x27;;+--------------------------+----------------------------+| Variable_name            | Value                      |+--------------------------+----------------------------+| character_set_client     | utf8                       || character_set_connection | utf8                       || character_set_database   | utf8                       || character_set_filesystem | binary                     || character_set_results    | utf8                       || character_set_server     | utf8                       || character_set_system     | utf8                       || character_sets_dir       | /usr/share/mysql/charsets/ |+--------------------------+----------------------------+MariaDB [(none)]&gt; show variables like &#x27;%collation%&#x27;;+----------------------+-----------------+| Variable_name        | Value           |+----------------------+-----------------+| collation_connection | utf8_general_ci || collation_database   | utf8_general_ci || collation_server     | utf8_general_ci |+----------------------+-----------------+\n\n设置权限授予root外网登陆权限\nmysql&gt;grant all privileges on *.* to root@&#x27;%&#x27; identified by &lt;password&gt; WITH GRANT OPTION;\n\n到此为止，mysql的安装及初始化配置已经完成。\n开启开机启动systemctl enable mariadb\n\n新版本的CentOS已经不使用chkconfig命令了\n设置开机不自动启动则是\nsystemctl disable mariadb\n\n查看此服务是否为开机启动\nsystemctl is-enabled mariadb\n\n查看开机自启动的命令\nls /etc/systemd/system/multi-user.target.wants/\n\n异常情况恢复如果root自己把自己的权限弄没了，可以通过如下命令进入\n# mysqld_safe --skip-grant-tables &amp;\n\n这时进入mysql是不需要密码的。\n如何把root用户都删除了，这时通过CREATE USER命令或者GRANT命令都是没有用的。需要通过SQL语句来插入一个用户。\nMariaDB [mysql]&gt; INSERT INTO mysql.user (Host, User, Password) VALUES(&#x27;localhost&#x27;, &#x27;root&#x27;, PASSWORD(&#x27;123456&#x27;));\n\n然后把root用户其他所有的XXX_priv字段改为Y。这时，root用户的权限才算恢复过来了。\n","categories":["Database"]},{"title":"Git管理下的项目如何更新文件","url":"/2018/03/Git%E7%AE%A1%E7%90%86%E4%B8%8B%E7%9A%84%E9%A1%B9%E7%9B%AE%E5%A6%82%E4%BD%95%E6%9B%B4%E6%96%B0%E6%96%87%E4%BB%B6/","content":"我们已经成功地添加并提交了一个readme.txt文件，现在，是时候继续工作了，于是，我们继续修改readme.txt文件，改成如下内容：(第一行添加一个单词distributed)\nGit is a distributed version control system.Git is free software.\n\n现在，运行git status命令看看结果：\n$ git statusOn branch masterChanges not staged for commit:  (use &quot;git add &lt;file&gt;...&quot; to update what will be committed)  (use &quot;git checkout -- &lt;file&gt;...&quot; to discard changes in working directory)\tmodified:   readme.txtno changes added to commit (use &quot;git add&quot; and/or &quot;git commit -a&quot;)\n\ngit status命令可以让我们时刻掌握仓库当前的状态，上面的命令告诉我们，readme.txt被修改过了，但还没有准备提交的修改。\n虽然Git告诉我们readme.txt被修改了，但如果能看看具体修改了什么内容，自然是很好的。因为可能已经记不清上次怎么修改的readme.txt，所以，需要用git diff这个命令看看：\n$ git diffdiff --git a/readme.txt b/readme.txtindex 46d49bf..9247db6 100644--- a/readme.txt+++ b/readme.txt@@ -1,2 +1,2 @@-Git is a version control system.+Git is a distributed version control system. Git is free software.\n\ngit diff顾名思义就是查看difference，显示的格式正是Unix通用的diff格式，可以从上面的命令输出看到，我们在第一行添加了一个“distributed”单词。\n知道了对readme.txt作了什么修改后，再把它提交到仓库就放心多了，提交修改和提交新文件是一样的两步，第一步是git add：\n$ git add readme.txt\n\n同样没有任何输出。在执行第二步git commit之前，我们再运行git status看看当前仓库的状态：\n$ git statusOn branch masterChanges to be committed:  (use &quot;git reset HEAD &lt;file&gt;...&quot; to unstage)\tmodified:   readme.txt\n\nstatus```告诉我们，将要被提交的修改包括readme.txt，下一步，就可以放心地提交了：```shell$ git commit -m &quot;add distributed&quot;[master 2a06975] add distributed 1 file changed, 1 insertion(+), 1 deletion(-)\n\n提交后，我们再用git status命令看看仓库的当前状态：\n$ git statusOn branch masternothing to commit, working tree clean\n\nGit告诉我们当前没有需要提交的修改，而且，工作目录是干净（working directory clean）的。\n此时，SourceTree的效果图如下：\n\n","categories":["Tools","Git"],"tags":["git","github","gitlab","reset","commit"]},{"title":"IntelliJ IDEA报错Cannot resolve symbol log","url":"/2018/03/IntelliJ-IDEA%E6%8A%A5%E9%94%99Cannot-resolve-symbol-log/","content":"之前在写程序需要输出日志时，往往是这样写的\nprivate static final Logger log = LoggerFactory.getLogger(XXX.class)\n\n之后，可以使用@Slf4j的注解\n&lt;dependency&gt;   &lt;groupId&gt;org.projectlombok&lt;/groupId&gt;   &lt;artifactId&gt;lombok&lt;/artifactId&gt;&lt;/dependency&gt;\n\n如果注解@Slf4j注入后找不到变量log，那就给IDE安装lombok插件。File  → settings →  Plugins, 然后点击“Browse repositories”，重启IDEA后，就不会出现Cannot resolve symbol ‘log’的错误了。\n","categories":["Tools","IntelliJ IDEA"]},{"title":"ssh客户端无需输入密码登录服务器","url":"/2018/03/ssh%E5%AE%A2%E6%88%B7%E7%AB%AF%E6%97%A0%E9%9C%80%E8%BE%93%E5%85%A5%E5%AF%86%E7%A0%81%E7%99%BB%E5%BD%95%E6%9C%8D%E5%8A%A1%E5%99%A8/","content":"原来通过ssh client登录服务器时：\n$ ssh root@192.168.1.100root@192.168.1.100&#x27;s password:\n是需要登录用户输入密码的，如果密码强度设置得比较高的话，还是不太容易记忆的。每次登录前都得把密码记录翻出来看看，工作效率不是很高。\n为了提高效率，我们可以在本地（client端）生成一个私钥，一个公钥。\n$ cd ~/.ssh$ ssh-keygenGenerating public/private rsa key pair.Enter file in which to save the key (/Users/simon/.ssh/id_rsa): Enter passphrase (empty for no passphrase): Enter same passphrase again: Your identification has been saved in /Users/simon/.ssh/id_rsa.Your public key has been saved in /Users/simon/.ssh/id_rsa.pub.The key fingerprint is:SHA256:aleqVoG5ecZ*****************qWkWaO0BTRG8 simon@Simon.localThe key&#x27;s randomart image is:+---[RSA 2048]----+|       ++.       ||      B o.+      ||     * @ BEo     ||    = O O.= o    ||   . + XSB.o     ||    o +.@o.      ||   o  o*oo       ||  .  .oo         ||     ..          |+----[SHA256]-----+$ lsconfig\t\t\t\tid_rsa.pub\t\t\tid_rsa\t\t\t\tknown_hosts\n\n然后登录到远程服务器，在~&#x2F;.ssh目录下新建authorized_keys文件。\n$ touch ~/.ssh/authorized_keys\n\n把公钥（client端）的~&#x2F;.ssh&#x2F;id_rsa.pub文件的内容复制到服务器的.ssh&#x2F;authorized_keys里面保存着。\n这时，当你通过ssh root@192.168.1.100登录服务器时，就已经不需要输入密码了。\n","categories":["Tools","SSH"]},{"title":"如何修改Ubuntu的系统时区","url":"/2018/03/%E5%A6%82%E4%BD%95%E4%BF%AE%E6%94%B9Ubuntu%E7%9A%84%E7%B3%BB%E7%BB%9F%E6%97%B6%E5%8C%BA/","content":"查看系统当前时区\n$ date -RTue, 20 Mar 2018 02:01:35 -0400\n\n网上有一堆人说通过执行tzselect命令可以修改时区。我们先执行此命令尝试一下。\n$ tzselectPlease identify a location so that time zone rules can be set correctly.Please select a continent, ocean, &quot;coord&quot;, or &quot;TZ&quot;. 1) Africa 2) Americas 3) Antarctica 4) Asia 5) Atlantic Ocean 6) Australia 7) Europe 8) Indian Ocean 9) Pacific Ocean10) coord - I want to use geographical coordinates.11) TZ - I want to specify the time zone using the Posix TZ format.#? 4Please select a country whose clocks agree with yours. 1) Afghanistan\t\t  18) Israel\t\t    35) Palestine 2) Armenia\t\t  19) Japan\t\t    36) Philippines 3) Azerbaijan\t\t  20) Jordan\t\t    37) Qatar 4) Bahrain\t\t  21) Kazakhstan\t    38) Russia 5) Bangladesh\t\t  22) Korea (North)\t    39) Saudi Arabia 6) Bhutan\t\t  23) Korea (South)\t    40) Singapore 7) Brunei\t\t  24) Kuwait\t\t    41) Sri Lanka 8) Cambodia\t\t  25) Kyrgyzstan\t    42) Syria 9) China\t\t  26) Laos\t\t    43) Taiwan10) Cyprus\t\t  27) Lebanon\t\t    44) Tajikistan11) East Timor\t\t  28) Macau\t\t    45) Thailand12) Georgia\t\t  29) Malaysia\t\t    46) Turkmenistan13) Hong Kong\t\t  30) Mongolia\t\t    47) United Arab Emirates14) India\t\t  31) Myanmar (Burma)\t    48) Uzbekistan15) Indonesia\t\t  32) Nepal\t\t    49) Vietnam16) Iran\t\t  33) Oman\t\t    50) Yemen17) Iraq\t\t  34) Pakistan#? 9Please select one of the following time zone regions.1) Beijing Time2) Xinjiang Time#? 1The following information has been given:\tChina\tBeijing TimeTherefore TZ=&#x27;Asia/Shanghai&#x27; will be used.Local time is now:\tTue Mar 20 14:05:28 CST 2018.Universal Time is now:\tTue Mar 20 06:05:28 UTC 2018.Is the above information OK?1) Yes2) No#? 1You can make this change permanent for yourself by appending the line\tTZ=&#x27;Asia/Shanghai&#x27;; export TZto the file &#x27;.profile&#x27; in your home directory; then log out and log in again.Here is that TZ value again, this time on standard output so that youcan use the /usr/bin/tzselect command in shell scripts:Asia/Shanghai\n\n程序结束，我们执行date -R发现，时区并没有变化。\n那就奇怪了，tzselect命令是用于干嘛的呢？那我执行man tzselect再看看。\nDESCRIPTION       This  manual page explains how you can use the tzselect utility to view the installed timezone. It comes handy when you want to know       what time it is in other countries, or if you just wonder what timezones exist.       tzselect is called without any parameters from the shell. It shows a list of about one dozen geographic areas one can roughly recog‐       nize as continents. After choosing a geographic area by number, a list of countries and cities in this area will be shown.       You  can press the Enter key to reprint the list. To choose a timezone, just press the number left to it.  If your input is invalid,       the list will be reprinted.       You may press Ctrl-C to interrupt the script at any time.       Note that tzselect will not actually change the timezone for you. Use &#x27;dpkg-reconfigure tzdata&#x27; to achieve this.\n\n注释中写得很清楚，tzselect命令只用于你查看系统存在哪些国家和时区，并且，此命令并不会真实的修改系统所在时区。我们需要执行dpkg-reconfigure tzdata来修改时区。\n$ sudo dpkg-reconfigure tzdata[sudo] password for root: ********\n\n\n\n\n\nCurrent default time zone: &#x27;Asia/Shanghai&#x27;Local time is now:      Tue Mar 20 14:16:40 CST 2018.Universal Time is now:  Tue Mar 20 06:16:40 UTC 2018.\n执行结束，系统的时区才会真实的改变了。\n$ date -RTue, 20 Mar 2018 14:17:25 +0800\n","categories":["OS","Ubuntu"]},{"title":"如何创建Git版本库","url":"/2018/03/%E5%A6%82%E4%BD%95%E5%88%9B%E5%BB%BAGit%E7%89%88%E6%9C%AC%E5%BA%93/","content":"#创建版本库\n什么是版本库呢？版本库又名仓库，英文名repository，你可以简单理解成一个目录，这个目录里面的所有文件都可以被Git管理起来，每个文件的修改、删除，Git都能跟踪，以便任何时刻都可以追踪历史，或者在将来某个时刻可以“还原”。\n所以，创建一个版本库非常简单，首先，选择一个合适的地方，创建一个空目录：\n$ pwd/Users/simon$ mkdir learngit$ cd learngit/$ pwd/Users/simon/learngit\n\n创建好learngit目录后，如果Mac下的SourceTree Git管理工具打开此目录，则会报如下错误：\nInvalid folderThe path /Users/simon/learngit does not refer to a working copy for Mercurial or Git.\n说明此文件夹还未变成Git可管理的文件夹（项目、仓库）。\n第二步，通过git init命令把这个目录变成Git可以管理的仓库：\n$ git initInitialized empty Git repository in /Users/simon/learngit/.git/\n\n瞬间Git就把仓库建好了，而且告诉你是一个空的仓库（empty Git repository），细心的读者可以发现当前目录下多了一个.git的目录，这个目录是Git来跟踪管理版本库的。\n如果你没有看到.git目录，那是因为这个目录默认是隐藏的，用ls -ah命令就可以看见。\n也不一定必须在空目录下创建Git仓库，选择一个已经有东西的目录也是可以的。\n把文件添加到版本库首先这里再明确一下，所有的版本控制系统，其实只能跟踪文本文件的改动，比如TXT文件，网页，所有的程序代码等等，Git也不例外。版本控制系统可以告诉你每次的改动，比如在第5行加了一个单词“Linux”，在第8行删了一个单词“Windows”。而图片、视频这些二进制文件，虽然也能由版本控制系统管理，但没法跟踪文件的变化，只能把二进制文件每次改动串起来，也就是只知道图片从100KB改成了120KB，但到底改了啥，版本控制系统不知道，也没法知道。\n不幸的是，Microsoft的Word格式是二进制格式，因此，版本控制系统是没法跟踪Word文件的改动的。\n因为文本是有编码的，比如中文有常用的GBK编码，日文有Shift_JIS编码，如果没有历史遗留问题，强烈建议使用标准的UTF-8编码，所有语言使用同一种编码，既没有冲突，又被所有平台所支持。\n现在我们编写一个readme.txt文件，内容如下：\nGit is a version control system.Git is free software.\n\n一定要放到learngit目录下（子目录也行），因为这是一个Git仓库。\n把一个文件放到Git仓库只需要两步。\n\n用命令git add告诉Git，把文件添加到仓库：\n\n$ git add readme.txt\n\n用命令git commit告诉Git，把文件提交到仓库：\n\n$ git commit -m &quot;wrote a readme file&quot;[master (root-commit) 7e08ce2] wrote a readme file 1 file changed, 2 insertions(+) create mode 100644 readme.txt\n\n\n简单解释一下git commit命令，-m后面输入的是本次提交的说明，可以输入任意内容，当然最好是有意义的，这样你就能从历史记录里方便地找到改动记录。\ngit commit命令执行成功后会告诉你，1个文件被改动（我们新添加的readme.txt文件），插入了两行内容（readme.txt有两行内容）。\n为什么Git添加文件需要add，commit一共两步呢？因为commit可以一次提交很多文件，所以你可以多次add不同的文件，比如：\n$ git add file1.txt$ git add file2.txt file3.txt$ git commit -m &quot;add 3 files.&quot;\n\n此时，通过SourceTree可以观察到如下效果。\n\n","categories":["Tools","Git"],"tags":["git","github","gitlab","reset","commit"]},{"title":"CentOS7上安装Tomcat7","url":"/2018/06/CentOS7%E4%B8%8A%E5%AE%89%E8%A3%85Tomcat7/","content":"下载tomcat7的tar.gz压缩文件\n解压到&#x2F;usr&#x2F;local目录\n$ tar -zxvf apache-tomcat-8.5.31.tar.gz\n\n编译server.xml文件，修改编码\n$ cd apache-tomcat-8.5.31/$ vi conf/server.xml\n\n找到这一行\n&lt;Connector port=&quot;8080&quot; protocol=&quot;HTTP/1.1&quot;               connectionTimeout=&quot;20000&quot;               redirectPort=&quot;8443&quot; /&gt;              \n\n增加URIEncoding&#x3D;”UTF-8”，替换为\n&lt;Connector port=&quot;8080&quot; protocol=&quot;HTTP/1.1&quot;               connectionTimeout=&quot;20000&quot;               redirectPort=&quot;8443&quot; URIEncoding=&quot;UTF-8&quot; /&gt;\n\n启动Tomcat7\nbin/startup.sh Using CATALINA_BASE:   /usr/local/apache-tomcat-8.5.31Using CATALINA_HOME:   /usr/local/apache-tomcat-8.5.31Using CATALINA_TMPDIR: /usr/local/apache-tomcat-8.5.31/tempUsing JRE_HOME:        /usrUsing CLASSPATH:       /usr/local/apache-tomcat-8.5.31/bin/bootstrap.jar:/usr/local/apache-tomcat-8.5.31/bin/tomcat-juli.jarTomcat started.\n\n查看本地ip地址\nifconfigeno16777736: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt;  mtu 1500        inet 192.168.1.30  netmask 255.255.255.0  broadcast 192.168.1.255...        \n访问http://192.168.1.30:8080出现下图\n\n\n表示tomcat7安装并启动成功了。\n","categories":["Java","Tomcat"],"tags":["java","tomcat","centos"]},{"title":"CentOS7下安装vsftpd","url":"/2018/06/CentOS7%E4%B8%8B%E5%AE%89%E8%A3%85vsftpd/","content":"通过yum安装vsftpdyum install vsftpdIs this ok [y/d/N]: yDownloading packages:vsftpd-3.0.2-22.el7.x86_64.rpm                                                                                    | 169 kB  00:00:00     Running transaction checkRunning transaction testTransaction test succeededRunning transaction  正在安装    : vsftpd-3.0.2-22.el7.x86_64                                                                                           1/1   验证中      : vsftpd-3.0.2-22.el7.x86_64                                                                                           1/1 已安装:  vsftpd.x86_64 0:3.0.2-22.el7                                                                                                           完毕！\n\n新建一个上传文件夹用于存放通过ftp上传的文件\npwd/usr/localmkdir ftpfile\n\n新建一个ftp用户新建ftpuser用户，该用户登录后的地址为&#x2F;usr&#x2F;local&#x2F;ftpfile，这个用户不能登录系统\nuseradd ftpuser -d /ftpfile -s /sbin/nologin\n\n修改&#x2F;usr&#x2F;local&#x2F;ftpfile文件夹的权限ll | grep ftpfiledrwxr-xr-x   2 root root       6 6月  10 14:05 ftpfilechown -R ftpuser.ftpuser ftpfile/ll | grep ftpfiledrwxr-xr-x   2 ftpuser ftpuser       6 6月  10 14:05 ftpfile\n\n为用户ftpuser添加密码passwd ftpuser\n\n修改vsftpd配置文件vi /etc/vsftpd/vsftpd.confftpd_banner=Welcome to simon&#x27;s FTP service.local_root=/usr/local/ftpfile # 添加本地根目录anon_root=/usr/local/ftpfile  # use_localtime=yeschroot_list_enable=YES # 修改# (default follows)chroot_list_file=/etc/vsftpd/chroot_list # 修改anonymous_enable=NO\n\n把用户ftpuser添加到&#x2F;etc&#x2F;vsftpd&#x2F;chroot_list\nvi /etc/vsftpd/chroot_list\n添加一行记录，内容为：ftpuser\n设置传输文件时的端口号pasv_min_port=61001pasv_max_port=62000\n\n设置pasv被动传输的端口号，这样可以通过防火墙开放这个范围内的端口，从而更加安全。\n启动vsftpdservice vsftpd startRedirecting to /bin/systemctl start  vsftpd.service\n\n测试在浏览器上输入：ftp://192.168.1.30/输入用户名和密码后，貌似不能进去。\n或者在终端输入\nftp 192.168.1.30Connected to 192.168.1.30.220 Welcome to simon&#x27;s FTP service.Name (192.168.1.30:simon): ftpuser331 Please specify the password.Password: 500 OOPS: vsftpd: refusing to run with writable root inside chroot()ftp: Login failed\n\n会出现以上错误，所以还需要在配置文件里面加入如下参数：\nallow_writeable_chroot=YES # 如果local_root是可写的就要设置这句\n\n重启vsftpd服务后，再次登录，就成功了\nftp 192.168.1.30Connected to 192.168.1.30.220 Welcome to simon&#x27;s FTP service.Name (192.168.1.30:simon): ftpuser331 Please specify the password.Password: 230 Login successful.Remote system type is UNIX.Using binary mode to transfer files.ftp&gt; \n\n提示#chroot_local_user=YES\n默认为NO，表示用户能看到的最上层目录为&#x2F;ftpfile。如果改为yes，那用户可以看到整个系统的目录结构。\n","categories":["Tools","FTP"],"tags":["centos","ftp"]},{"title":"CentOS7源码编译安装、配置nginx","url":"/2018/06/CentOS7%E6%BA%90%E7%A0%81%E7%BC%96%E8%AF%91%E5%AE%89%E8%A3%85%E3%80%81%E9%85%8D%E7%BD%AEnginx/","content":"安装依赖安装gcc-c++，安装pcre, pcre-devel（会依赖pcre)，安装zlib, zlib-devel和openssl，openssl-devel\nyum install -y gcc-c++, pcre, pcre-devel, zlib, zlib-devel, openssl, openssl-devel\n\n下载nginx源码包在官网上下载Stable version，稳定版本。http://nginx.org/en/download.html\n目前最新的稳定版本是：\tnginx-1.14.0 http://nginx.org/download/nginx-1.14.0.tar.gz\ncd /usr/localwget http://nginx.org/download/nginx-1.14.0.tar.gztar -zxvf nginx-1.14.0.tar.gz\n\n编译安装cd nginx-1.14.0./configuremake &amp;&amp; make install\n\n启动nginxwhereis nginxnginx: /usr/local/nginxcd /usr/local/nginxsbin/nginxps -ef | grep nginxroot      20240      1  0 11:09 ?        00:00:00 nginx: master process sbin/nginxnobody    20241  20240  0 11:09 ?        00:00:00 nginx: worker process\n\n访问http://192.168.1.30，如果在本机访问，则访问http://localhost，出现如下图片，表明已经安装成功。\n\n\n配置nginxcd confvi nginx.conf\n\n添加一条记录，增加虚拟主机的配置\ninclude vhost/*.conf;\n\n并新建一个vhost目录，用于放置*.conf文件\nmkdir vhost\n\n添加一个虚拟主机upstream web &#123;        server localhost:8080;&#125;server &#123;        listen       80;        server_name  domain.com www.domain.com;        location / &#123;                proxy_pass              http://web;                proxy_redirect          off;                proxy_set_header        Host $host;                proxy_set_header        X-Real-IP $remote_addr;                proxy_set_header        X-Forwarded-For $proxy_add_x_forwarded_for;         &#125;&#125;\n\n重新加载配置文件\nsbin/nginx -s reload\n\n这样，通过访问域名domain.com或www.domain.com的请求，就可以转发localhost:8080上了\n","categories":["Tools","Nginx"],"tags":["centos","nginx"]},{"title":"Java中的Integer取值范围（-2^32 ~ 2^31 - 1）分析","url":"/2018/05/Java%E4%B8%AD%E7%9A%84Integer%E5%8F%96%E5%80%BC%E8%8C%83%E5%9B%B4%EF%BC%88-2-32-2-31-1%EF%BC%89%E5%88%86%E6%9E%90/","content":"在Java中Integer的最小值（MIN_VALUE）、最大值（MAX_VALUE）定义如下：\n/**  * A constant holding the minimum value an &#123;@code int&#125; can  * have, -2&lt;sup&gt;31&lt;/sup&gt;.  */ @Native public static final int   MIN_VALUE = 0x80000000; /**  * A constant holding the maximum value an &#123;@code int&#125; can  * have, 2&lt;sup&gt;31&lt;/sup&gt;-1.  */ @Native public static final int   MAX_VALUE = 0x7fffffff;\n\n我们大家都知道Integer的最小值为-2^32，最大值为2^32-1，为什么是这样呢，我们来看看推导过程：\nInteger 实际占用的二进制码的位数一个 Integer 类型占 4 字节，一个字节占 8 位二进制码，因此一个 Integer 总共占 32 位二进制码。去除第一位的符号位，剩下 31 位来表示数值。\nMIN_VALUE = 0x80000000;  // 补码MAX_VALUE = 0x7fffffff;  // 补码\n\n原码、反码、补码在计算机中，数据是由二进制补码进行存储的，在 Java 代码中我们看到的 “0x80000000”、“0x7fffffff”，这些非10进制的数，都是以补码的形式存在的，通过转换成原码，我们才能知道其真实的值。\n原码转换成补码的公式：（用“|“来分隔每个字节，8位）\n\n当原码为正数时，反码和补码与原码相同。\n\n正数：1原码：0000 0000 | 0000 0000 | 0000 0000 | 0000 0001反码：0000 0000 | 0000 0000 | 0000 0000 | 0000 0001补码：0000 0000 | 0000 0000 | 0000 0000 | 0000 0001\n\n\n当原码为负数时，反码为其绝对值按位全部取反（包括符号位），补码为反码加1。\n\n负数：-1原码：1000 0000 | 0000 0000 | 0000 0000 | 0000 0001反码：1111 1111 | 1111 1111 | 1111 1111 | 1111 1110补码：1111 1111 | 1111 1111 | 1111 1111 | 1111 1111\n\n\n因此在程序中，我们定义16进制整形数时，0x00000001表示1，0xffffffff表示-1。\nInteger i = 0x00000001;System.out.println(i);1Integer j = 0xffffffff;System.out.println(j);-1\n\n最大值为什么是 2^31-1，而不是 2^31计算机中可表示的整数最大值的补码为 0111 1111 | 1111 1111 | 1111 1111 | 1111 1111 (0x7fffffff)，正数的补码与原码一致，转换为10进制数为2^31-1 &#x3D; 2147483647\nInteger k = Integer.valueOf(&quot;01111111111111111111111111111111&quot;, 2);System.out.println(k);2147483647\n\n最小值为什么是 -2^31，而不是 -（2^31-1）我们依次推算负数值，如下：\n负数：-1原码：1000 0000 | 0000 0000 | 0000 0000 | 0000 0001反码：1111 1111 | 1111 1111 | 1111 1111 | 1111 1110补码：1111 1111 | 1111 1111 | 1111 1111 | 1111 1111\n负数：-2原码：1000 0000 | 0000 0000 | 0000 0000 | 0000 0010反码：1111 1111 | 1111 1111 | 1111 1111 | 1111 1101补码：1111 1111 | 1111 1111 | 1111 1111 | 1111 1110\n\n观察补码的变化规律，可以推断最小的补码为：1000 0000 | 0000 0000 | 0000 0000 | 0000 0000(0x80000000)反码为被码-1（符号位除外）1111 1111 | 1111 1111 | 1111 1111 | 1111 1111负数的原码为反码除符号位取反1000 0000 | 0000 0000 | 0000 0000 | 0000 0000为-0，约定为-2^31 &#x3D; -2147483648\n","categories":["Java"],"tags":["java"]},{"title":"安装Git","url":"/2018/03/%E5%AE%89%E8%A3%85Git/","content":"#在Mac OS X上安装Git\n在 Mac 上安装 Git 有两种方式。最容易的当属使用图形化的 Git 安装工具，界面如图 1-7，下载地址在：http://sourceforge.net/projects/git-osx-installer/\n另一种是通过 MacPorts (http://www.macports.org) 安装。如果已经装好了 MacPorts，用下面的命令安装 Git：\n$ sudo port install git-core +svn +doc +bash_completion +gitweb\n\n这种方式就不需要再自己安装依赖库了，Macports 会帮你搞定这些麻烦事。一般上面列出的安装选项已经够用，要是你想用 Git 连接 Subversion 的代码仓库，还可以加上 +svn 选项，具体将在第八章作介绍。（译注：还有一种是使用 homebrew（https://github.com/mxcl/homebrew) ：brew install git。）\n安装完成后，还需要最后一步设置，在命令行输入：\n$ git config --global user.name &quot;Your Name&quot;$ git config --global user.email &quot;email@example.com&quot;\n\n因为Git是分布式版本控制系统，所以，每个机器都必须自报家门：你的名字和Email地址。你也许会担心，如果有人故意冒充别人怎么办？这个不必担心，首先我们相信大家都是善良无知的群众，其次，真的有冒充的也是有办法可查的。\n注意git config命令的–global参数，用了这个参数，表示你这台机器上所有的Git仓库都会使用这个配置，当然也可以对某个仓库指定不同的用户名和Email地址。\n","categories":["Tools","Git"],"tags":["git","github","gitlab","reset","commit"]},{"title":"将Centos7的yum源更换为国内阿里云的源","url":"/2018/06/%E5%B0%86Centos7%E7%9A%84yum%E6%BA%90%E6%9B%B4%E6%8D%A2%E4%B8%BA%E5%9B%BD%E5%86%85%E9%98%BF%E9%87%8C%E4%BA%91%E7%9A%84%E6%BA%90/","content":"CentOS7默认的yum源的地址是http://mirror.centos.org，需要把其换成国内的镜像，这样访问速度会更加快一些。\n\n备份系统默认的CentOS-Base.repo\n\n$ mv /etc/yum.repos.d/CentOS-Base.repo /etc/yum.repos.d/CentOS-Base.repo.backup\n\n\n下载阿里云的repo文件\n\n$ cd /etc/yum.repos.d/$ wget -c http://mirrors.aliyun.com/repo/Centos-7.repo\n\n\n生成缓存\n\n$ yum makecache已加载插件：fastestmirror, langpackshttp://mirrors.aliyuncs.com/centos/7/os/x86_64/repodata/repomd.xml: [Errno 12] Timeout on http://mirrors.aliyuncs.com/centos/7/os/x86_64/repodata/repomd.xml: (28, &#x27;Connection timed out after 30181 milliseconds&#x27;)正在尝试其它镜像。base                                                     | 3.6 kB     00:00     extras                                                   | 3.4 kB     00:00     updates                                                  | 3.4 kB     00:00     (1/5): extras/7/x86_64/filelists_db                        | 519 kB   00:00     (2/5): extras/7/x86_64/other_db                            |  95 kB   00:00     (3/5): updates/7/x86_64/filelists_db                       | 1.3 MB   00:01     (4/5): base/7/x86_64/filelists_db                          | 6.9 MB   00:03     (5/5): extras/7/x86_64/prestodelta                         |  48 kB   00:11     Loading mirror speeds from cached hostfile * base: mirrors.aliyun.com * extras: mirrors.aliyun.com * updates: mirrors.aliyun.com元数据缓存已建立"},{"title":"MacOS X安装MySQL","url":"/2018/05/MacOS-X%E5%AE%89%E8%A3%85MySQL/","content":"本文记录的是通过homebrew安装MySQL的过程\n$ brew install mysql  // 安装$ mysql.server start // 启动\n\n$ mysql_secure_installation // 配置向导Securing the MySQL server deployment.Connecting to MySQL using a blank password.VALIDATE PASSWORD PLUGIN can be used to test passwordsand improve security. It checks the strength of passwordand allows the users to set only those passwords which aresecure enough. Would you like to setup VALIDATE PASSWORD plugin?Press y|Y for Yes, any other key for No: nPlease set the password for root here.New password: ******Re-enter new password: ******By default, a MySQL installation has an anonymous user,allowing anyone to log into MySQL without having to havea user account created for them. This is intended only fortesting, and to make the installation go a bit smoother.You should remove them before moving into a productionenvironment.Remove anonymous users? (Press y|Y for Yes, any other key for No) : ySuccess.Normally, root should only be allowed to connect from&#x27;localhost&#x27;. This ensures that someone cannot guess atthe root password from the network.Disallow root login remotely? (Press y|Y for Yes, any other key for No) : n ... skipping.By default, MySQL comes with a database named &#x27;test&#x27; thatanyone can access. This is also intended only for testing,and should be removed before moving into a productionenvironment.Remove test database and access to it? (Press y|Y for Yes, any other key for No) : y - Dropping test database...Success. - Removing privileges on test database...Success.Reloading the privilege tables will ensure that all changesmade so far will take effect immediately.Reload privilege tables now? (Press y|Y for Yes, any other key for No) : ySuccess.All done! \n\n查看配置文件my.cnf的位置\n$ mysqld --help --verbose | less...Default options are read from the following files in the given order:/etc/my.cnf /etc/mysql/my.cnf /usr/local/etc/my.cnf ~/.my.cnf ...\n\n我们发现，在&#x2F;etc&#x2F;my.cnf文件并不存在。网上很多人说通过\n$ ls $(brew --prefix mysql)/support-files/my-*\n能查看到配置文件，然而并不能。当前我的版本是：\n$ brew info mysqlmysql: stable 5.7.18 (bottled)Open source relational database management systemhttps://dev.mysql.com/doc/refman/5.7/en/Conflicts with: mariadb, mariadb-connector-c, mysql-cluster, mysql-connector-c, percona-server/usr/local/Cellar/mysql/5.7.18_1 (320 files, 232.9MB) *  Poured from bottle on 2018-05-31 at 11:19:46From: https://github.com/Homebrew/homebrew-core/blob/master/Formula/mysql.rb\n\nmy.cnf配置文件的正确位置是在&#x2F;usr&#x2F;local&#x2F;etc&#x2F;my.cnf\n","categories":["Database","MySQL"],"tags":["mysql","mac"]},{"title":"Mac OS X系统Python3连接Mysql","url":"/2018/05/Mac-OS-X%E7%B3%BB%E7%BB%9FPython3%E8%BF%9E%E6%8E%A5Mysql/","content":"环境OS: MacOS Sierra 10.12.6Python: 3.6.5\n步骤python3连接Mysql用得多的两个库分别是PyMySQL和mysqlclient-python，两个库的作者都是同一人。PyMySQL安装起来相对简单，但实际应用中，mysqlclient-python运行速度更快。\n升级pip版本，如果不升级系统都会提示你升级的\n$ pip3 install --upgrade pip\n安装PyMySQL$ pip3 install pymysqlCollecting pymysql  Using cached https://files.pythonhosted.org/packages/32/e8/222d9e1c7821f935d6dba8d4c60b9985124149b35a9f93a84f0b98afc219/PyMySQL-0.8.1-py2.py3-none-any.whlInstalling collected packages: pymysqlSuccessfully installed pymysql-0.8.1\n\n安装mysqlclient-python参考官方文档https://github.com/PyMySQL/mysqlclient-python\n$ brew install mysql-connector-c$ sudo pip3 install mysqlclient\n\n结果报错：\nThe directory &#x27;/Users/simon/Library/Caches/pip/http&#x27; or its parent directory is not owned by the current user and the cache has been disabled. Please check the permissions and owner of that directory. If executing pip with sudo, you may want sudo&#x27;s -H flag.The directory &#x27;/Users/simon/Library/Caches/pip&#x27; or its parent directory is not owned by the current user and caching wheels has been disabled. check the permissions and owner of that directory. If executing pip with sudo, you may want sudo&#x27;s -H flag.Collecting mysqlclient  Downloading https://files.pythonhosted.org/packages/6f/86/bad31f1c1bb0cc99e88ca2adb7cb5c71f7a6540c1bb001480513de76a931/mysqlclient-1.3.12.tar.gz (89kB)    100% |████████████████████████████████| 92kB 13kB/s     Complete output from command python setup.py egg_info:    Traceback (most recent call last):      File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt;      File &quot;/private/tmp/pip-install-bqcij69j/mysqlclient/setup.py&quot;, line 17, in &lt;module&gt;        metadata, options = get_config()      File &quot;/private/tmp/pip-install-bqcij69j/mysqlclient/setup_posix.py&quot;, line 54, in get_config        libraries = [dequote(i[2:]) for i in libs if i.startswith(&#x27;-l&#x27;)]      File &quot;/private/tmp/pip-install-bqcij69j/mysqlclient/setup_posix.py&quot;, line 54, in &lt;listcomp&gt;        libraries = [dequote(i[2:]) for i in libs if i.startswith(&#x27;-l&#x27;)]      File &quot;/private/tmp/pip-install-bqcij69j/mysqlclient/setup_posix.py&quot;, line 12, in dequote        if s[0] in &quot;\\&quot;&#x27;&quot; and s[0] == s[-1]:    IndexError: string index out of range        ----------------------------------------Command &quot;python setup.py egg_info&quot; failed with error code 1 in /private/tmp/pip-install-bqcij69j/mysqlclient/\n\n需要说明的一点就是，MySQL Connector&#x2F;C（6.1.10）在macOS是存在Bug的。\n需要修改mysql_config文件\n把文件\n# on macOS, on or about line 112:# Create options libs=&quot;-L$pkglibdir&quot;libs=&quot;$libs -l &quot;\n修改为\n# Create options libs=&quot;-L$pkglibdir&quot;libs=&quot;$libs -lmysqlclient -lssl -lcrypto&quot;\n\n再次安装，继续报错：\nThe directory &#x27;/Users/simon/Library/Caches/pip/http&#x27; or its parent directory is not owned by the current user and the cache has been disabled. Please check the permissions and owner of that directory. If executing pip with sudo, you may want sudo&#x27;s -H flag.The directory &#x27;/Users/simon/Library/Caches/pip&#x27; or its parent directory is not owned by the current user and caching wheels has been disabled. check the permissions and owner of that directory. If executing pip with sudo, you may want sudo&#x27;s -H flag.Collecting mysqlclient  Downloading https://files.pythonhosted.org/packages/6f/86/bad31f1c1bb0cc99e88ca2adb7cb5c71f7a6540c1bb001480513de76a931/mysqlclient-1.3.12.tar.gz (89kB)    100% |████████████████████████████████| 92kB 122kB/s Installing collected packages: mysqlclient  Running setup.py install for mysqlclient ... error    Complete output from command /Library/Frameworks/Python.framework/Versions/3.6/bin/python3.6 -u -c &quot;import setuptools, tokenize;__file__=&#x27;/private/tmp/pip-install-l1_o52yl/mysqlclient/setup.py&#x27;;f=getattr(tokenize, &#x27;open&#x27;, open)(__file__);code=f.read().replace(&#x27;\\r\\n&#x27;, &#x27;\\n&#x27;);f.close();exec(compile(code, __file__, &#x27;exec&#x27;))&quot; install --record /private/tmp/pip-record-zkfu3qn7/install-record.txt --single-version-externally-managed --compile:    running install    running build    running build_py    creating build    creating build/lib.macosx-10.9-x86_64-3.6    copying _mysql_exceptions.py -&gt; build/lib.macosx-10.9-x86_64-3.6    creating build/lib.macosx-10.9-x86_64-3.6/MySQLdb    copying MySQLdb/__init__.py -&gt; build/lib.macosx-10.9-x86_64-3.6/MySQLdb    copying MySQLdb/compat.py -&gt; build/lib.macosx-10.9-x86_64-3.6/MySQLdb    copying MySQLdb/connections.py -&gt; build/lib.macosx-10.9-x86_64-3.6/MySQLdb    copying MySQLdb/converters.py -&gt; build/lib.macosx-10.9-x86_64-3.6/MySQLdb    copying MySQLdb/cursors.py -&gt; build/lib.macosx-10.9-x86_64-3.6/MySQLdb    copying MySQLdb/release.py -&gt; build/lib.macosx-10.9-x86_64-3.6/MySQLdb    copying MySQLdb/times.py -&gt; build/lib.macosx-10.9-x86_64-3.6/MySQLdb    creating build/lib.macosx-10.9-x86_64-3.6/MySQLdb/constants    copying MySQLdb/constants/__init__.py -&gt; build/lib.macosx-10.9-x86_64-3.6/MySQLdb/constants    copying MySQLdb/constants/CLIENT.py -&gt; build/lib.macosx-10.9-x86_64-3.6/MySQLdb/constants    copying MySQLdb/constants/CR.py -&gt; build/lib.macosx-10.9-x86_64-3.6/MySQLdb/constants    copying MySQLdb/constants/ER.py -&gt; build/lib.macosx-10.9-x86_64-3.6/MySQLdb/constants    copying MySQLdb/constants/FIELD_TYPE.py -&gt; build/lib.macosx-10.9-x86_64-3.6/MySQLdb/constants    copying MySQLdb/constants/FLAG.py -&gt; build/lib.macosx-10.9-x86_64-3.6/MySQLdb/constants    copying MySQLdb/constants/REFRESH.py -&gt; build/lib.macosx-10.9-x86_64-3.6/MySQLdb/constants    running build_ext    building &#x27;_mysql&#x27; extension    creating build/temp.macosx-10.9-x86_64-3.6    gcc -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -arch x86_64 -g -Dversion_info=(1,3,12,&#x27;final&#x27;,0) -D__version__=1.3.12 -I/usr/local/Cellar/mysql-connector-c/6.1.10/include -I/Library/Frameworks/Python.framework/Versions/3.6/include/python3.6m -c _mysql.c -o build/temp.macosx-10.9-x86_64-3.6/_mysql.o    gcc -bundle -undefined dynamic_lookup -arch x86_64 -g build/temp.macosx-10.9-x86_64-3.6/_mysql.o -L/usr/local/Cellar/mysql-connector-c/6.1.10/lib -lmysqlclient -lssl -lcrypto -o build/lib.macosx-10.9-x86_64-3.6/_mysql.cpython-36m-darwin.so    ld: library not found for -lssl    clang: error: linker command failed with exit code 1 (use -v to see invocation)    error: command &#x27;gcc&#x27; failed with exit status 1        ----------------------------------------Command &quot;/Library/Frameworks/Python.framework/Versions/3.6/bin/python3.6 -u -c &quot;import setuptools, tokenize;__file__=&#x27;/private/tmp/pip-install-l1_o52yl/mysqlclient/setup.py&#x27;;f=getattr(tokenize, &#x27;open&#x27;, open)(__file__);code=f.read().replace(&#x27;\\r\\n&#x27;, &#x27;\\n&#x27;);f.close();exec(compile(code, __file__, &#x27;exec&#x27;))&quot; install --record /private/tmp/pip-record-zkfu3qn7/install-record.txt --single-version-externally-managed --compile&quot; failed with error code 1 in /private/tmp/pip-install-l1_o52yl/mysqlclient/\n\n很显然，系统提示：ld: library not found for -lssl。作者也告诉了这个错误的解决办法，办法就是brew info openssl，说了相当于没说。\n$ brew info openssl...For compilers to find this software you may need to set:    LDFLAGS:  -L/usr/local/opt/openssl/lib    CPPFLAGS: -I/usr/local/opt/openssl/include\n\n看明白了吗？在编译时，需要加入LDFLAGS这个参数。\n最后，我们执行\n$ sudo LDFLAGS=-L/usr/local/opt/openssl/lib pip3 install mysqlclientThe directory &#x27;/Users/simon/Library/Caches/pip/http&#x27; or its parent directory is not owned by the current user and the cache has been disabled. Please check the permissions and owner of that directory. If executing pip with sudo, you may want sudo&#x27;s -H flag.The directory &#x27;/Users/simon/Library/Caches/pip&#x27; or its parent directory is not owned by the current user and caching wheels has been disabled. check the permissions and owner of that directory. If executing pip with sudo, you may want sudo&#x27;s -H flag.Collecting mysqlclient  Downloading https://files.pythonhosted.org/packages/6f/86/bad31f1c1bb0cc99e88ca2adb7cb5c71f7a6540c1bb001480513de76a931/mysqlclient-1.3.12.tar.gz (89kB)    100% |████████████████████████████████| 92kB 40kB/s Installing collected packages: mysqlclient  Running setup.py install for mysqlclient ... doneSuccessfully installed mysqlclient-1.3.12\n\nmysqlclient安装成功。\n","categories":["Python"]},{"title":"Set和List的集合操作","url":"/2018/04/Set%E5%92%8CList%E7%9A%84%E9%9B%86%E5%90%88%E6%93%8D%E4%BD%9C/","content":"在这里，我们通过程序来验证一下，Set集合和List的集合的各类集合操作各有什么不同。\nimport java.util.ArrayList;import java.util.HashSet;import java.util.List;import java.util.Set;public class CollectionTest &#123;    public static void main(String[] args) &#123;        Set&lt;Integer&gt; set1 = new HashSet&lt;Integer&gt;() &#123;            &#123;                add(1);                add(3);                add(5);            &#125;        &#125;;        Set&lt;Integer&gt; set2 = new HashSet&lt;Integer&gt;() &#123;            &#123;                add(1);                add(2);                add(3);            &#125;        &#125;;        Set&lt;Integer&gt; result = new HashSet&lt;&gt;();        result.addAll(set1);        System.out.println(&quot;set1：&quot; + set1);        System.out.println(&quot;set2：&quot; + set2);        result.retainAll(set2);        System.out.println(&quot;set1与set2的交集是: &quot; + result);        result.clear();        result.addAll(set2);        result.removeAll(set1);        System.out.println(&quot;set2与set1的差集是: &quot; + result);        result.clear();        result.addAll(set1);        result.addAll(set2);        System.out.println(&quot;set1和set2的并集: &quot; + result);        System.out.println(&quot;set集合并集，是去重复的&quot;);        // 测试List集合        List&lt;Integer&gt; list1 = new ArrayList&lt;Integer&gt;() &#123;&#123;            add(1);            add(3);            add(5);        &#125;&#125;;        List&lt;Integer&gt; list2 = new ArrayList&lt;Integer&gt;() &#123;            &#123;                add(1);                add(2);                add(3);            &#125;        &#125;;        List&lt;Integer&gt; list = new ArrayList&lt;&gt;();        list.addAll(list1);        System.out.println(&quot;list1: &quot; + list1);        System.out.println(&quot;list2: &quot; + list2);        list.retainAll(list2);        System.out.println(&quot;list1与list2的交集是: &quot; + list);        list.clear();        list.addAll(list2);        list.removeAll(list1);        System.out.println(&quot;list2与list1的差集是：&quot; + list);        list.clear();        list.addAll(list1);        list.addAll(list2);        System.out.println(&quot;list1和list2的并集: &quot; + list);        System.out.println(&quot;List集合并集, 是不去重复的&quot;);    &#125;&#125;set1：[1, 3, 5]set2：[1, 2, 3]set1与set2的交集是: [1, 3]set2与set1的差集是: [2]set1和set2的并集: [1, 2, 3, 5]set集合并集，是去重复的list1: [1, 3, 5]list2: [1, 2, 3]list1与list2的交集是: [1, 3]list2与list1的差集是：[2]list1和list2的并集: [1, 3, 5, 1, 2, 3]List集合并集, 是不去重复的\n\n结论：Set集合的并集，是要去重的。而List的并集，是不会去重的。\n","categories":["Java"],"tags":["java"]},{"title":"SpringMVC使用ResponseBody返回json时中文乱码","url":"/2018/05/SpringMVC%E4%BD%BF%E7%94%A8ResponseBody%E8%BF%94%E5%9B%9Ejson%E6%97%B6%E4%B8%AD%E6%96%87%E4%B9%B1%E7%A0%81/","content":"在SpringMVC中的Controller中，返回非String类时，不会遇到这种情况。但返回为String时，中文有可能会变成乱码。\npublic class StringHttpMessageConverter extends AbstractHttpMessageConverter&lt;String&gt; &#123;    public static final Charset DEFAULT_CHARSET = Charset.forName(&quot;ISO-8859-1&quot;);\n\n转换器的默认编码是ISO-8859-1，而非UTF-8。\n解决办法\n\n使用（produces &#x3D; “application&#x2F;json; charset&#x3D;utf-8”）\n\n@RequestMapping(value = &quot;/xxx&quot;, produces = &quot;produces=MediaType.APPLICATION_JSON_VALUE + &quot;;charset=utf-8&quot;)@ResponseBodypublic String fun() &#123;&#125;\n\n\n在spring-mvc.xml中添加：\n\n&lt;!-- 处理请求返回json字符串的中文乱码问题 --&gt;&lt;mvc:annotation-driven&gt;     &lt;mvc:message-converters&gt;        &lt;bean class=&quot;org.springframework.http.converter.StringHttpMessageConverter&quot;&gt;             &lt;property name=&quot;supportedMediaTypes&quot;&gt;                 &lt;list&gt;                     &lt;value&gt;application/json;charset=UTF-8&lt;/value&gt;                 &lt;/list&gt;             &lt;/property&gt;        &lt;/bean&gt;    &lt;/mvc:message-converters&gt;&lt;/mvc:annotation-driven&gt;\n","categories":["Java","SpringMVC"],"tags":["java","springmvc","json"]},{"title":"java多线程编程（初级）","url":"/2018/04/java%E5%A4%9A%E7%BA%BF%E7%A8%8B%E7%BC%96%E7%A8%8B%EF%BC%88%E5%88%9D%E7%BA%A7%EF%BC%89/","content":"Java 给多线程编程提供了内置的支持。 一条线程指的是进程中一个单一顺序的控制流，一个进程中可以并发多个线程，每条线程并行执行不同的任务。\n多线程是多任务的一种特别的形式，但多线程使用了更小的资源开销。\n这里定义和线程相关的另一个术语 - 进程：一个进程包括由操作系统分配的内存空间，包含一个或多个线程。一个线程不能独立的存在，它必须是进程的一部分。一个进程一直运行，直到所有的非守护线程都结束运行后才能结束。\n多线程能满足程序员编写高效率的程序来达到充分利用 CPU 的目的。\n一个线程的生命周期线程是一个动态执行的过程，它也有一个从产生到死亡的过程。\n下图显示了一个线程完整的生命周期。\n\n\n\n新建状态使用 new 关键字和 Thread 类或其子类建立一个线程对象后，该线程对象就处于新建状态。它保持这个状态直到程序 start() 这个线程。\n\n就绪状态当线程对象调用了start()方法之后，该线程就进入就绪状态。就绪状态的线程处于就绪队列中，要等待JVM里线程调度器的调度。\n\n运行状态如果就绪状态的线程获取 CPU 资源，就可以执行 run()，此时线程便处于运行状态。处于运行状态的线程最为复杂，它可以变为阻塞状态、就绪状态和死亡状态。\n\n阻塞状态如果一个线程执行了sleep（睡眠）、suspend（挂起）等方法，失去所占用资源之后，该线程就从运行状态进入阻塞状态。在睡眠时间已到或获得设备资源后可以重新进入就绪状态。可以分为三种：\n\n等待阻塞 运行状态中的线程执行 wait() 方法，使线程进入到等待阻塞状态。\n\n同步阻塞 线程在获取 synchronized 同步锁失败(因为同步锁被其他线程占用)。\n\n其他阻塞 通过调用线程的 sleep() 或 join() 发出了 I&#x2F;O 请求时，线程就会进入到阻塞状态。当sleep() 状态超时，join() 等待线程终止或超时，或者 I&#x2F;O 处理完毕，线程重新转入就绪状态。\n\n死亡状态 一个运行状态的线程完成任务或者其他终止条件发生时，该线程就切换到终止状态。\n\n\n线程的优先级每一个 Java 线程都有一个优先级，这样有助于操作系统确定线程的调度顺序。\nJava 线程的优先级是一个整数，其取值范围是 1 （Thread.MIN_PRIORITY ） - 10 （Thread.MAX_PRIORITY ）。\n默认情况下，每一个线程都会分配一个优先级 NORM_PRIORITY（5）。\n具有较高优先级的线程对程序更重要，并且应该在低优先级的线程之前分配处理器资源。但是，线程优先级不能保证线程执行的顺序，而且非常依赖于平台。\n创建一个线程Java 提供了三种创建线程的方法：\n\n通过实现 Runnable 接口；\n通过继承 Thread 类本身；\n通过 Callable 和 Future 创建线程。\n\n通过实现 Runnable 接口来创建线程创建一个线程，最简单的方法是创建一个实现 Runnable 接口的类。\n为了实现 Runnable，一个类只需要执行一个方法调用 run()，声明如下：\npublic void run()\n\n你可以重写该方法，重要的是理解的 run() 可以调用其他方法，使用其他类，并声明变量，就像主线程一样。\n在创建一个实现 Runnable 接口的类之后，你可以在类中实例化一个线程对象。\nThread 定义了几个构造方法，下面的这个是我们经常使用的：\npublic Thread(Runnable target, String name);\n\n这里，target 是一个实现 Runnable 接口的类的实例，并且 name 指定新线程的名字。\n新线程创建之后，你调用它的 start() 方法它才会运行。\npublic synchronized void start()\n\n下面是一个创建线程并开始让它执行的实例：\nTestThread.java\nclass RunnableDemo implements Runnable &#123;    private Thread t;    private String threadName;    RunnableDemo( String name) &#123;        threadName = name;        System.out.println(&quot;Creating &quot; +  threadName );    &#125;    public void run() &#123;        System.out.println(&quot;Running &quot; +  threadName );        try &#123;            for(int i = 4; i &gt; 0; i--) &#123;                System.out.println(&quot;Thread: &quot; + threadName + &quot;, &quot; + i);                // 让线程睡眠一会                Thread.sleep(50);            &#125;        &#125;catch (InterruptedException e) &#123;            System.out.println(&quot;Thread &quot; +  threadName + &quot; interrupted.&quot;);        &#125;        System.out.println(&quot;Thread &quot; +  threadName + &quot; exiting.&quot;);    &#125;    public void start () &#123;        System.out.println(&quot;Starting &quot; +  threadName );        if (t == null) &#123;            t = new Thread (this, threadName);            t.start ();        &#125;    &#125;&#125;public class TestThread &#123;    public static void main(String args[]) &#123;        RunnableDemo r1 = new RunnableDemo( &quot;Thread-1&quot;);        r1.start();        RunnableDemo r2 = new RunnableDemo( &quot;Thread-2&quot;);        r2.start();    &#125;&#125;\n\n输出结果\nCreating Thread-1Starting Thread-1Creating Thread-2Starting Thread-2Running Thread-1Thread: Thread-1, 4Running Thread-2Thread: Thread-2, 4Thread: Thread-1, 3Thread: Thread-2, 3Thread: Thread-1, 2Thread: Thread-2, 2Thread: Thread-1, 1Thread: Thread-2, 1Thread Thread-1 exiting.Thread Thread-2 exiting.\n\n通过继承Thread来创建线程创建一个线程的第二种方法是创建一个新的类，该类继承 Thread 类，然后创建一个该类的实例。\n继承类必须重写 run() 方法，该方法是新线程的入口点。它也必须调用 start() 方法才能执行。\n该方法尽管被列为一种多线程实现方式，但是本质上也是实现了 Runnable 接口的一个实例。\nThread 方法下表列出了Thread类的一些重要方法：\n\npublic void start()使该线程开始执行；Java 虚拟机调用该线程的 run 方法。\n\npublic void run()如果该线程是使用独立的 Runnable 运行对象构造的，则调用该 Runnable 对象的 run 方法；否则，该方法不执行任何操作并返回。\n\npublic final void setName(String name)改变线程名称，使之与参数 name 相同。\n\npublic final void setPriority(int priority) 更改线程的优先级。\n\npublic final void setDaemon(boolean on)将该线程标记为守护线程或用户线程。\n\npublic final void join(long millisec)等待该线程终止的时间最长为 millis 毫秒。\n\npublic void interrupt()中断线程。\n\npublic final boolean isAlive()测试线程是否处于活动状态。\n\n\n上述方法是被Thread对象调用的。下面的方法是Thread类的静态方法。\n\npublic static void yield()暂停当前正在执行的线程对象，并执行其他线程。\n\npublic static void sleep(long millisec)在指定的毫秒数内让当前正在执行的线程休眠（暂停执行），此操作受到系统计时器和调度程序精度和准确性的影响。\n\n\n yield和sleep的区别sleep()方法会给其他线程运行的机会，而不考虑其他线程的优先级，因此会给较低线程一个运行的机会;yield()方法只会给相同优先级或者更高优先级的线程一个运行的机会。\n 当线程执行了sleep(long millis)方法后，将转到阻塞状态，参数millis指定睡眠时间;当线程执行了yield()方法后，将转到就绪状态。\n\npublic static boolean holdsLock(Object x)当且仅当当前线程在指定的对象上保持监视器锁时，才返回 true。\n\npublic static Thread currentThread()返回对当前正在执行的线程对象的引用。\n\npublic static void dumpStack()将当前线程的堆栈跟踪打印至标准错误流。\n\n\n实例public class ThreadClassDemo &#123;    public static void main(String [] args) &#123;        Runnable hello = new DisplayMessage(&quot;Hello&quot;);        Thread thread1 = new Thread(hello);        thread1.setDaemon(true);        thread1.setName(&quot;hello&quot;);        System.out.println(&quot;Starting hello thread...&quot;);        thread1.start();        Runnable bye = new DisplayMessage(&quot;Goodbye&quot;);        Thread thread2 = new Thread(bye);        thread2.setPriority(Thread.MIN_PRIORITY);        thread2.setDaemon(true);        System.out.println(&quot;Starting goodbye thread...&quot;);        thread2.start();        System.out.println(&quot;Starting thread3...&quot;);        Thread thread3 = new GuessANumber(27);        thread3.start();        try &#123;            thread3.join();        &#125;catch(InterruptedException e) &#123;            System.out.println(&quot;Thread interrupted.&quot;);        &#125;        System.out.println(&quot;Starting thread4...&quot;);        Thread thread4 = new GuessANumber(75);        thread4.start();        System.out.println(&quot;main() is ending...&quot;);    &#125;&#125;class DisplayMessage implements Runnable &#123;    private String message;    public DisplayMessage(String message) &#123;        this.message = message;    &#125;    public void run() &#123;        while(true) &#123;            System.out.println(message);        &#125;    &#125;&#125;class GuessANumber extends Thread &#123;    private int number;    public GuessANumber(int number) &#123;        this.number = number;    &#125;    public void run() &#123;        int counter = 0;        int guess = 0;        do &#123;            guess = (int) (Math.random() * 100 + 1);            System.out.println(this.getName() + &quot; guesses &quot; + guess);            counter++;        &#125; while(guess != number);        System.out.println(&quot;** Correct!&quot; + this.getName() + &quot;in&quot; + counter + &quot;guesses.**&quot;);    &#125;&#125;Starting hello thread...Starting goodbye thread...HelloHelloHelloHelloHelloHelloGoodbyeGoodbyeGoodbyeGoodbyeGoodbye.......\n\n通过 Callable 和 Future 创建线程\n创建 Callable 接口的实现类，并实现 call() 方法，该 call() 方法将作为线程执行体，并且有返回值。\n\n创建 Callable 实现类的实例，使用 FutureTask 类来包装 Callable 对象，该 FutureTask 对象封装了该 Callable 对象的 call() 方法的返回值。\n\n使用 FutureTask 对象作为 Thread 对象的 target 创建并启动新线程。\n\n调用 FutureTask 对象的 get() 方法来获得子线程执行结束后的返回值。\n\n\npublic class CallableThreadTest implements Callable&lt;Integer&gt; &#123;    public static void main(String[] args)      &#123;          CallableThreadTest ctt = new CallableThreadTest();          FutureTask&lt;Integer&gt; ft = new FutureTask&lt;&gt;(ctt);          for(int i = 0;i &lt; 100;i++)          &#123;              System.out.println(Thread.currentThread().getName()+&quot; 的循环变量i的值&quot;+i);              if(i==20)              &#123;                  new Thread(ft,&quot;有返回值的线程&quot;).start();              &#125;          &#125;          try          &#123;              System.out.println(&quot;子线程的返回值：&quot;+ft.get());          &#125; catch (InterruptedException e)          &#123;              e.printStackTrace();          &#125; catch (ExecutionException e)          &#123;              e.printStackTrace();          &#125;        &#125;    @Override      public Integer call() throws Exception      &#123;          int i = 0;          for(;i&lt;100;i++)          &#123;              System.out.println(Thread.currentThread().getName()+&quot; &quot;+i);          &#125;          return i;      &#125;  &#125;\n\n创建线程的三种方式的对比\n采用实现 Runnable、Callable 接口的方式创建多线程时，线程类只是实现了 Runnable 接口或 Callable 接口，还可以继承其他类。\n\n使用继承 Thread 类的方式创建多线程时，编写简单，如果需要访问当前线程，则无需使用 Thread.currentThread() 方法，直接使用 this 即可获得当前线程。\n\n\n线程的几个主要概念在多线程编程时，你需要了解以下几个概念：\n线程同步线程间通信线程死锁线程控制：挂起、停止和恢复\n多线程的使用有效利用多线程的关键是理解程序是并发执行而不是串行执行的。例如：程序中有两个子系统需要并发执行，这时候就需要利用多线程编程。\n通过对多线程的使用，可以编写出非常高效的程序。不过请注意，如果你创建太多的线程，程序执行的效率实际上是降低了，而不是提升了。\n请记住，上下文的切换开销也很重要，如果你创建了太多的线程，CPU 花费在上下文的切换的时间将多于执行程序的时间！\n","categories":["Java","Thread"],"tags":["java","thread"]},{"title":"CentOS 7 如何关机重启","url":"/2018/04/CentOS-7-%E5%A6%82%E4%BD%95%E5%85%B3%E6%9C%BA%E9%87%8D%E5%90%AF/","content":"如果要关机，必须保证当前系统中没有其他用户在登录系统。我们可以使用who命令查看是否还有其他人登录，或者使用命令ps -aux查看是否还有后台进程运行。\nshutdown，halt，poweroff都为关机的命令，我们可以使用命令man shutdown查看其帮助文档。例如，我们运行如下命令关机。\nshutdown -h 10        #计算机将于10分钟后关闭，且会显示在登录用户的当前屏幕中shutdown -h now       #计算机会立刻关机shutdown -h 22:22     #计算机会在这个时刻关机shutdown -r now       #计算机会立刻重启shutdown -r +10       #计算机会将于10分钟后重启reboot                #重启halt                  #关机\n","categories":["OS"]},{"title":"macOS安装Homebrew","url":"/2018/07/macOS%E5%AE%89%E8%A3%85Homebrew/","content":"Homebrew的官网：https://brew.sh/\n安装命令\n$ /usr/bin/ruby -e &quot;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)&quot;==&gt; This script will install:/usr/local/bin/brew/usr/local/share/doc/homebrew/usr/local/share/man/man1/brew.1/usr/local/share/zsh/site-functions/_brew/usr/local/etc/bash_completion.d/brew/usr/local/HomebrewPress RETURN to continue or any other key to abort==&gt; Downloading and installing Homebrew...remote: Counting objects: 105993, done.remote: Compressing objects: 100% (45/45), done.remote: Total 105993 (delta 21), reused 26 (delta 9), pack-reused 105937Receiving objects: 100% (105993/105993), 24.07 MiB | 12.00 KiB/s, done.Resolving deltas: 100% (77327/77327), done.From https://github.com/Homebrew/brew * [new branch]          master     -&gt; origin/master * [new tag]             0.1        -&gt; 0.1 * [new tag]             0.2        -&gt; 0.2 * [new tag]             0.3        -&gt; 0.3 * [new tag]             0.4        -&gt; 0.4 * [new tag]             0.5        -&gt; 0.5 * [new tag]             0.6        -&gt; 0.6 * [new tag]             0.7        -&gt; 0.7 * [new tag]             0.7.1      -&gt; 0.7.1 * [new tag]             0.8        -&gt; 0.8 * [new tag]             0.8.1      -&gt; 0.8.1 * [new tag]             0.9        -&gt; 0.9 * [new tag]             0.9.1      -&gt; 0.9.1 * [new tag]             0.9.2      -&gt; 0.9.2 * [new tag]             0.9.3      -&gt; 0.9.3 * [new tag]             0.9.4      -&gt; 0.9.4 * [new tag]             0.9.5      -&gt; 0.9.5 * [new tag]             0.9.8      -&gt; 0.9.8 * [new tag]             0.9.9      -&gt; 0.9.9 * [new tag]             1.0.0      -&gt; 1.0.0 * [new tag]             1.0.1      -&gt; 1.0.1 * [new tag]             1.0.2      -&gt; 1.0.2 * [new tag]             1.0.3      -&gt; 1.0.3 * [new tag]             1.0.4      -&gt; 1.0.4 * [new tag]             1.0.5      -&gt; 1.0.5 * [new tag]             1.0.6      -&gt; 1.0.6 * [new tag]             1.0.7      -&gt; 1.0.7 * [new tag]             1.0.8      -&gt; 1.0.8 * [new tag]             1.0.9      -&gt; 1.0.9 * [new tag]             1.1.0      -&gt; 1.1.0 * [new tag]             1.1.1      -&gt; 1.1.1 * [new tag]             1.1.10     -&gt; 1.1.10 * [new tag]             1.1.11     -&gt; 1.1.11 * [new tag]             1.1.12     -&gt; 1.1.12 * [new tag]             1.1.13     -&gt; 1.1.13 * [new tag]             1.1.2      -&gt; 1.1.2 * [new tag]             1.1.3      -&gt; 1.1.3 * [new tag]             1.1.4      -&gt; 1.1.4 * [new tag]             1.1.5      -&gt; 1.1.5 * [new tag]             1.1.6      -&gt; 1.1.6 * [new tag]             1.1.7      -&gt; 1.1.7 * [new tag]             1.1.8      -&gt; 1.1.8 * [new tag]             1.1.9      -&gt; 1.1.9 * [new tag]             1.2.0      -&gt; 1.2.0 * [new tag]             1.2.1      -&gt; 1.2.1 * [new tag]             1.2.2      -&gt; 1.2.2 * [new tag]             1.2.3      -&gt; 1.2.3 * [new tag]             1.2.4      -&gt; 1.2.4 * [new tag]             1.2.5      -&gt; 1.2.5 * [new tag]             1.2.6      -&gt; 1.2.6 * [new tag]             1.3.0      -&gt; 1.3.0 * [new tag]             1.3.1      -&gt; 1.3.1 * [new tag]             1.3.2      -&gt; 1.3.2 * [new tag]             1.3.3      -&gt; 1.3.3 * [new tag]             1.3.4      -&gt; 1.3.4 * [new tag]             1.3.5      -&gt; 1.3.5 * [new tag]             1.3.6      -&gt; 1.3.6 * [new tag]             1.3.7      -&gt; 1.3.7 * [new tag]             1.3.8      -&gt; 1.3.8 * [new tag]             1.3.9      -&gt; 1.3.9 * [new tag]             1.4.0      -&gt; 1.4.0 * [new tag]             1.4.1      -&gt; 1.4.1 * [new tag]             1.4.2      -&gt; 1.4.2 * [new tag]             1.4.3      -&gt; 1.4.3 * [new tag]             1.5.0      -&gt; 1.5.0 * [new tag]             1.5.1      -&gt; 1.5.1 * [new tag]             1.5.10     -&gt; 1.5.10 * [new tag]             1.5.11     -&gt; 1.5.11 * [new tag]             1.5.12     -&gt; 1.5.12 * [new tag]             1.5.13     -&gt; 1.5.13 * [new tag]             1.5.14     -&gt; 1.5.14 * [new tag]             1.5.2      -&gt; 1.5.2 * [new tag]             1.5.3      -&gt; 1.5.3 * [new tag]             1.5.4      -&gt; 1.5.4 * [new tag]             1.5.5      -&gt; 1.5.5 * [new tag]             1.5.6      -&gt; 1.5.6 * [new tag]             1.5.7      -&gt; 1.5.7 * [new tag]             1.5.8      -&gt; 1.5.8 * [new tag]             1.5.9      -&gt; 1.5.9 * [new tag]             1.6.0      -&gt; 1.6.0 * [new tag]             1.6.1      -&gt; 1.6.1 * [new tag]             1.6.10     -&gt; 1.6.10 * [new tag]             1.6.11     -&gt; 1.6.11 * [new tag]             1.6.12     -&gt; 1.6.12 * [new tag]             1.6.13     -&gt; 1.6.13 * [new tag]             1.6.14     -&gt; 1.6.14 * [new tag]             1.6.15     -&gt; 1.6.15 * [new tag]             1.6.16     -&gt; 1.6.16 * [new tag]             1.6.17     -&gt; 1.6.17 * [new tag]             1.6.2      -&gt; 1.6.2 * [new tag]             1.6.3      -&gt; 1.6.3 * [new tag]             1.6.4      -&gt; 1.6.4 * [new tag]             1.6.5      -&gt; 1.6.5 * [new tag]             1.6.6      -&gt; 1.6.6 * [new tag]             1.6.7      -&gt; 1.6.7 * [new tag]             1.6.8      -&gt; 1.6.8 * [new tag]             1.6.9      -&gt; 1.6.9 * [new tag]             1.7.0      -&gt; 1.7.0 * [new tag]             1.7.1      -&gt; 1.7.1HEAD is now at 10a7e998a Merge pull request #4567 from Homebrew/revert-4561-curl-system-command==&gt; Downloading https://homebrew.bintray.com/bottles-portable-ruby/portable-ruby-2.3.7.leopard_64.bottle.tar.gz######################################################################## 100.0%==&gt; Pouring portable-ruby-2.3.7.leopard_64.bottle.tar.gz==&gt; Tapping homebrew/coreCloning into &#x27;/usr/local/Homebrew/Library/Taps/homebrew/homebrew-core&#x27;...remote: Counting objects: 4807, done.remote: Compressing objects: 100% (4590/4590), done.remote: Total 4807 (delta 57), reused 862 (delta 26), pack-reused 0Receiving objects: 100% (4807/4807), 3.90 MiB | 14.00 KiB/s, done.Resolving deltas: 100% (57/57), done.Tapped 2 commands and 4592 formulae (4,848 files, 12.2MB).==&gt; Cleaning up /Library/Caches/Homebrew...==&gt; Migrating /Library/Caches/Homebrew to /Users/simon/Library/Caches/Homebrew...==&gt; Deleting /Library/Caches/Homebrew...Already up-to-date.==&gt; Installation successful!==&gt; Homebrew has enabled anonymous aggregate user behaviour analytics.Read the analytics documentation (and how to opt-out) here:  https://docs.brew.sh/Analytics.html==&gt; Next steps:- Run `brew help` to get started- Further documentation:     https://docs.brew.sh\n\n如果网络有问题，可以通过如下方法安装\nhttps://docs.brew.sh/Installation\n","categories":["Tools","Homebrew"],"tags":["mac","homebrew"]},{"title":"ssh连接远程服务器经常断线的解决办法","url":"/2018/04/ssh%E8%BF%9E%E6%8E%A5%E8%BF%9C%E7%A8%8B%E6%9C%8D%E5%8A%A1%E5%99%A8%E7%BB%8F%E5%B8%B8%E6%96%AD%E7%BA%BF%E7%9A%84%E8%A7%A3%E5%86%B3%E5%8A%9E%E6%B3%95/","content":"用ssh连接远程服务器，如阿里云时，如果一段时间没有在ssh终端输入命令，就会断掉，然后又得重新登录，非常麻烦。解决方案如下：\nvi /etc/ssh/sshd_config\n\n找到下面两行\n#ClientAliveInterval 0#ClientAliveCountMax 3\n\n去掉注释，改成\nClientAliveInterval 30# 客户端每隔多少秒向服务发送一个心跳数据ClientAliveCountMax 86400# 客户端多少秒没有相应，服务器自动断掉连接\n\n重启sshd服务\nservice sshd restart\n","categories":["Tools","SSH"]},{"title":"macOS安装java jdk和maven并配置环境变量","url":"/2018/07/macOS%E5%AE%89%E8%A3%85java-jdk%E5%92%8Cmaven%E5%B9%B6%E9%85%8D%E7%BD%AE%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F/","content":"安装Java下载jdk网址：http://www.oracle.com/technetwork/java/javase/downloads/index.html下载文件：jdk-8u181-macosx-x64.dmg，双击安装\n安装maven下载maven压缩文件网址：http://maven.apache.org/download.cgi解压到&#x2F;Users&#x2F;simon&#x2F;Development&#x2F;maven&#x2F;apache-maven-3.5.4\n配置国内maven仓库镜像编辑maven的settings.xml文件\nvi /Users/simon/Development/maven/apache-maven-3.5.4/conf/settings.xml\n\n找到mirrors标签，并在标签里面添加如下内容\n&lt;mirror&gt;  &lt;id&gt;alimaven&lt;/id&gt;  &lt;name&gt;aliyun maven&lt;/name&gt;  &lt;url&gt;http://maven.aliyun.com/nexus/content/groups/public/&lt;/url&gt;  &lt;mirrorOf&gt;central&lt;/mirrorOf&gt;&lt;/mirror&gt;&lt;mirror&gt;  &lt;id&gt;alimaven&lt;/id&gt;  &lt;mirrorOf&gt;central&lt;/mirrorOf&gt;  &lt;name&gt;aliyun maven&lt;/name&gt;  &lt;url&gt;http://maven.aliyun.com/nexus/content/repositories/central/&lt;/url&gt;&lt;/mirror&gt;\n\n保存退出。以后项目就可以从国内阿里源下载jar包了，速度快得多了。\n配置环境变量vi ~/.bash_profile\n\n编辑如下内容\n# Javaexport JAVA_HOME=/Library/Java/JavaVirtualMachines/jdk1.8.0_181.jdk/Contents/Homeexport JRE_HOME=/Library/Java/JavaVirtualMachines/jdk1.8.0_181.jdk/Contents/Home/jreexport CLASSPATH=.:$CLASSPATH:$JAVA_HOME/lib/tools.jar:$JAVA_HOME/lib/dt.jar# Mavenexport M2_HOME=/Users/simon/Development/apache-maven-3.5.4export PATH=$JAVA_HOME/bin:$M2_HOME/bin:$PATH\n\n让设置生效\nsource ~/.bash_profile\n","categories":["Java","Maven"],"tags":["java","maven"]},{"title":"Git服务器生成ssh公钥","url":"/2018/08/Git%E6%9C%8D%E5%8A%A1%E5%99%A8%E7%94%9F%E6%88%90ssh%E5%85%AC%E9%92%A5/","content":"初次登录gitlab时，一般会有这样的提示\n\nYou won’t be able to pull or push project code via SSH until you add an SSH key to your profile\n\n这时，需要生成ssh公钥。可参考文章：ssh客户端无需输入密码登录服务器\n$ cd ~/.ssh$ lsauthorized_keys2  id_dsa       known_hostsconfig            id_dsa.pub\n\n我们需要寻找一对以 id_dsa 或 id_rsa 命名的文件，其中一个带有 .pub 扩展名。 .pub 文件是你的公钥，另一个则是私钥。 如果找不到这样的文件（或者根本没有 .ssh 目录），你可以通过运行 ssh-keygen 程序来创建它们。\n$ cat ~/.ssh/id_rsa.pub\n\n把内容复制到git中ssh Key里面，下次就可以自动登录和密码操作git了\n","categories":["Tools","Git"],"tags":["git","github","gitlab","reset","commit"]},{"title":"Linux(CentOS)环境安装Redis 4","url":"/2018/08/Linux-CentOS-%E7%8E%AF%E5%A2%83%E5%AE%89%E8%A3%85Redis-4/","content":"下载地址：http://redis.io/download，下载最新稳定版本。\n本次使用最新版本4.0.11。\n目前可以安装Redis 5了，可参考：Linux(CentOS)环境下安装Redis 5\n# wget http://download.redis.io/releases/redis-4.0.11.tar.gz# tar -zxvf redis-4.0.11.tar.gz# cd redis-4.0.11# make\n\n编译时可能遇到的问题：\n/bin/sh: cc: 未找到命令\n\n系统中缺少gcc，安装一下gcc即可解决问题\n# yum -y install gcc automake autoconf libtool make\n\nzmalloc.h:50:31: 致命错误：jemalloc/jemalloc.h：没有那个文件或目录\n分配器allocator，如果有MALLOC这个环境变量，会有用这个环境变量的去建立Redis。\n而且libc并不是默认的分配器，默认的是jemalloc,因为jemalloc被证明有更少的fragmentation problems相比于libc。\n但是如果你又没有jemalloc而只有libc当然make出错。所以加这么一个参数,运行如下命令：\n# make MALLOC=libc\n\n编译成功。\n接下来修改redis.conf配置文件，让redis服务在后台运行。把daemonize no改为daemonize yes\n# By default Redis does not run as a daemon. Use &#x27;yes&#x27; if you need it.# Note that Redis will write a pid file in /var/run/redis.pid when daemonized.daemonize yes\n\n最后一步，启动redis服务\n# src/redis-server redis.conf\n\n测试\n# src/redis-cli 127.0.0.1:6379&gt; pingPONG\n","categories":["Middleware","Redis"],"tags":["redis"]},{"title":"macOS下Anaconda的安装及环境切换（Python2/Python3）","url":"/2018/08/macOS%E4%B8%8BAnaconda%E7%9A%84%E5%AE%89%E8%A3%85%E5%8F%8A%E7%8E%AF%E5%A2%83%E5%88%87%E6%8D%A2%EF%BC%88Python2-Python3%EF%BC%89/","content":"安装Anaconda官网下载安装包http://www.anaconda.com\n默认下载安装的是python3.6版本，安装好以后，可以增加其他版本如python2.7的环境。\n\n\n查看当前系统下的环境两个命令conda info -e和conda env list输出结果相同\n$ conda info -e# conda environments:#base                  *  /Users/simon/anaconda3python27                 /Users/simon/anaconda3/envs/python27$ conda env list# conda environments:#base                  *  /Users/simon/anaconda3python27                 /Users/simon/anaconda3/envs/python27\n\n激活某个环境$ source activate base // 激活base环境$ source activate python27 // 激活python27环境\n\n删除环境conda env remove -n &lt;env_name&gt; \n\n管理包安装包当然，numpy和pandas默认已经自带了。\nconda install numpy pandas\n\n删除包conda remove &lt;package_name&gt;\n\n之前在编译前端代码时，出现以下错误。提示Python版本需要 &gt;&#x3D; v2.5.0 &amp; &lt; 3.0.0\nconfigure errorgyp  ERR! node -v v10.7.0gyp ERR! node-gyp -v v3.7.0gyp ERR! not ok gyp ERR! stack Error: Python executable &quot;/Users/simon/anaconda3/bin/python&quot; is v3.6.5, which is not supported by gyp.gyp ERR! stack You can pass the --python switch to point to Python &gt;= v2.5.0 &amp; &lt; 3.0.0.\n\n所以需要执行命令source activate python27再编译执行，就可以解决上述问题了\n","categories":["Python"],"tags":["python"]},{"title":"Virtualbox虚拟机设置桥接bridge网络模式","url":"/2018/08/Virtualbox%E8%99%9A%E6%8B%9F%E6%9C%BA%E8%AE%BE%E7%BD%AE%E6%A1%A5%E6%8E%A5bridge%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%BC%8F/","content":"网卡1设置静态ip，这时需要设置IPADDR,GATEWAY,NETMASK,DNS,BOOTPROTO&#x3D;static等信息在某些时候，并不能通过dhcp获取到ip（什么原因目前不知道，网络还需要学习）。所以采取这种方式。\n# cat /etc/sysconfig/network-scripts/ifcfg-enp0s3TYPE=EthernetPROXY_METHOD=noneBROWSER_ONLY=noBOOTPROTO=staticDEFROUTE=yesIPV4_FAILURE_FATAL=noIPV6INIT=yesIPV6_AUTOCONF=yesIPV6_DEFROUTE=yesIPV6_FAILURE_FATAL=noIPV6_ADDR_GEN_MODE=stable-privacyNAME=enp0s3DEVICE=enp0s3ONBOOT=yesUUID=b7e2e81c-74ec-4662-b4f0-24ad9d7f55b0IPADDR=192.168.1.230GATEWAY=192.168.1.1NETMASK=255.255.255.0DNS=192.168.1.1\n\n网卡2设置通过dhcp能获取Ip的方式\n# cat /etc/sysconfig/network-scripts/ifcfg-enp0s8TYPE=EthernetPROXY_METHOD=noneBROWSER_ONLY=no#BOOTPROTO=staticBOOTPROTO=dhcpDEFROUTE=yesIPV4_FAILURE_FATAL=noIPV6INIT=yesIPV6_AUTOCONF=yesIPV6_DEFROUTE=yesIPV6_FAILURE_FATAL=noIPV6_ADDR_GEN_MODE=stable-privacyNAME=enp0s8DEVICE=enp0s8ONBOOT=yes\n\n\n"},{"title":"macOS High Sierra(10.13)安装Virtualbox失败及解决方案","url":"/2018/08/macOS-High-Sierra-10-13-%E5%AE%89%E8%A3%85Virtualbox%E5%A4%B1%E8%B4%A5%E5%8F%8A%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88/","content":"环境OS: macOS High Sierra 10.13.6Virtualbox: 5.2.18\n问题启动虚拟机，报错，Kernel drivers not installed (rc = -1908)\n然后重新安装Virtualbox，安装virtualbox的时候却总是安装失败。具体症状为在最后一步的时候提示安装失败。\n原因\nHigh Sierra comes with a new security feature: Secure Kernel Extension Loading, which blocks kernel extension loading.\n\nvirtualbox在安装的时候需要安装内核扩展（Kernel extenstion），而macOS 10.13 High Sierra的新安全特性会阻止外部内核扩展的安装，所以安装总是会被系统拦截。\n方案进入Preferences &gt; Security &amp; Privacy &gt; General: 点击“允许”\n"},{"title":"IntelliJ IDEA验证码 license server","url":"/2018/08/IntelliJ-IDEA%E9%AA%8C%E8%AF%81%E7%A0%81-license-server/","content":"首先表明必须支持正版，尊重知识产权。\n最早的方法可能已经不能成功使用了，看最新的方法就是了。\n2019&#x2F;10&#x2F;20日更新链接: https://pan.baidu.com/s/1EtXP4MS7jgrhCvT6XX5a8A&amp;shfl=shareset 提取码: uerw\n下载jetbrains-agent.jar到idea-IU-192.6817.14/bin目录，具体哪个目录无所谓，只要IDEA能通过绝对路径访问到就行了。\n先点击Evaluation进入，在IDEA欢迎页面，点击Configure -&gt; Edit Custom VM Options...\n在最后一行添加内容\n-javaagent:/usr/local/idea-IU-192.6817.14/bin/jetbrains-agent.jar\n\n保存退出，重启IDEA。\n注：需要用root启动。用sudo都不行。\n如果显示下面内容，代表设置成功了。\nbin/idea.sh OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release. ==================================================== =======        Jetbrains License Crack       ======= =======           https://zhile.io           ======= ==================================================== @See: https://zhile.io/2018/08/17/jetbrains-license-server-crack.html @Version: 2.2.0, @Build Date: 2019-07-19\n\n启动以后，Configure -&gt; Manage License，这时在License server选项中，就会有http://jetbrains-license-server。点击Activate按钮激活就可以了。\n2018&#x2F;8&#x2F;3日更新以下是IntelliJ的license server地址\n2018-7-30亲测可用http://idea.congm.in\n2018-8-3亲测可用http://idea.toocruel.net\n","categories":["IDE"]},{"title":"macOS系统查看网络的基本命令","url":"/2018/08/macOS%E7%B3%BB%E7%BB%9F%E6%9F%A5%E7%9C%8B%E7%BD%91%E7%BB%9C%E7%9A%84%E5%9F%BA%E6%9C%AC%E5%91%BD%E4%BB%A4/","content":"查看网关netstat -r\n\n查看dnsnslookup www.baidu.com\n\n或者\nscutil --dns\n","categories":["OS","MacOS"]},{"title":"HBase入门","url":"/2019/01/HBase%E5%85%A5%E9%97%A8/","content":"HBase的应用场景及特点HBase是一个分布式的数据库，主要用于海量数据的存储和海量数据的准实时查询。HBase可支持100亿行*100万列的表。而对于关系型数据库，单表超过500万行，列超过30列则需要重新设计了。\n应用场景交通\n金融\n电商\n移动 - 如通话记录\nHBase的特点容量大TB级别，上亿行的数据\n面向列列式存储，可以自由增加列\n多版本一行中，多列对应多版本\n稀疏性由于是列式存储，列的值不存在，则不需要占用空间。\n扩展性底层依赖于HDFS，动态增加DataNode就行了\n可靠性基于HDFS分布式文件系统。基于副本数和日志信息可恢复。\n高性能LSM数据结构和Rowkey有序排列，读写性能高。\nHBase的概念与定位官方对于HBase的概念描述hadoop生态系统中对于HBase的定位HBase的架构体系与设计模型服务架构体系HBase主要进程Master\nRegionServer\n类似于NameNode和DataNode\nHBase依赖的两个外部服务Zookeeper\nHDFS\n设计模型表结构表数据个人信息 | 教育经历 | 工作经历\n列簇\n一张表列蔟不会超过5个\n每个列蔟中的列数没有限制\n列只有插入数据后存在\n列在列蔟中是有序的\nHBase表与关系型数据库表结构对比列动态增加\n数据自动切分\n高并发读写\n不支持条件查询，但关系型支持复杂查询\nHBase的安装部署部署条件JDK 1.7以上\nHadoop 2.5.x以上\nZookeeper 3.4.x以上\n部署hbase-env.sh\nhbase-site.xml(hbase-default.xml)\nregionserver\nHBase ShellDDL 操作create\ndescribe\ndisable\nenable\ndrop\nDML 操作put\nget\ndelete\ncount\nscan\n","categories":["大数据","HBase"]},{"title":"Hive环境搭建","url":"/2019/01/Hive%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/","content":"下载Hive我们还是使用cloudera cdh5版本 hive-1.1.0-cdh5.7.0\ncd /usr/local/hivewget http://archive.cloudera.com/cdh5/cdh/5/hive-1.1.0-cdh5.7.0.tar.gztar -zxvf hive-1.1.0-cdh5.7.0.tar.gz\n\n配置环境变量vi ~/.bash_profile\n\n在文件末尾加上如下内容\nexport HIVE_HOME=/usr/local/hive/hive-1.1.0-cdh5.7.0\n\n执行source命令让配置生效\nsource ~/.bash_profile\n\n配置Hive配置hive-env.shcp conf/hive-env.sh.template conf/hive-env.shvi conf/hive-env.sh\n\n配置HADOOP_HOME参数\nHADOOP_HOME=/usr/local/hadoop/hadoop-2.6.0-cdh5.7.0\n\n配置hive-site.xml可以参考hcatalog/etc/hcatalog/proto-hive-site.xml文件的内容\nvi conf/hive-site.xml\n\n添加如下内容：\n&lt;?xml version=&quot;1.0&quot;?&gt;&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;&lt;!--    Licensed to the Apache Software Foundation (ASF) under one    or more contributor license agreements.  See the NOTICE file    distributed with this work for additional information    regarding copyright ownership.  The ASF licenses this file    to you under the Apache License, Version 2.0 (the    &quot;License&quot;); you may not use this file except in compliance    with the License.  You may obtain a copy of the License at        http://www.apache.org/licenses/LICENSE-2.0    Unless required by applicable law or agreed to in writing,    software distributed under the License is distributed on an    &quot;AS IS&quot; BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY    KIND, either express or implied.  See the License for the    specific language governing permissions and limitations    under the License.--&gt;&lt;configuration&gt;&lt;property&gt;  &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt;  &lt;value&gt;jdbc:mysql://localhost:3306/hivemetastoredb?createDatabaseIfNotExist=true&lt;/value&gt;  &lt;description&gt;JDBC connect string for a JDBC metastore&lt;/description&gt;&lt;/property&gt;&lt;property&gt;  &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt;  &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt;  &lt;description&gt;Driver class name for a JDBC metastore&lt;/description&gt;&lt;/property&gt;&lt;property&gt;  &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt;  &lt;value&gt;root&lt;/value&gt;  &lt;description&gt;username to use against metastore database&lt;/description&gt;&lt;/property&gt;&lt;property&gt;  &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt;  &lt;value&gt;PASSWORD&lt;/value&gt;  &lt;description&gt;password to use against metastore database&lt;/description&gt;&lt;/property&gt;&lt;/configuration&gt;\n\n然后我们还需要把mysql驱动包从Maven仓库拷贝到$HIVE_HOME&#x2F;lib里面。\ncp /Users/simon/.m2/repository/mysql/mysql-connector-java/5.1.46/mysql-connector-java-5.1.46.jar lib/\n\n安装Mysql在这里我就先不讲如何安装Mysql了，可参考 MacOS X安装MySQL\n启动Hive启动hive之前必须得保证hadoop hdfs已经启动。\nbin/hivels: /usr/local/spark/spark-2.2.3-bin-hadoop2.6/lib/spark-assembly-*.jar: No such file or directory2019-01-11 10:12:46,772 WARN  [main] mapreduce.TableMapReduceUtil: The hbase-prefix-tree module jar containing PrefixTreeCodec is not present.  Continuing without it.SLF4J: Class path contains multiple SLF4J bindings.SLF4J: Found binding in [jar:file:/usr/local/hbase/hbase-1.2.0-cdh5.7.0/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]SLF4J: Found binding in [jar:file:/usr/local/hadoop/hadoop-2.6.0-cdh5.7.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]2019-01-11 10:12:46,954 WARN  [main] util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicableLogging initialized using configuration in jar:file:/usr/local/hive/hive-1.1.0-cdh5.7.0/lib/hive-common-1.1.0-cdh5.7.0.jar!/hive-log4j.propertiesThu Jan 11 10:12:49 CST 2019 WARN: Establishing SSL connection without server&#x27;s identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn&#x27;t set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to &#x27;false&#x27;. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification.Thu Jan 11 10:12:49 CST 2019 WARN: Establishing SSL connection without server&#x27;s identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn&#x27;t set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to &#x27;false&#x27;. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification.Thu Jan 11 10:12:49 CST 2019 WARN: Establishing SSL connection without server&#x27;s identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn&#x27;t set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to &#x27;false&#x27;. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification.Thu Jan 11 10:12:49 CST 2019 WARN: Establishing SSL connection without server&#x27;s identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn&#x27;t set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to &#x27;false&#x27;. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification.Thu Jan 11 10:12:51 CST 2019 WARN: Establishing SSL connection without server&#x27;s identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn&#x27;t set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to &#x27;false&#x27;. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification.Thu Jan 11 10:12:51 CST 2019 WARN: Establishing SSL connection without server&#x27;s identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn&#x27;t set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to &#x27;false&#x27;. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification.Thu Jan 11 10:12:51 CST 2019 WARN: Establishing SSL connection without server&#x27;s identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn&#x27;t set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to &#x27;false&#x27;. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification.Thu Jan 11 10:12:51 CST 2019 WARN: Establishing SSL connection without server&#x27;s identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn&#x27;t set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to &#x27;false&#x27;. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification.WARNING: Hive CLI is deprecated and migration to Beeline is recommended.hive&gt;\n\ncd $MYSQL_HOMEbin/mysql -u root -pEnter password:Welcome to the MySQL monitor.  Commands end with ; or \\g.Your MySQL connection id is 533Server version: 5.7.17 MySQL Community Server (GPL)Copyright (c) 2000, 2016, Oracle and/or its affiliates. All rights reserved.Oracle is a registered trademark of Oracle Corporation and/or itsaffiliates. Other names may be trademarks of their respectiveowners.Type &#x27;help;&#x27; or &#x27;\\h&#x27; for help. Type &#x27;\\c&#x27; to clear the current input statement.mysql&gt;show databases;+---------------------------+| Database                  |+---------------------------+| information_schema        || hivemetastoredb           || mysql                     || performance_schema        |+---------------------------+11 rows in set (0.01 sec)mysql&gt;use hivemetastoredb;Database changed\n\n可以看到之前设置的数据库hivemetastoredb已经成功生成了。此数据库里面，还自动生成了很多表，如TBLS。\n创建表回到hive shell\nhive&gt; create table hive_wordcount(context string);OKTime taken: 1.312 seconds\n\n回到Mysql终端\nmysql&gt; select * FROM TBLS;Empty set (0.00 sec)mysql&gt; select * from TBLS;+--------+-------------+-------+------------------+-------+-----------+-------+----------------+---------------+--------------------+--------------------+| TBL_ID | CREATE_TIME | DB_ID | LAST_ACCESS_TIME | OWNER | RETENTION | SD_ID | TBL_NAME       | TBL_TYPE      | VIEW_EXPANDED_TEXT | VIEW_ORIGINAL_TEXT |+--------+-------------+-------+------------------+-------+-----------+-------+----------------+---------------+--------------------+--------------------+|      1 |  1548903355 |     1 |                0 | simon |         0 |     1 | hive_wordcount | MANAGED_TABLE | NULL               | NULL               |+--------+-------------+-------+------------------+-------+-----------+-------+----------------+---------------+--------------------+--------------------+1 row in set (0.00 sec)mysql&gt; select * from COLUMNS_V2;+-------+---------+-------------+-----------+-------------+| CD_ID | COMMENT | COLUMN_NAME | TYPE_NAME | INTEGER_IDX |+-------+---------+-------------+-----------+-------------+|     1 | NULL    | context     | string    |           0 |+-------+---------+-------------+-----------+-------------+1 row in set (0.00 sec)\n\n可以看到已经有一张表hive_wordcount已经成功生成了，这张表里面有一个context字段。\n加载数据到表中cd $HADOOP_HOMEbin/hadoop fs -text /data/input19/01/11 11:00:18 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicablehello worldhello hello world\n\n我们把hdfs:&#x2F;&#x2F;data&#x2F;input加载到表中\nhive&gt; LOAD DATA INPATH &#x27;/data/input&#x27; INTO TABLE hive_wordcount;Loading data to table default.hive_wordcountTable default.hive_wordcount stats: [numFiles=1, totalSize=30]OKTime taken: 1.078 seconds\n\n如果文件不在hfds里面，则命令为LOAD DATA LOCAL\n执行查询hive&gt; select word, count(1) from hive_wordcount lateral view explode(split(context, &#x27; &#x27;)) wc as word group by word;Query ID = simon_20190111105555_39805c49-890c-4241-8388-56542c90fa2eTotal jobs = 1Launching Job 1 out of 1Number of reduce tasks not specified. Estimated from input data size: 1In order to change the average load for a reducer (in bytes):  set hive.exec.reducers.bytes.per.reducer=&lt;number&gt;In order to limit the maximum number of reducers:  set hive.exec.reducers.max=&lt;number&gt;In order to set a constant number of reducers:  set mapreduce.job.reduces=&lt;number&gt;Starting Job = job_1548765492126_0003, Tracking URL = http://localhost:8088/proxy/application_1548765492126_0003/Kill Command = /usr/local/hadoop/hadoop-2.6.0-cdh5.7.0/bin/hadoop job  -kill job_1548765492126_0003Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 12019-01-11 11:11:01,153 Stage-1 map = 0%,  reduce = 0%2019-01-11 11:11:06,299 Stage-1 map = 100%,  reduce = 0%2019-01-11 11:11:12,475 Stage-1 map = 100%,  reduce = 100%Ended Job = job_1548765492126_0003MapReduce Jobs Launched:Stage-Stage-1: Map: 1  Reduce: 1   HDFS Read: 8786 HDFS Write: 16 SUCCESSTotal MapReduce CPU Time Spent: 0 msecOKhello\t3world\t2Time taken: 18.997 seconds, Fetched: 2 row(s)\n\nlateral view explode即把每行记录按照指定分隔符进行拆解。\n最后我们成功的得到了如下结果\nhello\t3world\t2","categories":["大数据","Hive"],"tags":["Hive"]},{"title":"IntelliJ IDEA Maven项目添加依赖自动提示","url":"/2019/01/IntelliJ-IDEA-Maven%E9%A1%B9%E7%9B%AE%E6%B7%BB%E5%8A%A0%E4%BE%9D%E8%B5%96%E8%87%AA%E5%8A%A8%E6%8F%90%E7%A4%BA/","content":"使用IntelliJ IDEA，我们在给Maven项目添加依赖时，可以设置让IDEA自动提示依赖包。\nPreferences &gt; Build, Execution, Deployment &gt; Build Tools -&gt; Maven &gt; Repositories &gt; Indexed Maven Repositores\n选中URL，再点击Update按钮。更新后，在添加依赖时，则可以自动提示了。\n\n","categories":["Tools","IntelliJ IDEA"]},{"title":"hexo安装sitemap","url":"/2019/01/hexo%E5%AE%89%E8%A3%85sitemap/","content":"通过Hexo搭建了博客，准备做一些简单的SEO，让搜索引擎收录博客中的页面。所以安装了两个hexo的插件。\n安装sitemap插件npm install hexo-generator-sitemapnpm install hexo-generator-baidu-sitemap\n\n一个是传统的sitemap，一个是百度专用的sitemap。\n安装成功后，启动本地服务\nhexo s\n\n则可以通过如下链接访问到sitemap的页面\n\nhttp://localhost:4000/sitemap.xmlhttp://localhost:4000/baidusitemap.xml\n\n注册百度搜索资源平台之前叫做百度站长平台，现在已经改为搜索资源平台。\n进入后，添加自己的网站到平台。按要求绑定一个CNAME，让百度确定这个网站是归属于你的。\n提交sitemap把刚才提到的两个sitemap.xml在公网上的地址提交到百度平台即可。成功以后如图所示：\n\n\n","categories":["Tools","Hexo"]},{"title":"YARN介绍及环境搭建","url":"/2019/01/YARN%E4%BB%8B%E7%BB%8D%E5%8F%8A%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/","content":"YARN架构1 RM(ResourceManager) + n NM(NodeManager)\nResourceManager的职责一个集群active状态的RM只有一个，负责整个集群的资源管理和调度\n\n处理客户端的请求（启动 &#x2F; kill）\n\n启动 &#x2F; 监控ApplicationMaster（一个作业对应一个AM）\n\n监控NM\n\n系统的资源分配和调度\n\n\nNodeManager集群中有n个NodeManager，负责单个节点的资源管理和使用以及task的运行情况\n\n定期向RM汇报本节点的资源使用情况和各个Container的运行状态\n\n接收并处理RM的container启停的各种命令\n\n单个节点的资源管理和任务管理\n\n\nApplicationMaster每个应用 &#x2F; 作业对应一个，负责应用程序的管理\n\n数据切分\n\n为应用程序向RM申请资源(container)，并分配给内部任务\n\n与NM通信以启停task，task是运行在container中的\n\ntask的监控和容错\n\n\nContainer对任务运行情况的描述：cpu, memory, 环境变量\nYARN执行流程\n用户向YARN提交作业\n\nRM为该作业分配第一个container（AM）\n\nRM与对应的NM通信，要求NM在这个container上启动应用程序的AM\n\nAM首先向RM注册，然后AM将为各个任务申请资源，并监控运行情况\n\nAM采用轮询的方式通过RPC协议向RM申请和领取资源\n\nAM申请到资源以后，便和相应的NM通信，要求NM启动任务\n\nNM启动我们作业对应的task\n\n\nYARN配置配置etc&#x2F;hadoop&#x2F;mapred-site.xmlcd $HADOOP_HOMEcp etc/hadoop/mapred-site.xml.template etc/hadoop/mapred-site.xmlvi etc/hadoop/mapred-site.xml\n\n添加如下内容\n&lt;configuration&gt;    &lt;property&gt;        &lt;name&gt;mapreduce.framework.name&lt;/name&gt;        &lt;value&gt;yarn&lt;/value&gt;    &lt;/property&gt;&lt;/configuration&gt;\n\n配置etc&#x2F;hadoop&#x2F;yarn-site.xmlvi etc/hadoop/yarn-site.xml\n\n添加如下内容\n&lt;configuration&gt;    &lt;property&gt;        &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;        &lt;value&gt;mapreduce_shuffle&lt;/value&gt;    &lt;/property&gt;&lt;/configuration&gt;\n\n启动YARNsbin/start-yarn.shstarting yarn daemonsstarting resourcemanager, logging to /usr/local/hadoop/hadoop-2.6.0-cdh5.7.0/logs/yarn-simon-resourcemanager-localhost.outlocalhost: starting nodemanager, logging to /usr/local/hadoop/hadoop-2.6.0-cdh5.7.0/logs/yarn-simon-nodemanager-localhost.out\n\njps41620 NodeManager41546 ResourceManager\n\n说明YARN已经成功启动。\n通过浏览器访问地址 http://localhost:8088，可见页面如下\n\n\n提交作业bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0-cdh5.7.0.jar wordcountUsage: wordcount &lt;in&gt; [&lt;in&gt;...] &lt;out&gt;\n\n说明需要提供输入文件及输出结果的位置。\n我们先创建一个测试文件\nvi /tmp/inputhello worldhello hello world\n\n把input文件上传到hdfs:&#x2F;&#x2F;data&#x2F;目录下\nbin/hadoop fs -put /tmp/input /data\n\n再次执行命令\nbin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0-cdh5.7.0.jar wordcount /data/input /data/out\n\n查看执行结果\nbin/hadoop fs -ls /data/out19/01/09 21:07:44 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicableFound 2 items-rw-r--r--   1 simon supergroup          0 2019-01-29 21:06 /data/out/_SUCCESS-rw-r--r--   1 simon supergroup         16 2019-01-29 21:06 /data/out/part-r-00000\n\nwordcount的结果在文件&#x2F;data&#x2F;out&#x2F;part-r-00000里面\nbin/hadoop fs -text /data/out/part-r-0000019/01/09 21:08:54 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicablehello\t3world\t2\n\n如果重复提交作业，会出现如下FileAlreadyExistsException异常。\nbin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0-cdh5.7.0.jar wordcount /data/input /data/out19/01/09 21:09:52 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable19/01/09 21:09:53 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:803219/01/09 21:09:53 WARN security.UserGroupInformation: PriviledgedActionException as:simon (auth:SIMPLE) cause:org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory hdfs://localhost:8020/data/out already existsorg.apache.hadoop.mapred.FileAlreadyExistsException: Output directory hdfs://localhost:8020/data/out already exists\n\n所以如果要重复执行，则需要把输出文件先删除掉。\n","categories":["大数据","YARN"],"tags":["YARN"]},{"title":"创建Spark Streaming Maven项目","url":"/2019/01/%E5%88%9B%E5%BB%BASpark-Streaming-Maven%E9%A1%B9%E7%9B%AE/","content":"通过IDEA创建一个Maven项目, spark-streaming-demo\n编辑pom.xml文件\n因为我们使用了CDH 5组件，所以需要引入CDH 5的Maven仓库。\n在project标签下面加入\n&lt;repositories&gt;    &lt;repository&gt;        &lt;id&gt;cloudera&lt;/id&gt;        &lt;url&gt;https://repository.cloudera.com/artifactory/cloudera-repos/&lt;/url&gt;    &lt;/repository&gt;&lt;/repositories&gt;\n\n在properties标签下面添加组件版本信息\n&lt;properties&gt;    &lt;scala.compat.version&gt;2.11&lt;/scala.compat.version&gt;    &lt;spark.version&gt;2.2.3&lt;/spark.version&gt;    &lt;hadoop.version&gt;2.6.0-cdh5.7.0&lt;/hadoop.version&gt;    &lt;hbase.version&gt;1.2.0-cdh5.7.0&lt;/hbase.version&gt;&lt;/properties&gt;\n\n在dependencies标签里面添加如下依赖\n&lt;!-- Hadoop 依赖--&gt;&lt;dependency&gt;    &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;    &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt;    &lt;version&gt;$&#123;hadoop.version&#125;&lt;/version&gt;&lt;/dependency&gt;&lt;!-- HBase 依赖 --&gt;&lt;dependency&gt;    &lt;groupId&gt;org.apache.hbase&lt;/groupId&gt;    &lt;artifactId&gt;hbase-client&lt;/artifactId&gt;    &lt;version&gt;$&#123;hbase.version&#125;&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt;    &lt;groupId&gt;org.apache.hbase&lt;/groupId&gt;    &lt;artifactId&gt;hbase-server&lt;/artifactId&gt;    &lt;version&gt;$&#123;hbase.version&#125;&lt;/version&gt;&lt;/dependency&gt;&lt;!-- Spark streaming 依赖 --&gt;&lt;dependency&gt;    &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;    &lt;artifactId&gt;spark-streaming_$&#123;scala.compat.version&#125;&lt;/artifactId&gt;    &lt;version&gt;$&#123;spark.version&#125;&lt;/version&gt;&lt;/dependency&gt;","categories":["大数据","Spark"],"tags":["Spark"]},{"title":"mac查询当前用户和用户组命令","url":"/2019/01/mac%E6%9F%A5%E8%AF%A2%E5%BD%93%E5%89%8D%E7%94%A8%E6%88%B7%E5%92%8C%E7%94%A8%E6%88%B7%E7%BB%84%E5%91%BD%E4%BB%A4/","content":"以下是macOS用户查询当前用户及用户组信息的命令\n查询当前用户所属用户组信息$ groupsstaff everyone localaccounts _appserverusr admin _appserveradm _lpadmin _appstore _lpoperator _developer _analyticsuserscom.apple.access_ftp com.apple.access_screensharing com.apple.access_ssh com.apple.sharepoint.group.1\n\n查询指定用户所属用户组信息groups &lt;username&gt;\n$ groups simonstaff everyone localaccounts _appserverusr admin _appserveradm _lpadmin _appstore _lpoperator _developer _analyticsuserscom.apple.access_ftp com.apple.access_screensharing com.apple.access_ssh com.apple.sharepoint.group.1\n\n查询指定用户的信息id -a &lt;username&gt;\n$ id -a simonuid=501(simon) gid=20(staff) groups=20(staff),12(everyone),61(localaccounts),79(_appserverusr),80(admin),81(_appserveradm),98(_lpadmin),33(_appstore),100(_lpoperator),204(_developer),250(_analyticsusers),395(com.apple.access_ftp),398(com.apple.access_screensharing),399(com.apple.access_ssh),701(com.apple.sharepoint.group.1)\n\n查询当前用户$ whoamisimon\n\n查询所有用户$ cat /etc/passwd\n","categories":["OS","MacOS"],"tags":["mac"]},{"title":"安装Hadoop","url":"/2019/01/%E5%AE%89%E8%A3%85Hadoop/","content":"下载Hadoop安装包我们选择安装hadoop cdh5版本，hadoop-2.6.0-cdh5.7.0\n下载并解压\ncd /usr/local/hadoopwget http://archive.cloudera.com/cdh5/cdh/5/hadoop-2.6.0-cdh5.7.0.tar.gztar -zxvf hadoop-2.6.0-cdh5.7.0.tar.gz\n\n添加环境变量vi ~/.bash_profile\n\n添加如下内容\nexport HADOOP_HOME=/usr/local/hadoop/hadoop-2.6.0-cdh5.7.0\n\n执行source命令让其配置生效\nsource ~/.bash_profile\n\n设置ssh免密登录ssh-keygen -t rsassh-copy-id -i ~/.ssh/id_rsa.pub simon@localhost\n\n以上设置成功后，我们可以发现ssh登录时不需要密码了。\nssh simon@localhostLast login: Mon Jan 08 10:45:10 2019\n\n其他Linux配置我目前所有的大数据组件都是用于开发验证，所以都是安装在本地的macOS环境下。如果在真实的Linux集群环境下安装，还需要配置hostname等信息\nLinux下修改hostname\nvi /etc/sysconfig/network\n\n添加如下内容\nNETWORKING=yesHOSTNAME=hadoop001\n\n修改/etc/hosts文件，配置主机和ip的映射关系\nHadoop配置文件修改etc&#x2F;hadoop&#x2F;hadoop-env.shexport JAVA_HOME=/Library/Java/JavaVirtualMachines/jdk1.8.0_181.jdk/Contents/Home\n\netc&#x2F;hadoop&#x2F;core-site.xml伪分布式配置\nHadoop 2.x版本默认端口为8020而非1.x版本的9000。\n&lt;configuration&gt;    &lt;property&gt;        &lt;name&gt;fs.defaultFS&lt;/name&gt;        &lt;value&gt;hdfs://localhost:8020&lt;/value&gt;    &lt;/property&gt;&lt;/configuration&gt;\n\n设置tmp文件夹，默认地址为&#x2F;tmp&#x2F;hadoop-${user.name}\n&lt;property&gt;    &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;    &lt;value&gt;/usr/local/hadoop/tmp&lt;/value&gt;&lt;/property&gt;\n\netc&#x2F;hadoop&#x2F;hdfs-site.xml设置文件系统复本系数。\n&lt;configuration&gt;    &lt;property&gt;        &lt;name&gt;dfs.replication&lt;/name&gt;        &lt;value&gt;1&lt;/value&gt;    &lt;/property&gt;&lt;/configuration&gt;\n\n格式化HDFS这一步操作只需要执行一次，格式化后，HDFS上的数据会被清空。\nbin/hdfs namenode -format\n\n注：bin目录存放客户端命令，sbin存放服务端命令。\n启动HDFS启动NameNode和DataNodesbin/start-dfs.sh19/01/08 13:11:40 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicableStarting namenodes on [localhost]localhost: starting namenode, logging to /usr/local/hadoop/hadoop-2.6.0-cdh5.7.0/logs/hadoop-simon-namenode-localhost.outlocalhost: starting datanode, logging to /usr/local/hadoop/hadoop-2.6.0-cdh5.7.0/logs/hadoop-simon-datanode-localhost.outStarting secondary namenodes [0.0.0.0]0.0.0.0: starting secondarynamenode, logging to /usr/local/hadoop/hadoop-2.6.0-cdh5.7.0/logs/hadoop-simon-secondarynamenode-localhost.out19/01/08 13:11:55 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n\n我们注意到这一句localhost: starting datanode，这句里面的localhost，是和slaves从节点中的名字相同的。\ncat etc/hadoop/slaveslocalhost\n\n验证是否启动成功jps18242 SecondaryNameNode18150 DataNode18079 NameNode\n\n拥有这三个进程就说明已经启动成功。\n同时我们还可以通过Web UI查看\nhttp://localhost:50070/\n","categories":["大数据","Hadoop"],"tags":["Hadoop"]},{"title":"安装Spark","url":"/2019/01/%E5%AE%89%E8%A3%85Spark/","content":"这篇文章，我们选择记录最简单的安装方式，而非编译安装。\n下载安装包到Apache Spark官网的下载页面 http://spark.apache.org/downloads.html\n选择Spark2.2.3版本，package type为Pre-built for Apache Hadoop 2.6，因为目前环境hadoop版本是hadoop-2.6.0-cdh5.7.0\nwget https://archive.apache.org/dist/spark/spark-2.2.3/spark-2.2.3-bin-hadoop2.6.tgztar -zxvf spark-2.2.3-bin-hadoop2.6.tgz\n\n添加环境变量vi ~/.bash_profile\n\n添加如下内容\nexport SPARK_HOME=/usr/local/spark/spark-2.2.3-bin-hadoop2.6\n\n执行source命令让其配置生效\nsource ~/.bash_profile\n\n启动Sparkcd $SPARK_HOMEbin/spark-shell --helpUsage: ./bin/spark-shell [options]Options:  --master MASTER_URL         spark://host:port, mesos://host:port, yarn, or local....  \n\n从上面可以看出可以通过spark://host:port, mesos://host:port, yarn和local等方式启动Spark。\n我们先采用本地模式启动。\nbin/spark-shell --master local[2]Using Spark&#x27;s default log4j profile: org/apache/spark/log4j-defaults.propertiesSetting default log level to &quot;WARN&quot;.To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).19/01/22 22:56:54 WARN Utils: Your hostname, localhost resolves to a loopback address: 127.0.0.1; using 192.168.1.6 instead (on interface en0)19/01/22 22:56:54 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address19/01/22 22:56:54 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicableSpark context Web UI available at http://192.168.1.6:4040Spark context available as &#x27;sc&#x27; (master = local[2], app id = local-1548169015800).Spark session available as &#x27;spark&#x27;.Welcome to      ____              __     / __/__  ___ _____/ /__    _\\ \\/ _ \\/ _ `/ __/  &#x27;_/   /___/ .__/\\_,_/_/ /_/\\_\\   version 2.2.3      /_/Using Scala version 2.11.8 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_181)Type in expressions to have them evaluated.Type :help for more information.scala&gt;\n\n启动成功，我们同时可以看看Spark context Web UI, http://localhost:4040\n","categories":["大数据","Spark"],"tags":["Spark"]},{"title":"npm太慢，配置国内淘宝源","url":"/2018/10/npm%E5%A4%AA%E6%85%A2%EF%BC%8C%E9%85%8D%E7%BD%AE%E5%9B%BD%E5%86%85%E6%B7%98%E5%AE%9D%E6%BA%90/","content":"今天在npm install前端代码时，下载速度非常慢，要下载的东西下载不下来。\nDownloading binary from https://github.com/sass/node-sass/releases/download/v4.7.2/darwin-x64-64_bin\n\n还遇到如下错误：\nERROR in ./~/css-loader?sourceMap!./~/resolve-url-loader?sourceMap!./~/sass-loader?sourceMap!./src/directives/plan-task/index.scss    Module build failed: Error: Cannot find module &#x27;node-sass&#x27;\n\n所以得考虑把npm的源换到国内，换成淘宝的镜像，命令如下：\nnpm config set registry https://registry.npm.taobao.org\n\n设置成功后，可以查看目前系统使用的源\nnpm config get registryhttps://registry.npm.taobao.org/\n\n像之前遇到的问题Cannot find module ‘node-sass’，我们可以再次执行npm install node-sass安装。\n如果不想使用淘宝源了，可以还原回去。\nnpm config delete registrynpm config delete disturl\n或者 \nnpm config edit \n找到淘宝那两行，删除即可。\n"},{"title":"安装HBase","url":"/2019/01/%E5%AE%89%E8%A3%85HBase/","content":"这篇文章将详细的讲述一下HBase的安装。\n下载安装包进入CDH Archive页面 http://archive.cloudera.com/cdh5/cdh/5/\n选择需要安装的HBase版本，这次安装选择的是hbase-1.2.0-cdh5.7.0\n下载链接 http://archive.cloudera.com/cdh5/cdh/5/hbase-1.2.0-cdh5.7.0.tar.gz\ncd /usr/local/hbasewget http://archive.cloudera.com/cdh5/cdh/5/hbase-1.2.0-cdh5.7.0.tar.gz\n\n解压安装包tar -zxvf hbase-1.2.0-cdh5.7.0.tar.gz\n\n添加环境变量vi ~/.bash_profile\n\n在文件里面加入以下内容\nexport HBASE_HOME=/usr/local/hbase/hbase-1.2.0-cdh5.7.0/\n\n然后执行source命令使配置文件生效\nsource ~/.bash_profileecho $HBASE_HOME/usr/local/hbase/hbase-1.2.0-cdh5.7.0/\n\n修改配置文件进入到$HBASE_HOME目录，并修改conf/hbase-env.sh配置文件\ncd $HBASE_HOMEvi conf/hbase-env.sh\n\n修改JAVA_HOME参数，指定JAVA_HOME的位置\nexport JAVA_HOME=/Library/Java/JavaVirtualMachines/jdk1.8.0_181.jdk/Contents/Home\n\n修改HBASE_MANAGES_ZK参数为false，不让HBase管理自己的Zookeeper实例，因为我们需要自己安装Zookeeper\nexport HBASE_MANAGES_ZK=false\n\n修改hbase-site.xml文件\n&lt;configuration&gt;        &lt;property&gt;                &lt;name&gt;hbase.rootdir&lt;/name&gt;                &lt;value&gt;hdfs://localhost:8020/hbase&lt;/value&gt;        &lt;/property&gt;        &lt;property&gt;                &lt;name&gt;hbase.cluster.distributed&lt;/name&gt;                &lt;value&gt;true&lt;/value&gt;        &lt;/property&gt;        &lt;property&gt;                &lt;name&gt;hbase.zookeeper.quorum&lt;/name&gt;                &lt;value&gt;localhost:2181&lt;/value&gt;        &lt;/property&gt;&lt;/configuration&gt;\n\n\nregionserver文件，如果是单机伪分布环境，则可以不用改，默认为localhost\n启动zookeeper在启动HBase之前，我们需要先启动zookeeper\ncd $ZK_HOMEsbin/zkServer.sh startJMX enabled by defaultUsing config: /usr/local/zookeeper/zookeeper-3.4.5-cdh5.7.0/sbin/../conf/zoo.cfgStarting zookeeper ... STARTED\n\njps50469 QuorumPeerMain\n\nzookeeper实例已经启动\n启动HBasecd $HBASE_HOMEbin/start-hbase.sh\n\nstarting master, logging to /usr/local/hbase/hbase-1.2.0-cdh5.7.0//logs/hbase-simon-master-localhost.outJava HotSpot(TM) 64-Bit Server VM warning: ignoring option PermSize=128m; support was removed in 8.0Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=128m; support was removed in 8.0starting regionserver, logging to /usr/local/hbase/hbase-1.2.0-cdh5.7.0//logs/hbase-simon-1-regionserver-localhost.out\n\n我们可以看到starting master和starting regionserver\njps50607 HMaster50703 HRegionServer\n\n说明HBase已经成功的启动了。\ncd $HADOOP_HOMEbin/hadoop fs -ls /19/01/22 13:42:14 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicableFound 4 itemsdrwxr-xr-x   - simon supergroup          0 2019-01-22 13:34 /hbase\n\n可以看到前面设置的hdfs://localhost:8020/hbase已经被成功创建了。\n通过Web UI查看HBASE信息在浏览器地址栏输入 http://localhost:60010\n\n\nHBase shellbin/hbase shell2019-01-22 13:41:26,984 INFO  [main] Configuration.deprecation: hadoop.native.lib is deprecated. Instead, use io.native.lib.available2019-01-22 13:41:28,153 WARN  [main] util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicableSLF4J: Class path contains multiple SLF4J bindings.SLF4J: Found binding in [jar:file:/usr/local/hbase/hbase-1.2.0-cdh5.7.0/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]SLF4J: Found binding in [jar:file:/usr/local/hadoop/hadoop-2.6.0-cdh5.7.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]HBase Shell; enter &#x27;help&lt;RETURN&gt;&#x27; for list of supported commands.Type &quot;exit&lt;RETURN&gt;&quot; to leave the HBase ShellVersion 1.2.0-cdh5.7.0, rUnknown, Wed Mar 23 11:46:29 PDT 2016hbase(main):001:0&gt;\n\n查看版本号\nhbase(main):001:0&gt; version1.2.0-cdh5.7.0, rUnknown, Wed Mar 23 11:46:29 PDT 2016\n\n查看状态\nhbase(main):002:0&gt; status1 active master, 0 backup masters, 1 servers, 0 dead, 2.0000 average load\n\n创建表member，包含info和address两个列蔟\nhbase(main):003:0&gt; create &#x27;member&#x27;, &#x27;info&#x27;, &#x27;address&#x27;0 row(s) in 2.3700 seconds=&gt; Hbase::Table - member\n\n查看数据库表列表\nhbase(main):004:0&gt; listTABLEmember1 row(s) in 0.0380 seconds=&gt; [&quot;member&quot;]\n\n查看member表信息\nhbase(main):005:0&gt; describe &#x27;member&#x27;Table member is ENABLEDmemberCOLUMN FAMILIES DESCRIPTION&#123;NAME =&gt; &#x27;address&#x27;, BLOOMFILTER =&gt; &#x27;ROW&#x27;, VERSIONS =&gt; &#x27;1&#x27;, IN_MEMORY =&gt; &#x27;false&#x27;, KEEP_DELETED_CELLS =&gt; &#x27;FALSE&#x27;, DATA_BLOCK_ENCODING =&gt; &#x27;NONE&#x27;, TTL =&gt; &#x27;FOREVER&#x27;, COMPRESSION =&gt; &#x27;NONE&#x27;, MIN_VERSIONS =&gt; &#x27;0&#x27;, BLOCKCACHE =&gt; &#x27;true&#x27;, BLOCKSIZE =&gt; &#x27;65536&#x27;, REPLICATION_SCOPE =&gt; &#x27;0&#x27;&#125;&#123;NAME =&gt; &#x27;info&#x27;, BLOOMFILTER =&gt; &#x27;ROW&#x27;, VERSIONS =&gt; &#x27;1&#x27;, IN_MEMORY =&gt; &#x27;false&#x27;, KEEP_DELETED_CELLS =&gt; &#x27;FALSE&#x27;, DATA_BLOCK_ENCODING =&gt; &#x27;NONE&#x27;, TTL =&gt; &#x27;FOREVER&#x27;, COMPRESSION =&gt; &#x27;NONE&#x27;, MIN_VERSIONS =&gt; &#x27;0&#x27;, BLOCKCACHE =&gt; &#x27;true&#x27;, BLOCKSIZE =&gt; &#x27;65536&#x27;, REPLICATION_SCOPE =&gt; &#x27;0&#x27;&#125;2 row(s) in 0.1090 seconds\n\n我们再次查看HBase Web UI\n\n\n发现在Tables标题栏里面已经多了一条记录了。\n清空数据表\nhbase(main):009:0&gt; truncate &#x27;member&#x27;Truncating &#x27;member&#x27; table (it may take a while): - Disabling table... - Truncating table...0 row(s) in 4.0020 seconds","categories":["大数据","HBase"]},{"title":"运行Spark Streaming案例","url":"/2019/01/%E8%BF%90%E8%A1%8CSpark-Streaming%E6%A1%88%E4%BE%8B/","content":"\nSpark Streaming is an extension of the core Spark API that enables scalable, high-throughput, fault-tolerant stream processing of live data streams.\n\n这篇文章将描述运行Spark官网上一个案例程序。\n执行spark-submit命令，查看命令使用方法。\ncd $SPARK_HOMEbin/spark-submitUsage: spark-submit [options] &lt;app jar | python file&gt; [app arguments]Usage: spark-submit --kill [submission ID] --master [spark://...]Usage: spark-submit --status [submission ID] --master [spark://...]Usage: spark-submit run-example [options] example-class [example args]Options:  --master MASTER_URL         spark://host:port, mesos://host:port, yarn, or local.  --deploy-mode DEPLOY_MODE   Whether to launch the driver program locally (&quot;client&quot;) or                              on one of the worker machines inside the cluster (&quot;cluster&quot;)                              (Default: client).  --class CLASS_NAME          Your application&#x27;s main class (for Java / Scala apps).  ...\n\n运行NetworkWordCount案例，我们需要指定master, class参数。\nbin/spark-submit --master local[2] --class org.apache.spark.examples.streaming.NetworkWordCount examples/jars/spark-examples_2.11-2.2.3.jarUsage: NetworkWordCount &lt;hostname&gt; &lt;port&gt;\n\n终端又给我们使用方法的提示，需要提供网络服务器地址hostname和端口port。\n如果网络上还没有提供服务时，执行上述命令，则会遇到如下错误。\n19/01/24 10:49:42 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connectingto localhost:9999 - java.net.ConnectException: Connection refused (Connection refused)\n\n所以，另开一个Shell终端，启动Netcat, 在端口9999上提供服务\nnc -lk 9999\n\n继续执行NetworkWordCount案例\nbin/spark-submit --master local[2] --class org.apache.spark.examples.streaming.NetworkWordCount examples/jars/spark-examples_2.11-2.2.3.jar localhost 9999Using Spark&#x27;s default log4j profile: org/apache/spark/log4j-defaults.properties19/01/24 11:04:46 INFO StreamingExamples: Setting log level to [WARN] for streaming example. To override add a custom log4j.properties to the classpath.19/01/24 11:04:47 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable19/01/24 11:04:47 WARN Utils: Your hostname, localhost resolves to a loopback address: 127.0.0.1; using 192.168.1.112 instead (on interface en0)19/01/24 11:04:47 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address19/01/24 11:04:47 WARN Utils: Service &#x27;SparkUI&#x27; could not bind on port 4040. Attempting port 4041.-------------------------------------------Time: 1548299089000 ms-------------------------------------------\n\n我们在Netcat终端上输入几个单词，看看统计的结果。\nhello hello spark spark spark\n\n很快的，我们在spark终端上就能看到统计的结果了\n-------------------------------------------Time: 1548298250000 ms-------------------------------------------(hello,2)(spark,3)\n\n我们再换一种方式，通过spark-shell的方式，输入代码，运行一下单词统计的程序。\nbin/spark-shell --master local[2]Using Spark&#x27;s default log4j profile: org/apache/spark/log4j-defaults.propertiesSetting default log level to &quot;WARN&quot;.To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).19/01/24 12:45:00 WARN Utils: Your hostname, localhost resolves to a loopback address: 127.0.0.1; using 192.168.1.112 instead (on interface en0)19/01/24 12:45:00 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address19/01/24 12:45:00 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicableSpark context Web UI available at http://192.168.1.112:4040Spark context available as &#x27;sc&#x27; (master = local[2], app id = local-1548305101841).Spark session available as &#x27;spark&#x27;.Welcome to      ____              __     / __/__  ___ _____/ /__    _\\ \\/ _ \\/ _ `/ __/  &#x27;_/   /___/ .__/\\_,_/_/ /_/\\_\\   version 2.2.3      /_/Using Scala version 2.11.8 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_181)Type in expressions to have them evaluated.Type :help for more information.scala&gt;\n\n我们按exmaples案例，输入以下代码。源代码可参看 https://github.com/apache/spark/blob/master/examples/src/main/scala/org/apache/spark/examples/streaming/NetworkWordCount.scala\nscala&gt;import org.apache.spark.storage.StorageLevelimport org.apache.spark.streaming.&#123;Seconds, StreamingContext&#125;val ssc = new StreamingContext(sc, Seconds(1))val lines = ssc.socketTextStream(&quot;localhost&quot;, 9999, StorageLevel.MEMORY_AND_DISK_SER)val words = lines.flatMap(_.split(&quot; &quot;))val wordCounts = words.map(x =&gt; (x, 1)).reduceByKey(_ + _)wordCounts.print()ssc.start()ssc.awaitTermination()\n\n在netcat终端输入\nhello hello spark spark spark\n\n我们一样能看到统计的结果\n-------------------------------------------Time: 1548305187000 ms-------------------------------------------(hello,2)(spark,3)","categories":["大数据","Spark"],"tags":["Spark Streaming"]},{"title":"在macOS上面安装elasticsearch组件及问题解决","url":"/2018/10/%E5%9C%A8macOS%E4%B8%8A%E9%9D%A2%E5%AE%89%E8%A3%85elasticsearch%E7%BB%84%E4%BB%B6%E5%8F%8A%E9%97%AE%E9%A2%98%E8%A7%A3%E5%86%B3/","content":"下载MAC版本安装包进入页面https://www.elastic.co/downloads/apm ，选择mac平台的安装包 下载文件到&#x2F;usr&#x2F;local&#x2F;elasticsearch&#x2F;elasticsearch-6.4.2.tar.gz\n解压文件$ sudo tar -zxvf elasticsearch-6.4.2.tar.gz\n\n运行$ bin/elasticsearchException in thread &quot;main&quot; java.nio.file.AccessDeniedException: /usr/local/elasticsearch/elasticsearch-6.4.2/config/jvm.options\tat sun.nio.fs.UnixException.translateToIOException(UnixException.java:84)\tat sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)\tat sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)\tat sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)\tat java.nio.file.Files.newByteChannel(Files.java:361)\tat java.nio.file.Files.newByteChannel(Files.java:407)\tat java.nio.file.spi.FileSystemProvider.newInputStream(FileSystemProvider.java:384)\tat java.nio.file.Files.newInputStream(Files.java:152)\tat org.elasticsearch.tools.launchers.JvmOptionsParser.main(JvmOptionsParser.java:60)\n\n\n权限不够，查看一下，登录用户是没有修改文件的权限的\n$ ls -l elasticsearch-6.4.2-rw-r--r--@  1 root  wheel   13675 Sep 26 21:30 LICENSE.txt-rw-r--r--@  1 root  wheel  401465 Sep 26 21:38 NOTICE.txt-rw-r--r--@  1 root  wheel    8511 Sep 26 21:30 README.textiledrwxr-xr-x@ 43 root  wheel    1376 Sep 26 21:38 bindrwxr-xr-x@  9 root  wheel     288 Sep 26 21:38 configdrwxr-xr-x@ 42 root  wheel    1344 Sep 26 21:38 libdrwxr-xr-x@  2 root  wheel      64 Sep 26 21:38 logsdrwxr-xr-x@ 27 root  wheel     864 Sep 26 21:38 modulesdrwxr-xr-x@  2 root  wheel      64 Sep 26 21:38 plugins\n\n修改权限\n$ sudo chown -R simon elasticsearch-6.4.2\n\n再次启动elasticsearch程序\n$ bin/elasticsearch[2018-10-17T10:04:22,029][INFO ][o.e.n.Node               ] [] initializing ...[2018-10-17T10:04:22,100][INFO ][o.e.e.NodeEnvironment    ] [VhQno38] using [1] data paths, mounts [[/ (/dev/disk1s1)]], net usable_space [353.6gb], net total_space [465.6gb], types [apfs][2018-10-17T10:04:22,101][INFO ][o.e.e.NodeEnvironment    ] [VhQno38] heap size [990.7mb], compressed ordinary object pointers [true][2018-10-17T10:04:22,104][INFO ][o.e.n.Node               ] [VhQno38] node name derived from node ID [VhQno38hQfO03l3Sxt6Bnw]; set [node.name] to override[2018-10-17T10:04:22,104][INFO ][o.e.n.Node               ] [VhQno38] version[6.4.2], pid[12313], build[default/tar/04711c2/2018-09-26T13:34:09.098244Z], OS[Mac OS X/10.13.6/x86_64], JVM[Oracle Corporation/Java HotSpot(TM) 64-Bit Server VM/1.8.0_181/25.181-b13][2018-10-17T10:04:22,104][INFO ][o.e.n.Node               ] [VhQno38] JVM arguments [-Xms1g, -Xmx1g, -XX:+UseConcMarkSweepGC, -XX:CMSInitiatingOccupancyFraction=75, -XX:+UseCMSInitiatingOccupancyOnly, -XX:+AlwaysPreTouch, -Xss1m, -Djava.awt.headless=true, -Dfile.encoding=UTF-8, -Djna.nosys=true, -XX:-OmitStackTraceInFastThrow, -Dio.netty.noUnsafe=true, -Dio.netty.noKeySetOptimization=true, -Dio.netty.recycler.maxCapacityPerThread=0, -Dlog4j.shutdownHookEnabled=false, -Dlog4j2.disable.jmx=true, -Djava.io.tmpdir=/var/folders/2h/4vmz9_k14pb4x5338vs98nmw0000gn/T/elasticsearch.98Xlp9JQ, -XX:+HeapDumpOnOutOfMemoryError, -XX:HeapDumpPath=data, -XX:ErrorFile=logs/hs_err_pid%p.log, -XX:+PrintGCDetails, -XX:+PrintGCDateStamps, -XX:+PrintTenuringDistribution, -XX:+PrintGCApplicationStoppedTime, -Xloggc:logs/gc.log, -XX:+UseGCLogFileRotation, -XX:NumberOfGCLogFiles=32, -XX:GCLogFileSize=64m, -Des.path.home=/usr/local/elasticsearch/elasticsearch-6.4.2, -Des.path.conf=/usr/local/elasticsearch/elasticsearch-6.4.2/config, -Des.distribution.flavor=default, -Des.distribution.type=tar][2018-10-17T10:04:24,127][INFO ][o.e.p.PluginsService     ] [VhQno38] loaded module [aggs-matrix-stats][2018-10-17T10:04:24,127][INFO ][o.e.p.PluginsService     ] [VhQno38] loaded module [analysis-common][2018-10-17T10:04:24,127][INFO ][o.e.p.PluginsService     ] [VhQno38] loaded module [ingest-common][2018-10-17T10:04:24,127][INFO ][o.e.p.PluginsService     ] [VhQno38] loaded module [lang-expression][2018-10-17T10:04:24,127][INFO ][o.e.p.PluginsService     ] [VhQno38] loaded module [lang-mustache][2018-10-17T10:04:24,128][INFO ][o.e.p.PluginsService     ] [VhQno38] loaded module [lang-painless][2018-10-17T10:04:24,128][INFO ][o.e.p.PluginsService     ] [VhQno38] loaded module [mapper-extras][2018-10-17T10:04:24,128][INFO ][o.e.p.PluginsService     ] [VhQno38] loaded module [parent-join][2018-10-17T10:04:24,128][INFO ][o.e.p.PluginsService     ] [VhQno38] loaded module [percolator][2018-10-17T10:04:24,128][INFO ][o.e.p.PluginsService     ] [VhQno38] loaded module [rank-eval][2018-10-17T10:04:24,128][INFO ][o.e.p.PluginsService     ] [VhQno38] loaded module [reindex][2018-10-17T10:04:24,128][INFO ][o.e.p.PluginsService     ] [VhQno38] loaded module [repository-url][2018-10-17T10:04:24,129][INFO ][o.e.p.PluginsService     ] [VhQno38] loaded module [transport-netty4][2018-10-17T10:04:24,129][INFO ][o.e.p.PluginsService     ] [VhQno38] loaded module [tribe][2018-10-17T10:04:24,129][INFO ][o.e.p.PluginsService     ] [VhQno38] loaded module [x-pack-core][2018-10-17T10:04:24,130][INFO ][o.e.p.PluginsService     ] [VhQno38] loaded module [x-pack-deprecation][2018-10-17T10:04:24,130][INFO ][o.e.p.PluginsService     ] [VhQno38] loaded module [x-pack-graph][2018-10-17T10:04:24,130][INFO ][o.e.p.PluginsService     ] [VhQno38] loaded module [x-pack-logstash][2018-10-17T10:04:24,131][INFO ][o.e.p.PluginsService     ] [VhQno38] loaded module [x-pack-ml][2018-10-17T10:04:24,131][INFO ][o.e.p.PluginsService     ] [VhQno38] loaded module [x-pack-monitoring][2018-10-17T10:04:24,131][INFO ][o.e.p.PluginsService     ] [VhQno38] loaded module [x-pack-rollup][2018-10-17T10:04:24,131][INFO ][o.e.p.PluginsService     ] [VhQno38] loaded module [x-pack-security][2018-10-17T10:04:24,131][INFO ][o.e.p.PluginsService     ] [VhQno38] loaded module [x-pack-sql][2018-10-17T10:04:24,131][INFO ][o.e.p.PluginsService     ] [VhQno38] loaded module [x-pack-upgrade][2018-10-17T10:04:24,131][INFO ][o.e.p.PluginsService     ] [VhQno38] loaded module [x-pack-watcher][2018-10-17T10:04:24,132][INFO ][o.e.p.PluginsService     ] [VhQno38] no plugins loaded[2018-10-17T10:04:27,786][INFO ][o.e.x.s.a.s.FileRolesStore] [VhQno38] parsed [0] roles from file [/usr/local/elasticsearch/elasticsearch-6.4.2/config/roles.yml][2018-10-17T10:04:28,270][INFO ][o.e.x.m.j.p.l.CppLogMessageHandler] [controller/12332] [Main.cc@109] controller (64 bit): Version 6.4.2 (Build 660eefe6f2ea55) Copyright (c) 2018 Elasticsearch BV[2018-10-17T10:04:28,685][DEBUG][o.e.a.ActionModule       ] Using REST wrapper from plugin org.elasticsearch.xpack.security.Security[2018-10-17T10:04:28,911][INFO ][o.e.d.DiscoveryModule    ] [VhQno38] using discovery type [zen][2018-10-17T10:04:30,161][INFO ][o.e.n.Node               ] [VhQno38] initialized[2018-10-17T10:04:30,161][INFO ][o.e.n.Node               ] [VhQno38] starting ...[2018-10-17T10:04:30,380][INFO ][o.e.t.TransportService   ] [VhQno38] publish_address &#123;127.0.0.1:9300&#125;, bound_addresses &#123;[::1]:9300&#125;, &#123;127.0.0.1:9300&#125;[2018-10-17T10:04:33,445][INFO ][o.e.c.s.MasterService    ] [VhQno38] zen-disco-elected-as-master ([0] nodes joined)[, ], reason: new_master &#123;VhQno38&#125;&#123;VhQno38hQfO03l3Sxt6Bnw&#125;&#123;jy1_1dpHToOxN2CCtH5ztw&#125;&#123;127.0.0.1&#125;&#123;127.0.0.1:9300&#125;&#123;ml.machine_memory=8589934592, xpack.installed=true, ml.max_open_jobs=20, ml.enabled=true&#125;[2018-10-17T10:04:33,450][INFO ][o.e.c.s.ClusterApplierService] [VhQno38] new_master &#123;VhQno38&#125;&#123;VhQno38hQfO03l3Sxt6Bnw&#125;&#123;jy1_1dpHToOxN2CCtH5ztw&#125;&#123;127.0.0.1&#125;&#123;127.0.0.1:9300&#125;&#123;ml.machine_memory=8589934592, xpack.installed=true, ml.max_open_jobs=20, ml.enabled=true&#125;, reason: apply cluster state (from master [master &#123;VhQno38&#125;&#123;VhQno38hQfO03l3Sxt6Bnw&#125;&#123;jy1_1dpHToOxN2CCtH5ztw&#125;&#123;127.0.0.1&#125;&#123;127.0.0.1:9300&#125;&#123;ml.machine_memory=8589934592, xpack.installed=true, ml.max_open_jobs=20, ml.enabled=true&#125; committed version [1] source [zen-disco-elected-as-master ([0] nodes joined)[, ]]])[2018-10-17T10:04:33,473][INFO ][o.e.x.s.t.n.SecurityNetty4HttpServerTransport] [VhQno38] publish_address &#123;127.0.0.1:9200&#125;, bound_addresses &#123;[::1]:9200&#125;, &#123;127.0.0.1:9200&#125;[2018-10-17T10:04:33,474][INFO ][o.e.n.Node               ] [VhQno38] started[2018-10-17T10:04:33,482][WARN ][o.e.x.s.a.s.m.NativeRoleMappingStore] [VhQno38] Failed to clear cache for realms [[]][2018-10-17T10:04:33,536][INFO ][o.e.g.GatewayService     ] [VhQno38] recovered [0] indices into cluster_state[2018-10-17T10:04:33,699][INFO ][o.e.c.m.MetaDataIndexTemplateService] [VhQno38] adding template [.triggered_watches] for index patterns [.triggered_watches*][2018-10-17T10:04:33,721][INFO ][o.e.c.m.MetaDataIndexTemplateService] [VhQno38] adding template [.watches] for index patterns [.watches*][2018-10-17T10:04:33,774][INFO ][o.e.c.m.MetaDataIndexTemplateService] [VhQno38] adding template [.watch-history-9] for index patterns [.watcher-history-9*][2018-10-17T10:04:33,807][INFO ][o.e.c.m.MetaDataIndexTemplateService] [VhQno38] adding template [.monitoring-logstash] for index patterns [.monitoring-logstash-6-*][2018-10-17T10:04:33,870][INFO ][o.e.c.m.MetaDataIndexTemplateService] [VhQno38] adding template [.monitoring-es] for index patterns [.monitoring-es-6-*][2018-10-17T10:04:33,893][INFO ][o.e.c.m.MetaDataIndexTemplateService] [VhQno38] adding template [.monitoring-beats] for index patterns [.monitoring-beats-6-*][2018-10-17T10:04:33,912][INFO ][o.e.c.m.MetaDataIndexTemplateService] [VhQno38] adding template [.monitoring-alerts] for index patterns [.monitoring-alerts-6][2018-10-17T10:04:33,932][INFO ][o.e.c.m.MetaDataIndexTemplateService] [VhQno38] adding template [.monitoring-kibana] for index patterns [.monitoring-kibana-6-*][2018-10-17T10:04:34,025][INFO ][o.e.l.LicenseService     ] [VhQno38] license [b8e36ec7-af24-44d2-8e35-2a8c543c8aaf] mode [basic] - valid\n\n当看到组件 started 后，代表elasticsearch已经成功启动。\n[2018-10-17T10:04:33,474][INFO ][o.e.n.Node               ] [VhQno38] started"},{"title":"MacOSX平台安装MongoDB","url":"/2018/10/MacOSX%E5%B9%B3%E5%8F%B0%E5%AE%89%E8%A3%85MongoDB/","content":"下载MongoDB从官网下载MongoDB Mac版本，下载地址： https://www.mongodb.com/download-center/community?jmp=docs\ncd /usr/local/mongodbsudo curl -O https://fastdl.mongodb.org/osx/mongodb-osx-ssl-x86_64-4.0.3.tgz\n\n\n解压到&#x2F;usr&#x2F;localsudo tar -zxvf mongodb-osx-ssl-x86_64-4.0.3.tgz\n\n创建数据库存储目录默认目录是&#x2F;data&#x2F;db，并把权限授予给用户simon\n如果数据库目录不是&#x2F;data&#x2F;db，可以通过 –dbpath 来指定。\nsudo mkdir -p /data/dbsudo chown -R simon /data\n\n启动mongodbsudo bin/mongod\n\n进入客户端$ mongoMongoDB shell version v4.0.2connecting to: mongodb://127.0.0.1:27017MongoDB server version: 4.0.2Server has startup warnings:2018-10-25T20:07:13.893+0800 I CONTROL  [initandlisten]2018-10-25T20:07:13.893+0800 I CONTROL  [initandlisten] ** WARNING: Access control is not enabled for the database.2018-10-25T20:07:13.893+0800 I CONTROL  [initandlisten] **          Read and write access to data and configuration is unrestricted.2018-10-25T20:07:13.893+0800 I CONTROL  [initandlisten]2018-10-25T20:07:13.893+0800 I CONTROL  [initandlisten] ** WARNING: This server is bound to localhost.2018-10-25T20:07:13.893+0800 I CONTROL  [initandlisten] **          Remote systems will be unable to connect to this server.2018-10-25T20:07:13.893+0800 I CONTROL  [initandlisten] **          Start the server with --bind_ip &lt;address&gt; to specify which IP2018-10-25T20:07:13.893+0800 I CONTROL  [initandlisten] **          addresses it should serve responses from, or with --bind_ip_all to2018-10-25T20:07:13.893+0800 I CONTROL  [initandlisten] **          bind to all interfaces. If this behavior is desired, start the2018-10-25T20:07:13.893+0800 I CONTROL  [initandlisten] **          server with --bind_ip 127.0.0.1 to disable this warning.2018-10-25T20:07:13.893+0800 I CONTROL  [initandlisten]---Enable MongoDB&#x27;s free cloud-based monitoring service, which will then receive and displaymetrics about your deployment (disk utilization, CPU, operation statistics, etc).The monitoring data will be available on a MongoDB website with a unique URL accessible to youand anyone you share the URL with. MongoDB may use this information to make productimprovements and to suggest MongoDB products and deployment options to you.To enable free monitoring, run the following command: db.enableFreeMonitoring()To permanently disable this reminder, run the following command: db.disableFreeMonitoring()---&gt; 1 + 12&gt;\n\n可视化客户端可以下载Robo 3T，www.robomongo.org\n","categories":["Database","MongoDB"],"tags":["NoSQL","MongoDB"]},{"title":"如何在Anaconda中修改python版本","url":"/2019/03/%E5%A6%82%E4%BD%95%E5%9C%A8Anaconda%E4%B8%AD%E4%BF%AE%E6%94%B9python%E7%89%88%E6%9C%AC/","content":"如何安装Anaconda及基本的使用请参考 macOS下Anaconda的安装及环境切换（Python2&#x2F;Python3）\n我在安装Anaconda的时候，软件自带的版本是Python3.6的，如果我现在需要使用Python2.7或者Python3.7，应该怎么操作呢？\n如果我们在Anaconda Navigator控制台上做Create操作，环境是可以成功创建，但环境里面携带的包是非常少的。\n所以可以参考如下方式进行Python版本修改。\nconda create -n python37 python=3.7 anaconda\n\n如果conda的版本比较旧的话，会出现如下提示：\n==&gt; WARNING: A newer version of conda exists. &lt;==  current version: 4.6.2  latest version: 4.6.7Please update conda by running    $ conda update -n base -c defaults conda\n\n所以我们可以先执行命令\nconda update -n base -c defaults conda\n\n然后再执行conda create命令\nconda create -n python37 python=3.7 anacondaCollecting package metadata: doneSolving environment: done## Package Plan ##  environment location: /Users/simon/anaconda3/envs/python37  added / updated specs:    - anaconda    - python=3.7The following packages will be downloaded:    package                    |            build    ---------------------------|-----------------    mkl-2019.1                 |              144       154.4 MB    ------------------------------------------------------------                                           Total:       154.4 MBThe following NEW packages will be INSTALLED:  alabaster          pkgs/main/osx-64::alabaster-0.7.12-py37_0  anaconda           pkgs/main/osx-64::anaconda-2018.12-py37_0  ...  ...  Proceed ([y]/n)? y      Downloading and Extracting Packages  mkl-2019.1           | 154.4 MB  | ###################################################################################################### | 100%  Preparing transaction: done  Verifying transaction: done  Executing transaction: done  #  # To activate this environment, use:  # &gt; source activate python37  #  # To deactivate an active environment, use:  # &gt; source deactivate  #\n\n按照上面的提示，我们可以执行如下命令激活或失效某个python环境。\n\n激活\n\nsource activate python37\n\n\n失效\n\nsource deactivate\n","categories":["Python"],"tags":["python"]},{"title":"连接Mysql时出现too many connections的一些解决办法","url":"/2018/10/%E8%BF%9E%E6%8E%A5Mysql%E6%97%B6%E5%87%BA%E7%8E%B0too-many-connections%E7%9A%84%E4%B8%80%E4%BA%9B%E8%A7%A3%E5%86%B3%E5%8A%9E%E6%B3%95/","content":"通过Navicat客户端连接Mysql数据库时，出现too many connections的提示，可以进行如下操作\n登录mysql数据库# mysql -u root -pEnter password:Welcome to the MySQL monitor.  Commands end with ; or \\g.Your MySQL connection id is 2965Server version: 5.7.18 MySQL Community Server (GPL)Copyright (c) 2000, 2017, Oracle and/or its affiliates. All rights reserved.Oracle is a registered trademark of Oracle Corporation and/or itsaffiliates. Other names may be trademarks of their respectiveowners.Type &#x27;help;&#x27; or &#x27;\\h&#x27; for help. Type &#x27;\\c&#x27; to clear the current input statement.\n\n查看连接数mysql&gt; show processlist;+------+------+---------------------+--------------+---------+------+----------+------------------+| Id   | User | Host                | db           | Command | Time | State    | Info             |+------+------+---------------------+--------------+---------+------+----------+------------------+| 2821 | root | 192.168.1.124:58526 | testdb       | Sleep   | 7180 |          | NULL             || 2822 | root | 192.168.1.124:58527 | testdb       | Sleep   | 7180 |          | NULL             || 2840 | root | 192.168.1.124:58741 | testdb       | Sleep   | 5790 |          | NULL             || 2841 | root | 192.168.1.124:58742 | testdb       | Sleep   | 5820 |          | NULL             |\n\n将会发现有很多连接是处于Sleep状态的，这些连接是暂时没有用的，可以删除掉。如何删除请继续往下看。\n查看最大连接数mysql&gt; show variables like &quot;max_connections&quot;;+-----------------+-------+| Variable_name   | Value |+-----------------+-------+| max_connections | 151   |+-----------------+-------+1 row in set (0.01 sec)\n\n当processlist的数量大于查询到的max_connections时，就会出现too many connections的提示信息。\n修改最大连接数set GLOBAL max_connections=1000;\n\n这不是一劳永逸的方法，应该要让它自动杀死那些sleep的进程。\n查看关闭一个连接的等待时间，并修改show global variables like &#x27;wait_timeout&#x27;;set global wait_timeout=300;\n\n修改这个数值，这里可以随意，最好控制在几分钟内。\n其他待验证set global interactive_timeout&#x3D;500;修改这个数值，表示mysql在关闭一个连接之前要等待的秒数，至此可以让mysql自动关闭那些没用的连接，但要注意的是，正在使用的连接到了时间也会被关闭，因此这个时间值要合适\n批量kill之前没用的sleep连接，在网上搜索的方法对我都不奏效，因此只好使用最笨的办法，一个一个kill\nselect concat(‘KILL ‘,id,’;’) from information_schema.processlist where user&#x3D;’root’;\n先把要kill的连接id都查询出来复制中间的kill id;内容到word文档替换掉符号“|”和回车符（在word中查询^p即可查询到回车符）把修改过的内容复制回终端，最后按回车执行\n产生这种问题的原因是：\n连接数超过了 MySQL 设置的值，与 max_connections 和 wait_timeout  都有关系。wait_timeout 的值越大，连接的空闲等待就越长，这样就会造成当前连接数越大。\n解决方法：\n修改MySQL配置文件&#x2F;etc&#x2F;my.cnf,设置成max_connections&#x3D;1000,wait_timeout&#x3D;5。如果没有此项设置可以自行添加，修改后重启MySQL服务即可。要不经常性报此错误，则要对服务器作整体性能优化\n注：\n为了防止发生too many connections时候无法登录的问题，mysql manual有如下的说明：\nmysqld actually allows max_connections+1 clients to connect. The extra connection is reserved for use by accounts that have theSUPER privilege. By granting the SUPER privilege to administrators and not to normal users (who should not need it), an administrator can connect to the server and use SHOW PROCESSLIST to diagnose problems even if the maximum number of unprivileged clients are connected.\n因此, 必须只赋予root用户的SUPER权限，同时所有数据库连接的帐户不能赋予SUPER权限。前面说到的报错后无法登录就是由于我们的应用程序直接配置的root用户\n总结，解决问题的最终方法：\n1.修改配置文件&#x2F;etc&#x2F;my.cnf,调整连接参数\n2.检查程序代码，对于没有关闭的链接及时进行关闭\n","categories":["Database","MySQL"],"tags":["mysql"]},{"title":"如何使用Sublime Text3格式化json","url":"/2018/11/%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8Sublime-Text3%E6%A0%BC%E5%BC%8F%E5%8C%96json/","content":"有一些网站提供了格式化json的功能，如果没有网络时，或者json字符串很长时，还是需要一个可以格式json的软件，这里我们使用Sublime Text3来格式化json。\n下载Sublime Text3http://www.sublimetext.com/3\n我们选择OS X系统\n安装Package Control使用快捷键ctrl+ 或 View -&gt; Show Console 打开输入框，复制以下内容到输入框。\nSUBLIME TEXT 3  \n安装pretty json快捷键ctrl+shift+p, 打开面板，选中Package Control: Install Package并回车，然后再输入pretty json，找到插件回车安装即可。\n使用pretty json使用ctrl+alt+j快捷键来格式化当前页面的内容。\n","categories":["Tools","Sublime"]},{"title":"重启mysql报错 ERROR 2002 (HY000)","url":"/2018/09/%E9%87%8D%E5%90%AFmysql%E6%8A%A5%E9%94%99-ERROR-2002-HY000/","content":"mysql在非正常关闭后，再次启动时，会报错。客户端也连接不上。\n连接时的错误信息：\nERROR 2002 (HY000): Can&#x27;t connect to local MySQL server through socket &#x27;/var/lib/mysql/mysql.sock&#x27; (111)\n\n尝试重启mysql服务\n# systemctl status mysqld.service● mysqld.service - SYSV: MySQL database server.   Loaded: loaded (/etc/rc.d/init.d/mysqld; bad; vendor preset: disabled)   Active: failed (Result: exit-code) since 二 2018-09-25 10:48:39 CST; 1min 28s ago     Docs: man:systemd-sysv-generator(8)  Process: 3351 ExecStart=/etc/rc.d/init.d/mysqld start (code=exited, status=1/FAILURE)9月 25 10:48:38 deploy systemd[1]: Starting SYSV: MySQL database server....9月 25 10:48:39 deploy mysqld[3351]: MySQL Daemon failed to start.9月 25 10:48:39 deploy mysqld[3351]: Starting mysqld:  [FAILED]9月 25 10:48:39 deploy systemd[1]: mysqld.service: control process exited, code=exited status=19月 25 10:48:39 deploy systemd[1]: Failed to start SYSV: MySQL database server..9月 25 10:48:39 deploy systemd[1]: Unit mysqld.service entered failed state.9月 25 10:48:39 deploy systemd[1]: mysqld.service failed.\n\n查看错误日志\n# journalctl -xe9月 25 10:45:47 deploy systemd[1]: mysqld.service: control process exited, code=exited status=19月 25 10:45:47 deploy systemd[1]: Failed to start SYSV: MySQL database server..-- Subject: Unit mysqld.service has failed-- Defined-By: systemd-- Support: http://lists.freedesktop.org/mailman/listinfo/systemd-devel---- Unit mysqld.service has failed.---- The result is failed.\n\n解决方案\n在mysql非正常关闭时，&#x2F;var&#x2F;run&#x2F;mysqld会消失掉。我们查看&#x2F;var&#x2F;run&#x2F;mysqld文件夹是否不存在，如果是，那重新建立此文件夹&#x2F;var&#x2F;run&#x2F;mysqld，并设置权限chown mysql.mysql &#x2F;var&#x2F;run&#x2F;mysqld\n再次重启，正常启动\n","categories":["Database","MySQL"]},{"title":"SpringBean生命周期","url":"/2019/06/SpringBean%E7%94%9F%E5%91%BD%E5%91%A8%E6%9C%9F/","content":"最新理解\n稍微总结一下 Bean 的生命周期，在 BeanFactory 这个接口的注释里面，也可以看到。\n\n实现一堆 Aware 接口。（当实现了某个 Aware 接口后，此 Bean 就能通过 setXXX 的方法获取到容器中存在的对象）\n执行 BeanPostProcessor 的 before 方法\n执行 init 方法\n执行 BeanPostProcessor 的 after 方法\n调用 DisposableBean destroy 方法\n调用 destroy 方法\n\n下面代码里面的 @PostConstruct 和 @PreDestroy 是 javax 中的注解，和 Spring 里面的 initMethod 和 destroyMethod 还是有一定差异的。后者是 @Bean 注解里面的属性，或者在 xml 文件中定义。\n\nSpring框架中，一旦把一个Bean纳入Spring IOC容器之中，这个Bean的生命周期就会交由容器进行管理，一般担当管理角色的是BeanFactory或者ApplicationContext。\n下面以BeanFactory为例，说明一个Bean的生命周期活动。\n\nBean的建立，由BeanFactory读取Bean定义文件，并生成各个实例\n\nSetter注入，执行Bean的属性依赖注入\n\nBeanNameAware的setBeanName(), 如果实现该接口，则执行其setBeanName方法\n\nBeanFactoryAware的setBeanFactory()，如果实现该接口，则执行其setBeanFactory方法\n\nBeanPostProcessor的processBeforeInitialization()，如果有关联的processor，则在Bean初始化之前都会执行这个实例的processBeforeInitialization()方法\n\nBean定义文件中定义init-method\n\nInitializingBean的afterPropertiesSet()，如果实现了该接口，则执行其afterPropertiesSet()方法\n\nBeanPostProcessors的processAfterInitialization()，如果有关联的processor，则在Bean初始化之前都会执行这个实例的processAfterInitialization()方法\n\nBean定义文件中定义destroy-method，在容器关闭时，可以在 Bean 定义文件中使用 destory-method 定义的方法\n\nDisposableBean的destroy()，在容器关闭时，如果 Bean 类实现了该接口，则执行它的 destroy() 方法\n\n\n如果使用ApplicationContext来维护一个Bean的生命周期，则基本上与上边的流程相同，只不过在执行BeanNameAware的setBeanName()后，若有Bean类实现了org.springframework.context.ApplicationContextAware接口，则执行其setApplicationContext()方法，然后再进行BeanPostProcessors的processBeforeInitialization()\n实际上，ApplicationContext除了向BeanFactory那样维护容器外，还提供了更加丰富的框架功能，如Bean的消息，事件处理机制等。\n下面通过代码展示一下各个方法被调用的顺序。\nMyBean.java\n\n@Componentpublic class MyBean implements BeanNameAware, BeanFactoryAware, ApplicationContextAware,        InitializingBean, DisposableBean &#123;    private static final Logger logger = LoggerFactory.getLogger(MyBean.class);    public MyBean() &#123;        logger.info(&quot;1. MyBean Constructor&quot;);    &#125;    @Override    public void setBeanName(String name) &#123;        logger.info(&quot;2. BeanNameAware.setBeanName&quot;);    &#125;    @Override    public void setBeanFactory(BeanFactory beanFactory) throws BeansException &#123;        logger.info(&quot;3. BeanFactoryAware.setBeanFactory&quot;);    &#125;    @Override    public void setApplicationContext(ApplicationContext applicationContext) throws BeansException &#123;        logger.info(&quot;4. ApplicationContextAware.setApplicationContext&quot;);    &#125;    @PostConstruct    public void postConstructMethod() &#123;        logger.info(&quot;6. javax&#x27;s PostConstruct&quot;);    &#125;    @Override    public void afterPropertiesSet() throws Exception &#123;        logger.info(&quot;7. InitializingBean.afterPropertiesSet&quot;);    &#125;    @PreDestroy    public void preDestroyMethod() &#123;        logger.info(&quot;9. javax&#x27;s PreDestroy&quot;);    &#125;    @Override    public void destroy() &#123;        logger.info(&quot;10. DisposableBean.destroy&quot;);    &#125;&#125;\nMyBeanPostProcessor.java\n\n@Componentpublic class MyBeanPostProcessor implements BeanPostProcessor &#123;    private static final Logger logger = LoggerFactory.getLogger(MyBeanPostProcessor.class);    @Override    public Object postProcessBeforeInitialization(Object bean, String beanName) throws BeansException &#123;        if (bean instanceof MyBean) &#123;            logger.info(&quot;5. BeanPostProcessor.postProcessBeforeInitialization&quot;);        &#125;        return bean;    &#125;    @Override    public Object postProcessAfterInitialization(Object bean, String beanName) throws BeansException &#123;        if (bean instanceof MyBean) &#123;            logger.info(&quot;8. BeanPostProcessor.postProcessAfterInitialization&quot;);        &#125;        return bean;    &#125;    &#125;\n\n运行结果\n1. MyBean Constructor2. BeanNameAware.setBeanName3. BeanFactoryAware.setBeanFactory4. ApplicationContextAware.setApplicationContext5. BeanPostProcessor.postProcessBeforeInitialization6. javax&#x27;s PostConstruct7. InitializingBean.afterPropertiesSet8. BeanPostProcessor.postProcessAfterInitializationStarted SpringBeanLifecycleApplication in 0.632 seconds (JVM running for 1.022)9. javax&#x27;s PreDestroy10. DisposableBean.destroy\n","categories":["Java","Spring"],"tags":["spring"]},{"title":"mac OS安装npm","url":"/2019/06/mac-OS%E5%AE%89%E8%A3%85npm/","content":"检查brew -v是否安装了homebrew这个macOS缺失的软件包的管理器。如果安装，跳转到第3步，否则跳转到第二步；\n安装homebrew。安装跳转到官网指导。等待安装好之后，输入brew -v，如果出现版本号说明已经安装好：\nbrew -vHomebrew 2.1.6Homebrew/homebrew-core (git revision 10e6; last commit 2019-06-23)\n\n执行brew update更新homebrew；\n执行命令行brew install npm安装npm。执行npm -v即可看到安装好的npm版本，如下：\nnpm -v6.9.0\n\n接下来就是npm的使用了，详情请看npm官网\n","tags":["npm"]},{"title":"下载Sublime Text3","url":"/2018/11/%E4%B8%8B%E8%BD%BDSublime-Text3/","content":"有一些网站提供了格式化json的功能，如果没有网络时，或者json字符串很长时，还是需要一个可以格式json的软件，这里我们使用Sublime Text3来格式化json。\n下载Sublime Text3http://www.sublimetext.com/3\n我们选择OS X系统\n安装Package Control使用快捷键ctrl+ 或 View -&gt; Show Console 打开输入框，复制以下内容到输入框。\nSUBLIME TEXT 3\nimport urllib.request,os,hashlib; h = &#x27;6f4c264a24d933ce70df5dedcf1dcaee&#x27; + &#x27;ebe013ee18cced0ef93d5f746d80ef60&#x27;; pf = &#x27;Package Control.sublime-package&#x27;; ipp = sublime.installed_packages_path(); urllib.request.install_opener( urllib.request.build_opener( urllib.request.ProxyHandler()) ); by = urllib.request.urlopen( &#x27;http://packagecontrol.io/&#x27; + pf.replace(&#x27; &#x27;, &#x27;%20&#x27;)).read(); dh = hashlib.sha256(by).hexdigest(); print(&#x27;Error validating download (got %s instead of %s), please try manual install&#x27; % (dh, h)) if dh != h else open(os.path.join( ipp, pf), &#x27;wb&#x27; ).write(by)\n\n安装pretty json快捷键ctrl+shift+p,打开面板，选中Package Control: Install Package并回车，然后再输入pretty json，找到插件回车安装即可。\n使用pretty json使用ctrl+alt+j快捷键来格式化当前页面的内容。\n","categories":["Tools","Sublime"]},{"title":"Codility Lesson 1 - BinaryGap","url":"/2019/05/Codility-Lesson-1-BinaryGap/","content":"BinaryGapFind longest sequence of zeros in binary representation of an integer.\nA binary gap within a positive integer N is any maximal sequence of consecutive zeros that is surrounded by ones at both ends in the binary representation of N.\nFor example, number 9 has binary representation 1001 and contains a binary gap of length 2. The number 529 has binary representation 1000010001 and contains two binary gaps: one of length 4 and one of length 3. The number 20 has binary representation 10100 and contains one binary gap of length 1. The number 15 has binary representation 1111 and has no binary gaps. The number 32 has binary representation 100000 and has no binary gaps.\nWrite a function:\nclass Solution &#123; public int solution(int N); &#125;\n\nthat, given a positive integer N, returns the length of its longest binary gap. The function should return 0 if N doesn’t contain a binary gap.\nFor example, given N &#x3D; 1041 the function should return 5, because N has binary representation 10000010001 and so its longest binary gap is of length 5. Given N &#x3D; 32 the function should return 0, because N has binary representation ‘100000’ and thus no binary gaps.\nWrite an efficient algorithm for the following assumptions:\nN is an integer within the range [1..2,147,483,647].\nCopyright 2009–2019 by Codility Limited. All Rights Reserved. Unauthorized copying, publication or disclosure prohibited.\nSolution数字循环除以2，直到0。每次取余，以统计1之间的0的个数。\nclass Solution &#123;    public int solution(int N) &#123;        int count = 0;        int maxGap = 0;        boolean first = true;   // 未找到第一个1时                while (N != 0) &#123;            if (first) &#123;                if (N % 2 == 1) &#123;                    first = false;                &#125;            &#125; else &#123;                if (N % 2 == 0) &#123;                    count ++;                &#125; else &#123;                    // 再次遇到1时，计算之间的gap                    if (count &gt; maxGap) &#123;                        maxGap = count;                    &#125;                    count = 0;                &#125;            &#125;            // 除以2            N &gt;&gt;= 1;        &#125;        return maxGap;    &#125;        public static void main(String[] args) &#123;            Solution solution = new Solution();            System.out.println(&quot;Number 9 (&quot; + Integer.toBinaryString(9) + &quot;) gap: &quot; + solution.solution(9));            System.out.println(&quot;Number 529 (&quot; + Integer.toBinaryString(529) + &quot;) gap: &quot; + solution.solution(529));            System.out.println(&quot;Number 20 (&quot; + Integer.toBinaryString(20) + &quot;) gap: &quot; + solution.solution(20));            System.out.println(&quot;Number 15 (&quot; + Integer.toBinaryString(15) + &quot;) gap: &quot; + solution.solution(15));            System.out.println(&quot;Number 32 (&quot; + Integer.toBinaryString(32) + &quot;) gap: &quot; + solution.solution(32));        &#125;&#125;\n\n结果如下：\nNumber 9 (1001) gap: 2Number 529 (1000010001) gap: 4Number 20 (10100) gap: 1Number 15 (1111) gap: 0Number 32 (100000) gap: 0","categories":["算法"],"tags":["codility","binaryGap"]},{"title":"zsh: command not found解决方案","url":"/2019/06/zsh-command-not-found%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88/","content":"zsh使用时问题当我们安装使用了zsh以后，刚开始使用的时候，有可能会遇到command not found的错误信息提示。\n如果安装zsh请参考文章Mac下安装zsh\n解决方案把bash shell中的.bash_profile里的全部环境变量加入zsh shell里就好了。\nvi ~/.zshrc\n\n编译.zshrc文件，在#User configuration下面添加一行\n# User configurationsource ~/.bash_profile\n\n保存退出。\n再执行source命令使其生效。\nsource ~/.zshrc\n","categories":["Tools","SSH"],"tags":["zsh","ssh"]},{"title":"mac执行brew install时卡在Updating Homebrew的解决方案","url":"/2019/06/mac%E6%89%A7%E8%A1%8Cbrew-install%E6%97%B6%E5%8D%A1%E5%9C%A8Updating-Homebrew%E7%9A%84%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88/","content":"今天通过homebrew安装软件，卡住了。其实以前也会经常遇到卡在更新Homebrew上面，主要是因为大陆网络环境的问题。\n如下：\nbrew install wgetUpdating Homebrew...\n\n一般我们可以按下control + c取消更新操作，这时，安装会继续执行下去。\n但推荐使用国内的Alibaba Homebrew镜像源进行替代。\n我们执行brew命令安装软件的时候，跟如下三个仓库地址有关：\n\nbrew.git\n\nhomebrew-core.git\n\nhomebrew-bottles\n\n\n接下来我们将上述3个仓库的地址全部替换为Alibaba提供的地址\ncd &quot;$(brew --repo)&quot;git remote set-url origin https://mirrors.aliyun.com/homebrew/brew.gitgit remote set-url origin https://mirrors.aliyun.com/homebrew/homebrew-core.gitecho &#x27;export HOMEBREW_BOTTLE_DOMAIN=https://mirrors.aliyun.com/homebrew/homebrew-bottles&#x27; &gt;&gt; ~/.bash_profilesource ~/.bash_profile\n\n通过上述步骤，我们在安装软件时，就不会卡在Updating Homebrew上面了。\n","tags":["homebrew"]},{"title":"Bash Shell快捷键-光标移动到行首、行尾等","url":"/2019/02/Bash-Shell%E5%BF%AB%E6%8D%B7%E9%94%AE-%E5%85%89%E6%A0%87%E7%A7%BB%E5%8A%A8%E5%88%B0%E8%A1%8C%E9%A6%96%E3%80%81%E8%A1%8C%E5%B0%BE%E7%AD%89/","content":"ctrl键组合\n\n\n快捷键\n功能\n\n\n\nctrl+a\n光标移到行首\n\n\nctrl+b\n光标左移一个字母\n\n\nctrl+c\n杀死当前进程\n\n\nctrl+d\n退出当前 Shell\n\n\nctrl+e\n光标移到行尾\n\n\nctrl+h\n删除光标前一个字符，同 backspace 键相同\n\n\nctrl+k\n清除光标后至行尾的内容\n\n\nctrl+l\n清屏，相当于clear\n\n\nctrl+r\n搜索之前打过的命令会有一个提示，根据你输入的关键字进行搜索bash的history\n\n\nctrl+u\n清除光标前至行首间的所有内容\n\n\nctrl+w\n移除光标前的一个单词\n\n\nctrl+t\n交换光标位置前的两个字符\n\n\nctrl+y\n粘贴或者恢复上次的删除\n\n\nctrl+d\n删除光标所在字母；注意和backspace以及ctrl+h的区别，这2个是删除光标前的字符\n\n\nctrl+f\n光标右移\n\n\nctrl+z\n把当前进程转到后台运行，使用fg命令恢复比如top -d1 然后ctrl+z，到后台，然后fg，重新恢复\n\n\nesc组合\n\n\n快捷键\n功能\n\n\n\nesc+d\n删除光标后的一个词\n\n\nesc+f\n往右跳一个词\n\n\nesc+b\n往左跳一个词\n\n\nesc+t\n交换光标位置前的两个单词\n\n\n","categories":["Tools","SSH"],"tags":["Bash"]},{"title":"Mac下使用crontab报错crontab: \"/usr/bin/vi\" exited with status 1","url":"/2019/02/Mac%E4%B8%8B%E4%BD%BF%E7%94%A8crontab%E6%8A%A5%E9%94%99crontab-usr-bin-vi-exited-with-status-1/","content":"在Mac OS X下使用crontab创建定时任务时报错。\ncrontab: no crontab for root - using an empty onecrontab: &quot;/usr/bin/vi&quot; exited with status 1\n\n原因大概是因为没有安装vi，目前我们使用的vi都是vim\n所以，可以按如下方式使用。\nEDITOR=vim crontab -e\n\n成功退出后，控制台上会打印如下内容：\ncrontab: installing new crontab\n","categories":["Tools","SSH"],"tags":["crontab"]},{"title":"MongoDB文档操作","url":"/2019/02/MongoDB%E6%96%87%E6%A1%A3%E6%93%8D%E4%BD%9C/","content":"本章节中我们将向大家介绍如何将数据插入到MongoDB的集合中。\n文档的数据结构和JSON基本一样。\n所有存储在集合中的数据都是BSON格式。\nBSON是一种类json的一种二进制形式的存储格式,简称Binary JSON。\n插入文档MongoDB 使用 insert() 或 save() 方法向集合中插入文档，语法如下：\ndb.COLLECTION_NAME.insert(document)\n\n&gt; document = (&#123;&quot;name&quot;: &quot;simon&quot;, &quot;age&quot;: 20, &quot;interests&quot;: [&quot;badminton&quot;, &quot;swimming&quot;, &quot;music&quot;]&#125;)&#123;\t&quot;name&quot; : &quot;simon&quot;,\t&quot;age&quot; : 20,\t&quot;interests&quot; : [\t\t&quot;badminton&quot;,\t\t&quot;swimming&quot;,\t\t&quot;music&quot;\t]&#125;&gt; db.col.insert(document)WriteResult(&#123; &quot;nInserted&quot; : 1 &#125;)\n\n插入文档你也可以使用 db.col.save(document) 命令。如果不指定 _id 字段 save() 方法类似于 insert() 方法。如果指定 _id 字段，则会更新该 _id 的数据。\n指定_id字段是&#123;&quot;_id&quot;: ObjectId(&quot;5c5be011dd377f0cd58338ae&quot;)&#125;\n更新文档update() 方法update() 方法用于更新已存在的文档。语法格式如下：\ndb.collection.update(   &lt;query&gt;,   &lt;update&gt;,   &#123;     upsert: &lt;boolean&gt;,     multi: &lt;boolean&gt;,     writeConcern: &lt;document&gt;   &#125;)\n\n参数说明：\n\nquery : update的查询条件，类似sql update查询内where后面的。\n\nupdate : update的对象和一些更新的操作符（如$,$inc…）等，也可以理解为sql update查询内set后面的\n\nupsert : 可选，这个参数的意思是，如果不存在update的记录，是否插入objNew,true为插入，默认是false，不插入。\n\nmulti : 可选，mongodb 默认是false,只更新找到的第一条记录，如果这个参数为true,就把按条件查出来多条记录全部更新。\n\nwriteConcern :可选，抛出异常的级别。\n\n\nsave() 方法save() 方法通过传入的文档来替换已有文档。语法格式如下：\ndb.collection.save(   &lt;document&gt;,   &#123;     writeConcern: &lt;document&gt;   &#125;)\n\n参数说明：\ndocument : 文档数据。\nwriteConcern :可选，抛出异常的级别。\n删除文档db.collection.remove(   &lt;query&gt;,   &#123;     justOne: &lt;boolean&gt;,     writeConcern: &lt;document&gt;   &#125;)\n\n参数说明：\n\nquery :（可选）删除的文档的条件。\n\njustOne : （可选）如果设为 true 或 1，则只删除一个文档，如果不设置该参数，或使用默认值 false，则删除所有匹配条件的文档。\n\nwriteConcern :（可选）抛出异常的级别。\n\n\nremove() 方法已经过时了，现在官方推荐使用 deleteOne() 和 deleteMany() 方法。\n如删除集合下全部文档：\ndb.inventory.deleteMany(&#123;&#125;)\n\n删除 status 等于 A 的全部文档：\ndb.inventory.deleteMany(&#123; status : &quot;A&quot; &#125;)\n\n删除 status 等于 D 的一个文档：\ndb.inventory.deleteOne( &#123; status: &quot;D&quot; &#125; )\n","categories":["Database","MongoDB"],"tags":["MongoDB"]},{"title":"MongoDB基本操作","url":"/2019/02/MongoDB%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/","content":"MongoDB创建数据库use DATABASE_NAME\n如果数据库不存在，则创建数据库，否则切换到指定数据库。\n&gt; use test2switched to db test2&gt; dbtest2\n\n查看所有数据库，使用show dbs命令\n&gt; show dbsadmin      0.000GBconfig     0.000GBlocal      0.000GBtest       0.000GB\n\n可以看到，我们刚创建的数据库 test2 并不在数据库的列表中， 要显示它，我们需要向 runoob 数据库插入一些数据。\ndb.test2.insert(&#123;&quot;name&quot;:&quot;simon&quot;&#125;)WriteResult(&#123; &quot;nInserted&quot; : 1 &#125;)&gt; show dbsadmin      0.000GBconfig     0.000GBlocal      0.000GBtest       0.000GBtest2      0.000GB\n\nMongoDB删除数据库&gt; db.dropDatabase()&#123; &quot;dropped&quot; : &quot;test2&quot;, &quot;ok&quot; : 1 &#125;\n\n删除集合collectionshow collections和show tables都可以查看集合列表。\n&gt; show collectionscollection1collection2&gt; db.collection1.drop()true\n\nMongoDB创建集合db.createCollection(name, options)\n\nname: 要创建的集合名称\noptions: 可选参数, 指定有关内存大小及索引的选项\noptions 可以是如下参数：\n\n\n\n字段\n类型\n描述\n\n\n\ncapped\n布尔\n（可选）如果为 true，则创建固定集合。固定集合是指有着固定大小的集合，当达到最大值时，它会自动覆盖最早的文档。当 该值为 true 时，必须指定 size 参数。\n\n\nautoIndexId\n布尔\n（可选）如为 true，自动在 _id 字段创建索引。默认为 false。\n\n\nsize\n数值\n（可选）为固定集合指定一个最大值（以字节计）。如果 capped 为 true，也需要指定该字段。\n\n\nmax\n数值\n（可选）指定固定集合中包含文档的最大数量。\n\n\n在插入文档时，MongoDB 首先检查固定集合的 size 字段，然后检查 max 字段。\n在test数据库中创建coll1集合\n&gt; use testswitched to db test&gt; db.createCollection(&#x27;coll1&#x27;)&#123; &quot;ok&quot; : 1 &#125;\n\n创建固定集合 mycol，整个集合空间大小 6142800 KB, 文档最大个数为 10000 个。\n&gt; db.createCollection(&quot;mycol&quot;, &#123;capped: true, autoIndexId: true, size: 6142800, max: 10000&#125;)&#123;\t&quot;note&quot; : &quot;the autoIndexId option is deprecated and will be removed in a future release&quot;,\t&quot;ok&quot; : 1&#125;\n\n在 MongoDB 中，你不需要创建集合。当你插入一些文档时，MongoDB 会自动创建集合。\ndb.mycol2.insert(&#123;&quot;name&quot;: &quot;simon&quot;&#125;)WriteResult(&#123; &quot;nInserted&quot; : 1 &#125;)show collectionsmycol2\n\nMongoDB删除集合db.collection.drop()\n&gt; show collectionsmycol2&gt; db.mycol2.drop()true\n\n启动mongodb服务并把服务暴露给任何ip, –fork是在后台运行。\nmongod --bind_ip 0.0.0.0 --dbpath /path/to/your/db --fork --logpath /path/to/logfile.log\n\n","categories":["Database","MongoDB"],"tags":["MongoDB"]},{"title":"Maven工程打包Spark Streaming程序 ","url":"/2019/02/Maven%E5%B7%A5%E7%A8%8B%E6%89%93%E5%8C%85Spark-Streaming%E7%A8%8B%E5%BA%8F/","content":"当我们把Spark Streaming程序开发好以后，放到测试环境测试时，如何通过Maven打包呢？\n在这篇文章中，我将介绍两种Maven的build方式。\n推荐使用第二种方式。\n所有依赖打成一个jar包通过这种方式，把所有的依赖都打成一个大的jar包。\n\n优点\n\n通过spark-submit执行程序时，简单方便，不需要指定jars参数。\n\n缺点\n\n依赖多的时候，有时候包会有几百M，在网络传输过程时效率不高。\npom.xml文件的build节点需要添加如下两个Maven插件。\n&lt;plugins&gt;    &lt;plugin&gt;        &lt;groupId&gt;org.scala-tools&lt;/groupId&gt;        &lt;artifactId&gt;maven-scala-plugin&lt;/artifactId&gt;        &lt;executions&gt;            &lt;execution&gt;                &lt;goals&gt;                    &lt;goal&gt;compile&lt;/goal&gt;                    &lt;goal&gt;testCompile&lt;/goal&gt;                &lt;/goals&gt;            &lt;/execution&gt;        &lt;/executions&gt;    &lt;/plugin&gt;    &lt;plugin&gt;        &lt;artifactId&gt;maven-assembly-plugin&lt;/artifactId&gt;        &lt;configuration&gt;            &lt;descriptorRefs&gt;                &lt;descriptorRef&gt;jar-with-dependencies&lt;/descriptorRef&gt;            &lt;/descriptorRefs&gt;            &lt;archive&gt;                &lt;manifest&gt;                    &lt;mainClass&gt;&lt;/mainClass&gt;                &lt;/manifest&gt;            &lt;/archive&gt;        &lt;/configuration&gt;        &lt;executions&gt;            &lt;execution&gt;                &lt;id&gt;make-assembly&lt;/id&gt; &lt;!-- this is used for inheritance merges --&gt;                &lt;phase&gt;package&lt;/phase&gt; &lt;!-- bind to the packaging phase --&gt;                &lt;goals&gt;                    &lt;goal&gt;single&lt;/goal&gt;                &lt;/goals&gt;            &lt;/execution&gt;        &lt;/executions&gt;    &lt;/plugin&gt;&lt;/plugins&gt;\n\n使用org.scala-tools:maven-scala-plugin插件有个好处是，如果项目里面有src/main/java和src/main/scala两种语言混编时，可以成功编译源码。\n[INFO] /Users/simon/Development/workspace/scala/spark-streaming-demo/src/main/java:-1: info: compiling[INFO] /Users/simon/Development/workspace/scala/spark-streaming-demo/src/main/scala:-1: info: compiling\n\n执行Spark任务命令：\nbin/spark-submit --master local[2] \\--class gy.finolo.spark.project.StreamingApp \\../data/app/spark-streaming-demo-1.0-SNAPSHOT.jar localhost:9092 streamingtopic\n\n打成一个小jar和lib依赖文件夹\n优点\n\nSpark Streaming源码被修改后，打成小包就可以上传服务器，传输流量小，速度快。\n\n缺点\n\n提交任务时，需要指定jars参数\npom.xml文件的build节点需要添加如下两个Maven插件。\n&lt;plugins&gt;    &lt;plugin&gt;        &lt;groupId&gt;org.scala-tools&lt;/groupId&gt;        &lt;artifactId&gt;maven-scala-plugin&lt;/artifactId&gt;        &lt;executions&gt;            &lt;execution&gt;                &lt;goals&gt;                    &lt;goal&gt;compile&lt;/goal&gt;                    &lt;goal&gt;testCompile&lt;/goal&gt;                &lt;/goals&gt;            &lt;/execution&gt;        &lt;/executions&gt;    &lt;/plugin&gt;    &lt;plugin&gt;        &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;        &lt;artifactId&gt;maven-dependency-plugin&lt;/artifactId&gt;        &lt;version&gt;2.10&lt;/version&gt;        &lt;executions&gt;            &lt;execution&gt;                &lt;id&gt;copy-dependencies&lt;/id&gt;                &lt;phase&gt;package&lt;/phase&gt;                &lt;goals&gt;                    &lt;goal&gt;copy-dependencies&lt;/goal&gt;                &lt;/goals&gt;                &lt;configuration&gt;                    &lt;outputDirectory&gt;$&#123;project.build.directory&#125;/lib&lt;/outputDirectory&gt;                    &lt;excludeScope&gt;provided&lt;/excludeScope&gt; &lt;!-- provided scope的依赖不需要放入到lib中 --&gt;                &lt;/configuration&gt;            &lt;/execution&gt;        &lt;/executions&gt;    &lt;/plugin&gt;&lt;/plugins&gt;\n\n执行Spark任务命令，需要指定jars参数，注意：这里不是lib的地址。\nbin/spark-submit --master local[2] \\--class gy.finolo.spark.project.StreamingApp \\--jars $(echo ../data/app/lib/*.jar | tr &#x27; &#x27; &#x27;,&#x27;) \\../data/lib/spark-streaming-demo-1.0-SNAPSHOT.jar localhost:9092 streamingtopic\n\n$(echo ../data/app/lib/*.jar | tr &#39; &#39; &#39;,&#39;)代码的意思是把jar包名以逗号分隔符拼接成一个字符串。\n","categories":["Java","Maven"],"tags":["java","maven","spark"]},{"title":"NoSQL简介","url":"/2019/02/NoSQL%E7%AE%80%E4%BB%8B/","content":"什么叫NoSQLNoSQL(NoSQL &#x3D; Not Only SQL )，意即”不仅仅是SQL”。\n在现代的计算系统上每天网络上都会产生庞大的数据量。\n这些数据有很大一部分是由关系数据库管理系统（RDMBSs）来处理。 1970年 E.F.Codd’s提出的关系模型的论文 “A relational model of data for large shared data banks”，这使得数据建模和应用程序编程更加简单。\n通过应用实践证明，关系模型是非常适合于客户服务器编程，远远超出预期的利益，今天它是结构化数据存储在网络和商务应用的主导技术。\nNoSQL 是一项全新的数据库革命性运动，早期就有人提出，发展至2009年趋势越发高涨。NoSQL的拥护者们提倡运用非关系型的数据存储，相对于铺天盖地的关系型数据库运用，这一概念无疑是一种全新的思维的注入。\n关系型数据库遵循ACID规则事务在英文中是transaction，和现实世界中的交易很类似，它有如下四个特性：\n\nA (Atomicity) 原子性原子性很容易理解，也就是说事务里的所有操作要么全部做完，要么都不做，事务成功的条件是事务里的所有操作都成功，只要有一个操作失败，整个事务就失败，需要回滚。比如银行转账，从A账户转100元至B账户，分为两个步骤：1）从A账户取100元；2）存入100元至B账户。这两步要么一起完成，要么一起不完成，如果只完成第一步，第二步失败，钱会莫名其妙少了100元。\n\nC (Consistency) 一致性一致性也比较容易理解，也就是说数据库要一直处于一致的状态，事务的运行不会改变数据库原本的一致性约束。例如现有完整性约束a+b&#x3D;10，如果一个事务改变了a，那么必须得改变b，使得事务结束后依然满足a+b&#x3D;10，否则事务失败。\n\nI (Isolation) 独立性所谓的独立性是指并发的事务之间不会互相影响，如果一个事务要访问的数据正在被另外一个事务修改，只要另外一个事务未提交，它所访问的数据就不受未提交事务的影响。比如现有有个交易是从A账户转100元至B账户，在这个交易还未完成的情况下，如果此时B查询自己的账户，是看不到新增加的100元的。\n\nD (Durability) 持久性持久性是指一旦事务提交后，它所做的修改将会永久的保存在数据库上，即使出现宕机也不会丢失。\n\n\nRDBMS vs NoSQLRDBMS\n高度组织化结构化数据 \n结构化查询语言（SQL） (SQL) \n数据和关系都存储在单独的表中。 \n数据操纵语言，数据定义语言 \n严格的一致性\n基础事务\n\nNoSQL\n代表着不仅仅是SQL\n没有声明性查询语言\n没有预定义的模式\n键 - 值对存储，列存储，文档存储，图形数据库\n最终一致性，而非ACID属性\n非结构化和不可预知的数据\nCAP定理 \n高性能，高可用性和可伸缩性\n\nCAP定理（CAP theorem）在计算机科学中, CAP定理（CAP theorem）, 又被称作 布鲁尔定理（Brewer’s theorem）, 它指出对于一个分布式计算系统来说，不可能同时满足以下三点:\n一致性(Consistency) (所有节点在同一时间具有相同的数据)\n可用性(Availability) (保证每个请求不管成功或者失败都有响应)\n分隔容忍(Partition tolerance) (系统中任意信息的丢失或失败不会影响系统的继续运作)\nCAP理论的核心是：一个分布式系统不可能同时很好的满足一致性，可用性和分区容错性这三个需求，最多只能同时较好的满足两个。\n因此，根据 CAP 原理将 NoSQL 数据库分成了满足 CA 原则、满足 CP 原则和满足 AP 原则三 大类：\nCA - 单点集群，满足一致性，可用性的系统，通常在可扩展性上不太强大。\nCP - 满足一致性，分区容忍性的系统，通常性能不是特别高。\nAP - 满足可用性，分区容忍性的系统，通常可能对一致性要求低一些。\nNoSQL 数据库分类\n\n\n类型\n部分代表\n特点\n\n\n\n列存储\nHbase, Cassandra, Hypertable\n顾名思义，是按列存储数据的。最大的特点是方便存储结构化和半结构化数据，方便做数据压缩，对针对某一列或者某几列的查询有非常大的IO优势。\n\n\n文档存储\nMongoDB, CouchDB\n文档存储一般用类似json的格式存储，存储的内容是文档型的。这样也就有有机会对某些字段建立索引，实现关系数据库的某些功能。\n\n\nkey-value存储\nTokyo Cabinet &#x2F; Tyrant, Berkeley DB, MemcacheDB, Redis\n可以通过key快速查询到其value。一般来说，存储不管value的格式，照单全收。（Redis包含了其他功能）\n\n\n图存储\nNeo4J, FlockDB\n图形关系的最佳存储。使用传统关系数据库来解决的话性能低下，而且设计使用不方便。\n\n\n对象存储\ndb4o, Versant\n通过类似面向对象语言的语法操作数据库，通过对象的方式存取数据。\n\n\nxml数据库\nBerkeley DB XML, BaseX\n高效的存储XML数据，并支持XML的内部查询语法，比如XQuery,Xpath\n\n\n","categories":["Database"],"tags":["NoSQL"]},{"title":"Python多进程","url":"/2019/02/Python%E5%A4%9A%E8%BF%9B%E7%A8%8B/","content":"要让Python程序实现多进程（multiprocessing），我们先了解操作系统的相关知识。\nUnix&#x2F;Linux操作系统提供了一个fork()系统调用，它非常特殊。普通的函数调用，调用一次，返回一次，但是fork()调用一次，返回两次，因为操作系统自动把当前进程（称为父进程）复制了一份（称为子进程），然后，分别在父进程和子进程内返回。\n子进程永远返回0，而父进程返回子进程的ID。这样做的理由是，一个父进程可以fork出很多子进程，所以，父进程要记下每个子进程的ID，而子进程只需要调用getppid()就可以拿到父进程的ID。\nPython的os模块封装了常见的系统调用，其中就包括fork，可以在Python程序中轻松创建子进程：\nimport osprint(&#x27;Process (%s) starts...&#x27; % os.getpid())# Only works on Unix / Linux / Mac:pid = os.fork()if pid == 0:    print(&#x27;I am a child process (%s) and my parent is (%s).&#x27; % (os.getpid(), os.getppid()))else:    print(&#x27;I (%s) just created a child process (%s).&#x27; % (os.getpid(), pid))\n\n运行结果如下：\nProcess (56351) starts...I (56351) just created a child process (56352).I am a child process (56352) and my parent is (56351).\n\n由于Windows没有fork调用，上面的代码在Windows上无法运行。由于Mac系统是基于BSD（Unix的一种）内核，所以，在Mac下运行是没有问题的。\n有了fork调用，一个进程在接到新任务时就可以复制出一个子进程来处理新任务，常见的Apache服务器就是由父进程监听端口，每当有新的http请求时，就fork出子进程来处理新的http请求。\nmultiprocessing如果你打算编写多进程的服务程序，Unix&#x2F;Linux无疑是正确的选择。由于Windows没有fork调用，难道在Windows上无法用Python编写多进程的程序？\n由于Python是跨平台的，自然也应该提供一个跨平台的多进程支持。multiprocessing模块就是跨平台版本的多进程模块。\nmultiprocessing模块提供了一个Process类来代表一个进程对象，下面的例子演示了启动一个子进程并等待其结束：\nfrom multiprocessing import Processimport osimport time# 子进程要执行的代码def run_proc(name):    print(&#x27;Run child process %s (%s)&#x27; % (name, os.getpid()))    time.sleep(2)    print(&#x27;Child process ends&#x27;)if __name__ == &#x27;__main__&#x27;:    print(&#x27;Parent process %s.&#x27; % os.getpid())    p = Process(target=run_proc, args=(&#x27;test&#x27;,))    print(&#x27;Child process will start.&#x27;)    p.start()    p.join()    print(&#x27;Parent process end.&#x27;)\n\n执行结果如下：\nParent process 56782.Child process will start.Run child process test (56783)Child process endsParent process end.\n\n创建子进程时，只需要传入一个执行函数和函数的参数，创建一个Process实例，用start()方法启动，这样创建进程比fork()还要简单。\njoin()方法可以等待子进程结束后再继续往下运行，通常用于进程间的同步。\n如果不加p.join()这句代码，子进程也是可以被执行完成的。\nPool如果要启动大量的子进程，可以用进程池的方式批量创建子进程：\nfrom multiprocessing import Poolimport os, time, randomdef long_time_task(name):    print(&#x27;Run task %s (%s)...&#x27; % (name, os.getpid()))    start = time.time()    time.sleep(random.random() * 3)    end = time.time()    print(&#x27;Task %s runs %0.2f seconds.&#x27; % (name, (end - start)))if __name__==&#x27;__main__&#x27;:    print(&#x27;Parent process %s.&#x27; % os.getpid())    p = Pool(4)    for i in range(5):        p.apply_async(long_time_task, args=(i,))    print(&#x27;Waiting for all subprocesses done...&#x27;)    p.close()    p.join()    print(&#x27;All subprocesses done.&#x27;)\n\n执行结果如下：\nParent process 57316.Waiting for all subprocesses done...Run task 0 (57317)...Run task 1 (57318)...Run task 2 (57319)...Run task 3 (57320)...Task 0 runs 0.59 seconds.Run task 4 (57317)...Task 4 runs 1.06 seconds.Task 3 runs 1.96 seconds.Task 1 runs 2.02 seconds.Task 2 runs 2.99 seconds.All subprocesses done.\n\n对Pool对象调用join()方法会等待所有子进程执行完毕，调用join()之前必须先调用close()，不然会出现ValueError: Pool is still running错误。调用close()之后就不能继续添加新的Process了。\np.join()这句代码必须存在，不然子进程极有可能不会被执行完成。\n请注意输出的结果，task 0，1，2，3是立刻执行的，而task 4要等待前面某个task完成后才执行，这是因为Pool的默认大小在我的电脑上是4，因此，最多同时执行4个进程。这是Pool有意设计的限制，并不是操作系统的限制。如果改成：\np &#x3D; Pool(5)就可以同时跑5个进程。\n由于Pool的默认大小是CPU的核数，如果你不幸拥有8核CPU，你要提交至少9个子进程才能看到上面的等待效果。\n子进程很多时候，子进程并不是自身，而是一个外部进程。我们创建了子进程后，还需要控制子进程的输入和输出。\nsubprocess模块可以让我们非常方便地启动一个子进程，然后控制其输入和输出。\n下面的例子演示了如何在Python代码中运行命令nslookup www.python.org，这和命令行直接运行的效果是一样的：\nimport subprocessprint(&#x27;$ nslookup www.python.org&#x27;)r = subprocess.call([&#x27;nslookup&#x27;, &#x27;www.python.org&#x27;])print(&#x27;Exit code:&#x27;, r)\n\n运行结果：\n$ nslookup www.python.orgServer:        192.168.19.4Address:    192.168.19.4#53Non-authoritative answer:www.python.org    canonical name = python.map.fastly.net.Name:    python.map.fastly.netAddress: 199.27.79.223Exit code: 0\n\n如果子进程还需要输入，则可以通过communicate()方法输入：\nimport subprocessprint(&#x27;$ nslookup&#x27;)p = subprocess.Popen([&#x27;nslookup&#x27;], stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE)output, err = p.communicate(b&#x27;set q=mx\\npython.org\\nexit\\n&#x27;)print(output.decode(&#x27;utf-8&#x27;))print(&#x27;Exit code:&#x27;, p.returncode)上面的代码相当于在命令行执行命令nslookup，然后手动输入：set q=mxpython.orgexit\n\n运行结果如下：\n$ nslookupServer:        192.168.19.4Address:    192.168.19.4#53Non-authoritative answer:python.org    mail exchanger = 50 mail.python.org.Authoritative answers can be found from:mail.python.org    internet address = 82.94.164.166mail.python.org    has AAAA address 2001:888:2000:d::a6Exit code: 0\n\n进程间通信Process之间肯定是需要通信的，操作系统提供了很多机制来实现进程间的通信。Python的multiprocessing模块包装了底层的机制，提供了Queue、Pipes等多种方式来交换数据。\n我们以Queue为例，在父进程中创建两个子进程，一个往Queue里写数据，一个从Queue里读数据：\nfrom multiprocessing import Process, Queueimport os, time, random# 写数据进程执行的代码:def write(q):    print(&#x27;Process to write: %s&#x27; % os.getpid())    for value in [&#x27;A&#x27;, &#x27;B&#x27;, &#x27;C&#x27;]:        print(&#x27;Put %s to queue...&#x27; % value)        q.put(value)        time.sleep(random.random())# 读数据进程执行的代码:def read(q):    print(&#x27;Process to read: %s&#x27; % os.getpid())    while True:        value = q.get(True)        print(&#x27;Get %s from queue.&#x27; % value)if __name__==&#x27;__main__&#x27;:    # 父进程创建Queue，并传给各个子进程：    q = Queue()    pw = Process(target=write, args=(q,))    pr = Process(target=read, args=(q,))    # 启动子进程pw，写入:    pw.start()    # 启动子进程pr，读取:    pr.start()    # 等待pw结束:    pw.join()    # pr进程里是死循环，无法等待其结束，只能强行终止:    pr.terminate()\n\n运行结果如下：\nProcess to write: 50563Put A to queue...Process to read: 50564Get A from queue.Put B to queue...Get B from queue.Put C to queue...Get C from queue.\n\n在Unix&#x2F;Linux下，multiprocessing模块封装了fork()调用，使我们不需要关注fork()的细节。由于Windows没有fork调用，因此，multiprocessing需要“模拟”出fork的效果，父进程所有Python对象都必须通过pickle序列化再传到子进程去，所有，如果multiprocessing在Windows下调用失败了，要先考虑是不是pickle失败了。\n小结在Unix&#x2F;Linux下，可以使用fork()调用实现多进程。\n要实现跨平台的多进程，可以使用multiprocessing模块。\n进程间通信是通过Queue、Pipes等实现的。\n","categories":["Python"],"tags":["python"]},{"title":"Python多线程","url":"/2019/02/Python%E5%A4%9A%E7%BA%BF%E7%A8%8B/","content":"多任务可以由多进程完成，也可以由一个进程内的多线程完成。\n我们前面提到了进程是由若干线程组成的，一个进程至少有一个线程。\n由于线程是操作系统直接支持的执行单元，因此，高级语言通常都内置多线程的支持，Python也不例外，并且，Python的线程是真正的Posix Thread，而不是模拟出来的线程。\nPython的标准库提供了两个模块：_thread和threading，_thread是低级模块，threading是高级模块，对_thread进行了封装。绝大多数情况下，我们只需要使用threading这个高级模块。\n启动一个线程就是把一个函数传入并创建Thread实例，然后调用start()开始执行：\nimport time, threading# 新线程执行的代码:def loop():    print(&#x27;thread %s is running...&#x27; % threading.current_thread().name)    n = 0    while n &lt; 5:        n = n + 1        print(&#x27;thread %s &gt;&gt;&gt; %s&#x27; % (threading.current_thread().name, n))        time.sleep(1)    print(&#x27;thread %s ended.&#x27; % threading.current_thread().name)print(&#x27;thread %s is running...&#x27; % threading.current_thread().name)t = threading.Thread(target=loop, name=&#x27;LoopThread&#x27;)t.start()t.join()print(&#x27;thread %s ended.&#x27; % threading.current_thread().name)\n\n执行结果如下：\nthread MainThread is running...thread LoopThread is running...thread LoopThread &gt;&gt;&gt; 1thread LoopThread &gt;&gt;&gt; 2thread LoopThread &gt;&gt;&gt; 3thread LoopThread &gt;&gt;&gt; 4thread LoopThread &gt;&gt;&gt; 5thread LoopThread ended.thread MainThread ended.\n\n由于任何进程默认就会启动一个线程，我们把该线程称为主线程，主线程又可以启动新的线程，Python的threading模块有个current_thread()函数，它永远返回当前线程的实例。主线程实例的名字叫MainThread，子线程的名字在创建时指定，我们用LoopThread命名子线程。名字仅仅在打印时用来显示，完全没有其他意义，如果不起名字Python就自动给线程命名为Thread-1，Thread-2…\nLock多线程和多进程最大的不同在于，多进程中，同一个变量，各自有一份拷贝存在于每个进程中，互不影响，而多线程中，所有变量都由所有线程共享，所以，任何一个变量都可以被任何一个线程修改，因此，线程之间共享数据最大的危险在于多个线程同时改一个变量，把内容给改乱了。\n来看看多个线程同时操作一个变量怎么把内容给改乱了：\nimport time, threading# 假定这是你的银行存款:balance = 0def change_it(n):    # 先存后取，结果应该为0:    global balance    balance = balance + n    balance = balance - ndef run_thread(n):    for i in range(100000):        change_it(n)t1 = threading.Thread(target=run_thread, args=(5,))t2 = threading.Thread(target=run_thread, args=(8,))t1.start()t2.start()t1.join()t2.join()print(balance)\n\n我们定义了一个共享变量balance，初始值为0，并且启动两个线程，先存后取，理论上结果应该为0，但是，由于线程的调度是由操作系统决定的，当t1、t2交替执行时，只要循环次数足够多，balance的结果就不一定是0了。\n原因是因为高级语言的一条语句在CPU执行时是若干条语句，即使一个简单的计算：\nbalance &#x3D; balance + n也分两步：\n计算balance + n，存入临时变量中；将临时变量的值赋给balance。也就是可以看成：\nx &#x3D; balance + nbalance &#x3D; x由于x是局部变量，两个线程各自都有自己的x，当代码正常执行时：\n初始值 balance &#x3D; 0\nt1: x1 &#x3D; balance + 5 # x1 &#x3D; 0 + 5 &#x3D; 5t1: balance &#x3D; x1     # balance &#x3D; 5t1: x1 &#x3D; balance - 5 # x1 &#x3D; 5 - 5 &#x3D; 0t1: balance &#x3D; x1     # balance &#x3D; 0\nt2: x2 &#x3D; balance + 8 # x2 &#x3D; 0 + 8 &#x3D; 8t2: balance &#x3D; x2     # balance &#x3D; 8t2: x2 &#x3D; balance - 8 # x2 &#x3D; 8 - 8 &#x3D; 0t2: balance &#x3D; x2     # balance &#x3D; 0\n结果 balance &#x3D; 0但是t1和t2是交替运行的，如果操作系统以下面的顺序执行t1、t2：\n初始值 balance &#x3D; 0\nt1: x1 &#x3D; balance + 5  # x1 &#x3D; 0 + 5 &#x3D; 5\nt2: x2 &#x3D; balance + 8  # x2 &#x3D; 0 + 8 &#x3D; 8t2: balance &#x3D; x2      # balance &#x3D; 8\nt1: balance &#x3D; x1      # balance &#x3D; 5t1: x1 &#x3D; balance - 5  # x1 &#x3D; 5 - 5 &#x3D; 0t1: balance &#x3D; x1      # balance &#x3D; 0\nt2: x2 &#x3D; balance - 8  # x2 &#x3D; 0 - 8 &#x3D; -8t2: balance &#x3D; x2   # balance &#x3D; -8\n结果 balance &#x3D; -8究其原因，是因为修改balance需要多条语句，而执行这几条语句时，线程可能中断，从而导致多个线程把同一个对象的内容改乱了。\n两个线程同时一存一取，就可能导致余额不对，你肯定不希望你的银行存款莫名其妙地变成了负数，所以，我们必须确保一个线程在修改balance的时候，别的线程一定不能改。\n如果我们要确保balance计算正确，就要给change_it()上一把锁，当某个线程开始执行change_it()时，我们说，该线程因为获得了锁，因此其他线程不能同时执行change_it()，只能等待，直到锁被释放后，获得该锁以后才能改。由于锁只有一个，无论多少线程，同一时刻最多只有一个线程持有该锁，所以，不会造成修改的冲突。创建一个锁就是通过threading.Lock()来实现：\nbalance = 0lock = threading.Lock()def run_thread(n):    for i in range(100000):        # 先要获取锁:        lock.acquire()        try:            # 放心地改吧:            change_it(n)        finally:            # 改完了一定要释放锁:            lock.release()         \n\n当多个线程同时执行lock.acquire()时，只有一个线程能成功地获取锁，然后继续执行代码，其他线程就继续等待直到获得锁为止。\n获得锁的线程用完后一定要释放锁，否则那些苦苦等待锁的线程将永远等待下去，成为死线程。所以我们用try…finally来确保锁一定会被释放。\n锁的好处就是确保了某段关键代码只能由一个线程从头到尾完整地执行，坏处当然也很多，首先是阻止了多线程并发执行，包含锁的某段代码实际上只能以单线程模式执行，效率就大大地下降了。其次，由于可以存在多个锁，不同的线程持有不同的锁，并试图获取对方持有的锁时，可能会造成死锁，导致多个线程全部挂起，既不能执行，也无法结束，只能靠操作系统强制终止。\n多核CPU如果你不幸拥有一个多核CPU，你肯定在想，多核应该可以同时执行多个线程。\n如果写一个死循环的话，会出现什么情况呢？\n打开Mac OS X的Activity Monitor，或者Windows的Task Manager，都可以监控某个进程的CPU使用率。\n我们可以监控到一个死循环线程会100%占用一个CPU。\n如果有两个死循环线程，在多核CPU中，可以监控到会占用200%的CPU，也就是占用两个CPU核心。\n要想把N核CPU的核心全部跑满，就必须启动N个死循环线程。\n试试用Python写个死循环：\nimport threading, multiprocessingdef loop():    x = 0    while True:        x = x ^ 1for i in range(multiprocessing.cpu_count()):    t = threading.Thread(target=loop)    t.start()\n\n启动与CPU核心数量相同的N个线程，在4核CPU上可以监控到CPU占用率仅有102%，也就是仅使用了一核。\n但是用C、C++或Java来改写相同的死循环，直接可以把全部核心跑满，4核就跑到400%，8核就跑到800%，为什么Python不行呢？\n因为Python的线程虽然是真正的线程，但解释器执行代码时，有一个GIL锁：Global Interpreter Lock，任何Python线程执行前，必须先获得GIL锁，然后，每执行100条字节码，解释器就自动释放GIL锁，让别的线程有机会执行。这个GIL全局锁实际上把所有线程的执行代码都给上了锁，所以，多线程在Python中只能交替执行，即使100个线程跑在100核CPU上，也只能用到1个核。\nGIL是Python解释器设计的历史遗留问题，通常我们用的解释器是官方实现的CPython，要真正利用多核，除非重写一个不带GIL的解释器。\n所以，在Python中，可以使用多线程，但不要指望能有效利用多核。如果一定要通过多线程利用多核，那只能通过C扩展来实现，不过这样就失去了Python简单易用的特点。\n不过，也不用过于担心，Python虽然不能利用多线程实现多核任务，但可以通过多进程实现多核任务。多个Python进程有各自独立的GIL锁，互不影响。\n小结多线程编程，模型复杂，容易发生冲突，必须用锁加以隔离，同时，又要小心死锁的发生。\nPython解释器由于设计时有GIL全局锁，导致了多线程无法利用多核。\n","categories":["Python"],"tags":["python"]},{"title":"Python如何打印日志","url":"/2019/02/Python%E5%A6%82%E4%BD%95%E6%89%93%E5%8D%B0%E6%97%A5%E5%BF%97/","content":"日志日志是跟踪软件运行时所发生的事件的一种方法。软件开发者在代码中调用日志函数，表明发生了特定的事件。事件由描述性消息描述，该描述性消息可以可选地包含可变数据（即，对于事件的每次出现都潜在地不同的数据）。事件还具有开发者归因于事件的重要性；重要性也可以称为级别或严重性。\n什么时候使用Logginglogging提供了一组便利的函数，用来做简单的日志。它们是 debug()、 info()、 warning()、 error() 和 critical()。\nlogging函数根据它们用来跟踪的事件的级别或严重程度来命名。标准级别及其适用性描述如下（以严重程度递增排序）：\n\n\n\n级别\n何时使用\n\n\n\nDEBUG\n详细信息，一般只在调试问题时使用。\n\n\nINFO\n证明事情按预期工作。\n\n\nWARNING\n某些没有预料到的事件的提示，或者在将来可能会出现的问题提示。例如：磁盘空间不足。但是软件还是会照常运行。\n\n\nERROR\n由于更严重的问题，软件已不能执行一些功能了。\n\n\nCRITICAL\n严重错误，表明软件已不能继续运行了。\n\n\n打印日志到控制台由于默认设置的等级是warning，所有只有warning的信息会输出到控制台。\nimport logginglogging.warning(&#x27;Watch out!&#x27;)  # will print a message to the consolelogging.info(&#x27;I told you so&#x27;)  # will not print anythingWARNING:root:Watch out!\n\n打印日志到文件import logginglogging.basicConfig(filename=&#x27;example.log&#x27;,level=logging.DEBUG)logging.debug(&#x27;This message should go to the log file&#x27;)logging.info(&#x27;So should this&#x27;)logging.warning(&#x27;And this, too&#x27;)DEBUG:root:This message should go to the log fileINFO:root:So should thisWARNING:root:And this, too\n\n我们设置了logging的级别为DEBUG，所以所有信息都将被写入到example.log文件中。\n如果想要每次启动时，原来的打印日志都被清空的话，则需要把filemode由默认的a改为w\nlogging.basicConfig(filename=&#x27;example.log&#x27;, filemode=&#x27;w&#x27;, level=logging.DEBUG)\n\n日志的格式日志级别import logginglogging.basicConfig(format=&#x27;%(levelname)s:%(message)s&#x27;, level=logging.DEBUG)logging.debug(&#x27;This message should appear on the console&#x27;)logging.info(&#x27;So should this&#x27;)logging.warning(&#x27;And this, too&#x27;)DEBUG:This message should appear on the consoleINFO:So should thisWARNING:And this, too\n\n日期import logginglogging.basicConfig(format=&#x27;%(asctime)s %(message)s&#x27;)logging.warning(&#x27;is when this event was logged.&#x27;)2019-02-07 22:21:20,993 is when this event was logged.\n\n\n","categories":["Python"],"tags":["python"]},{"title":"Spark DataFrame和Dataset API基本操作","url":"/2019/02/Spark-DataFrame%E5%92%8CDataset-API%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/","content":"DataFrames API基本操作启动Spark需要加载MySQL JDBC驱动。\nbin/spark-shell --master local[2] \\--jars ~/.m2/repository/mysql/mysql-connector-java/5.1.46/mysql-connector-java-5.1.46.jar\n\n加载json文件\nscala&gt;val jsonPath = &quot;/usr/local/spark/spark-2.2.3-bin-2.6.0-cdh5.7.0/examples/src/main/resources/people.json&quot;val peopleDF = spark.read.format(&quot;json&quot;).load(jsonPath)\n\nAPI操作\n输出dataframe对应的schema信息\n\nscala&gt; peopleDF.printSchemaroot |-- age: long (nullable = true) |-- name: string (nullable = true)\n\n\n输出数据集前20条记录\n\nscala&gt; peopleDF.show+----+-------+| age|   name|+----+-------+|null|Michael||  30|   Andy||  19| Justin|+----+-------+\n\n\n查询某列所有的数据\n\n类似如下SQL语句：\nSELECT name FROM people;\n\nscala&gt; peopleDF.select(&quot;name&quot;).show+-------+|   name|+-------+|Michael||   Andy|| Justin|+-------+\n\n\nFILTER查询\n\nSQL的WHERE子句查询，类似如下SQL语句：\nSELECT * FROM people WHERE age &gt; 19;\n\npeopleDF.filter(&quot;age &gt; 19&quot;).show+---+----+|age|name|+---+----+| 30|Andy|+---+----+\n\n\n复杂查询\n\n类似如下SQL语句：\nSELECT name, age + 10 as new_age FROM people;\n\nscala&gt; peopleDF.select(peopleDF(&quot;name&quot;), (peopleDF(&quot;age&quot;) + 10).as(&quot;new_age&quot;)).show+-------+-------+|   name|new_age|+-------+-------+|Michael|   null||   Andy|     40|| Justin|     29|+-------+-------+\n\n\n聚合\n\n统计不同年龄的人数。类似如下SQL语句：\nSELECT age, count(1) FROM people GROUP BY age;\n\nscala&gt; peopleDF.groupBy(&quot;age&quot;).count().show+----+-----+| age|count|+----+-----+|  19|    1||null|    1||  30|    1|+----+-----+\n\n\n应用MySQL的函数\n\nscala&gt; peopleDF.filter(&quot;SUBSTRING(name, 0, 1) = &#x27;A&#x27;&quot;).show+---+----+|age|name|+---+----+| 30|Andy|+---+----+\n\n我们可以查看MySQL自带了哪些函数\nshow(rowNums, truncated)\nscala&gt; spark.sql(&quot;show functions&quot;).show(30, false)+---------------------+|function             |+---------------------+|!                    ||%                    ||&amp;                    ||*                    ||+                    ||-                    ||/                    ||&lt;                    ||&lt;=                   ||&lt;=&gt;                  ||=                    |...\n\n\n排序\n\nscala&gt; peopleDF.sort(peopleDF(&quot;name&quot;).desc).show+----+-------+| age|   name|+----+-------+|null|Michael||  19| Justin||  30|   Andy|+----+-------+\n\n\nJOIN\n\nscala&gt; val df2 = spark.read.format(&quot;json&quot;).load(jsonPath)peopleDF.join(df2, peopleDF(&quot;name&quot;) === df2(&quot;name&quot;)).show+----+-------+----+-------+| age|   name| age|   name|+----+-------+----+-------+|null|Michael|null|Michael||  30|   Andy|  30|   Andy||  19| Justin|  19| Justin|+----+-------+----+-------+\n\n\n其他\n\ntake, head(3), first\n\nRDD转DataframesInferring the Schema Using Reflection通过反射\n// For implicit conversions from RDDs to DataFramesimport spark.implicits._// Create an RDD of Person objects from a text file, convert it to a Dataframeval peopleDF = spark.sparkContext  .textFile(&quot;examples/src/main/resources/people.txt&quot;)  .map(_.split(&quot;,&quot;))  .map(attributes =&gt; Person(attributes(0), attributes(1).trim.toInt))  .toDF()// Register the DataFrame as a temporary viewpeopleDF.createOrReplaceTempView(&quot;people&quot;)// SQL statements can be run by using the sql methods provided by Sparkval teenagersDF = spark.sql(&quot;SELECT name, age FROM people WHERE age BETWEEN 13 AND 19&quot;)\n\nProgrammatically Specifying the Schema// Create an RDDval peopleRDD = spark.sparkContext.textFile(&quot;examples/src/main/resources/people.txt&quot;)// The schema is encoded in a stringval schemaString = &quot;name age&quot;// Generate the schema based on the string of schemaval fields = schemaString.split(&quot; &quot;)  .map(fieldName =&gt; StructField(fieldName, StringType, nullable = true))val schema = StructType(fields)// Convert records of the RDD (people) to Rowsval rowRDD = peopleRDD  .map(_.split(&quot;,&quot;))  .map(attributes =&gt; Row(attributes(0), attributes(1).trim))// Apply the schema to the RDDval peopleDF = spark.createDataFrame(rowRDD, schema)\n\nDataframs转Datasetcase class Person(name: String, age: Long)// DataFrames can be converted to a Dataset by providing a class. Mapping will be done by nameval path = &quot;examples/src/main/resources/people.json&quot;val peopleDS = spark.read.json(path).as[Person]peopleDS.show()","categories":["大数据","Spark","Spark SQL"],"tags":["Spark","Spark DataFrame","Spark Dataset"]},{"title":"从Hive平滑过渡到Spark SQL","url":"/2019/02/%E4%BB%8EHive%E5%B9%B3%E6%BB%91%E8%BF%87%E6%B8%A1%E5%88%B0Spark-SQL/","content":"这篇文章记录一下如何在Spark下面像Hive一样查询表的内容。\n启动Sparkbin/spark-shell --master local[2]\n\n查看一下有哪些表scala&gt; spark.sql(&quot;show tables&quot;).show19/02/12 14:37:49 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 1.2.019/02/12 14:37:49 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException19/02/12 14:37:50 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException+--------+---------+-----------+|database|tableName|isTemporary|+--------+---------+-----------++--------+---------+-----------+\n\n没有查看到东西。\n所以需要把hive下面的hive-site.xml文件复制到spark下面。\ncp $HIVE_HOME/conf/hive-site.xml $SPARK_HOME/conf\n\n重启Sparkbin/spark-shell --master local[2]\n\n再次执行命令。\nscala&gt; spark.sql(&quot;show tables&quot;).show19/02/12 15:53:46 WARN HiveMetaStore: Retrying creating default database after error: Error creating transactional connection factoryjavax.jdo.JDOFatalInternalException: Error creating transactional connection factoryCaused by: org.datanucleus.store.rdbms.connectionpool.DatastoreDriverNotFoundException: The specified datastore driver (&quot;com.mysql.jdbc.Driver&quot;) was not found in the CLASSPATH. Please check your CLASSPATH specification, and the name of the driver.\n\n说明MySQL的驱动未被加载。\n第三次启动Spark，通过参数--jars指定MySQL驱动的位置。\nbin/spark-shell --master local[2] \\--jars ~/.m2/repository/mysql/mysql-connector-java/5.1.46/mysql-connector-java-5.1.46.jar\n\n查看表scala&gt; spark.sql(&quot;show tables&quot;).show...+--------+--------------+-----------+|database|     tableName|isTemporary|+--------+--------------+-----------+| default|           emp|      false|| default|hive_wordcount|      false|+--------+--------------+-----------+\n\n可以看到有两个表，emp和hive_wordcount。\n验证SQL查询在hive client里面新建一个dept表hive&gt;CREATE TABLE dept(\tdeptno\t\tINT,\tdname\t\tSTRING,\tloc\t\t\tSTRING)ROW FORMAT DELIMITED FIELDS TERMINATED BY &#x27;\\t&#x27;;\n\n创建dept数据文件cat /usr/local/spark/data/dept10  ACCOUNTING  NEW YORK20  RESEARCH    DALLAS30  SALES   CHICAGO40  OPERATIONS\tBOSTON\n\n中间字段间间隔的是tab。\n加载数据到dept表里hive&gt; LOAD DATA LOCAL INPATH &#x27;/usr/local/spark/data/dept&#x27; OVERWRITE INTO TABLE dept;\n\nOVERWRITE参数的意义是：如果表里面有旧的数据，可以通过OVERWRITE先清除旧数据。\n验证查询回到spark shell。\nscala&gt; spark.sql(&quot;SELECT * FROM dept&quot;).show+------+----------+--------+|deptno|     dname|     loc|+------+----------+--------+|    10|ACCOUNTING|NEW YORK||    20|  RESEARCH|  DALLAS||    30|     SALES| CHICAGO||    40|OPERATIONS|  BOSTON|+------+----------+--------+\n\n如果省略spark.sql我们发现在执行SQL语句时，我们在都需要写spark.sql(sql query).show。这样比起hive来，就没那么简便。\n其实是可以通过启动spark-sql来达到这个简化目标的。\n能spark-sql方式启动spark shell。\nbin/spark-sql --master local[2] \\--jars ~/.m2/repository/mysql/mysql-connector-java/5.1.46/mysql-connector-java-5.1.46.jar\n\n启动后，直接写SQL语句查询\nspark-sql&gt; SELECT * FROM emp e JOIN dept d ON e.deptno = d.deptno;\n\n将返回如下内容：\n...19/02/12 16:39:50 INFO DAGScheduler: Job 1 finished: processCmd at CliDriver.java:376, took 0.059725 s7369\tSMITH\tCLERK\t7902\tNULL\t800.0\tNULL\t20\t20\tRESEARCH\tDALLAS7499\tALLEN\tSALESMAN\t7698\tNULL\t1600.0\t300.0\t30\t30\tSALES\tCHICAGO7521\tWARD\tSALESMAN\t7698\tNULL\t1250.0\t500.0\t30\t30\tSALES\tCHICAGO7566\tJONES\tMANAGER\t7839\tNULL\t2975.0\tNULL\t20\t20\tRESEARCH\tDALLAS7654\tMARTIN\tSALESMAN\t7698\tNULL\t1250.0\t1400.0\t30\t30\tSALES\tCHICAGO7698\tBLAKE\tMANAGER\t7839\tNULL\t2850.0\tNULL\t30\t30\tSALES\tCHICAGO7782\tCLARK\tMANAGER\t7839\tNULL\t2450.0\tNULL\t10\t10\tACCOUNTING\tNEW YORK7788\tSCOTT\tANALYST\t7566\tNULL\t3000.0\tNULL\t20\t20\tRESEARCH\tDALLAS7839\tKING\tPRESIDENT\tNULL\tNULL\t5000.0\tNULL\t10\t10\tACCOUNTING\tNEW YORK7844\tTURNER\tSALESMAN\t7698\tNULL\t1500.0\t0.0\t30\t30\tSALES\tCHICAGO7876\tADAMS\tCLERK\t7788\tNULL\t1100.0\tNULL\t20\t20\tRESEARCH\tDALLAS7900\tJAMES\tCLERK\t7698\tNULL\t950.0\tNULL\t30\t30\tSALES\tCHICAGO7902\tFORD\tANALYST\t7566\tNULL\t3000.0\tNULL\t20\t20\tRESEARCH\tDALLAS7934\tMILLER\tCLERK\t7782\tNULL\t1300.0\tNULL\t10\t10\tACCOUNTING\tNEW YORKTime taken: 3.793 seconds, Fetched 14 row(s)...\n\n查看执行计划创建一个表spark-sql&gt; CREATE TABLE t (key STRING, value STRING);\n\n执行如下SQL语句spark-sql&gt; EXPLAIN EXTENDED SELECT a.key * (2 + 3) FROM t a JOIN t b ON a.key = b.key AND a.key &gt; 3;19/02/12 16:45:52 INFO SparkSqlParser: Parsing command: EXPLAIN EXTENDED SELECT a.key * (2 + 3) FROM t a JOIN t b ON a.key = b.key AND a.key &gt; 319/02/12 16:45:52 INFO HiveMetaStore: 0: get_table : db=default tbl=t19/02/12 16:45:52 INFO audit: ugi=simon\tip=unknown-ip-addr\tcmd=get_table : db=default tbl=t19/02/12 16:45:52 INFO CatalystSqlParser: Parsing command: string19/02/12 16:45:52 INFO CatalystSqlParser: Parsing command: string19/02/12 16:45:52 INFO HiveMetaStore: 0: get_table : db=default tbl=t19/02/12 16:45:52 INFO audit: ugi=simon\tip=unknown-ip-addr\tcmd=get_table : db=default tbl=t19/02/12 16:45:52 INFO CatalystSqlParser: Parsing command: string19/02/12 16:45:52 INFO CatalystSqlParser: Parsing command: string== Parsed Logical Plan ==&#x27;Project [unresolvedalias((&#x27;a.key * (2 + 3)), None)]+- &#x27;Join Inner, ((&#x27;a.key = &#x27;b.key) &amp;&amp; (&#x27;a.key &gt; 3))   :- &#x27;SubqueryAlias a   :  +- &#x27;UnresolvedRelation `t`   +- &#x27;SubqueryAlias b      +- &#x27;UnresolvedRelation `t`== Analyzed Logical Plan ==(CAST(key AS DOUBLE) * CAST((2 + 3) AS DOUBLE)): doubleProject [(cast(key#34 as double) * cast((2 + 3) as double)) AS (CAST(key AS DOUBLE) * CAST((2 + 3) AS DOUBLE))#38]+- Join Inner, ((key#34 = key#36) &amp;&amp; (cast(key#34 as int) &gt; 3))   :- SubqueryAlias a   :  +- SubqueryAlias t   :     +- HiveTableRelation `default`.`t`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [key#34, value#35]   +- SubqueryAlias b      +- SubqueryAlias t         +- HiveTableRelation `default`.`t`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [key#36, value#37]== Optimized Logical Plan ==Project [(cast(key#34 as double) * 5.0) AS (CAST(key AS DOUBLE) * CAST((2 + 3) AS DOUBLE))#38]+- Join Inner, (key#34 = key#36)   :- Project [key#34]   :  +- Filter (isnotnull(key#34) &amp;&amp; (cast(key#34 as int) &gt; 3))   :     +- HiveTableRelation `default`.`t`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [key#34, value#35]   +- Project [key#36]      +- Filter (isnotnull(key#36) &amp;&amp; (cast(key#36 as int) &gt; 3))         +- HiveTableRelation `default`.`t`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [key#36, value#37]== Physical Plan ==*Project [(cast(key#34 as double) * 5.0) AS (CAST(key AS DOUBLE) * CAST((2 + 3) AS DOUBLE))#38]+- *SortMergeJoin [key#34], [key#36], Inner   :- *Sort [key#34 ASC NULLS FIRST], false, 0   :  +- Exchange hashpartitioning(key#34, 200)   :     +- *Filter (isnotnull(key#34) &amp;&amp; (cast(key#34 as int) &gt; 3))   :        +- HiveTableScan [key#34], HiveTableRelation `default`.`t`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [key#34, value#35]   +- *Sort [key#36 ASC NULLS FIRST], false, 0      +- ReusedExchange [key#36], Exchange hashpartitioning(key#34, 200)Time taken: 0.117 seconds, Fetched 1 row(s)19/02/12 16:45:52 INFO CliDriver: Time taken: 0.117 seconds, Fetched 1 row(s)\n\n关于执行计划的详细分析，我们会在后面的博客中详细分享。\n","categories":["大数据","Spark","Spark SQL"],"tags":["Hive","Spark SQL"]},{"title":"使用Thriftserver和Beeline","url":"/2019/02/%E4%BD%BF%E7%94%A8Thriftserver%E5%92%8CBeeline/","content":"我们已经有了spark-shell和spark-sql，为什么还要使用Thriftserver呢？\n\n每一个spark-shell &#x2F; spark-sql都是一个Spark Application。\n\nThriftserver不管启动多少个客户端（beeline &#x2F; code），都只会产生一个Spark Application\n\n\n启动thriftserver需要把MySQL JDBC驱动通过jars参数加入进来。\ncd $SPARK_HOME/sbin/start-thriftserver.sh --master local[2] \\--jars ~/.m2/repository/mysql/mysql-connector-java/5.1.46/mysql-connector-java-5.1.46.jar\n\n查看进程jps -m21522 SparkSubmit --master local[2] --class org.apache.spark.sql.hive.thriftserver.HiveThriftServer2--name Thrift JDBC/ODBC Server --jars /Users/simon/.m2/repository/mysql/mysql-connector-java/5.1.46/mysql-connector-java-5.1.46.jar spark-internal3459 Worker --webui-port 8081 spark://localhost:707718420 SparkSubmit --master local[2] --class org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver --jars /Users/simon/.m2/repository/mysql/mysql-connector-java/5.1.46/mysql-connector-java-5.1.46.jar spark-internal\n\n我们看到有两个SparkSubmit进程，21522那个进程就是刚才启动的那个，class是HiveThriftServer2。\n以spark-shell方式启动的class是SparkSQLCLIDriver。\nthriftserver端口默认端口为10000，可以修改为其他端口。\n在启动时增加hiveconf参数，比如把端口号设置为14000。\n--hiveconf hive.server2.thrift.port = 14000\n\nbeeline访问thriftserver需要设置jdbc uri: jdbc:hive2:&#x2F;&#x2F;localhost:10000\nbin/beeline -u jdbc:hive2://localhost:10000 -n simonConnecting to jdbc:hive2://localhost:10000log4j:WARN No appenders could be found for logger (org.apache.hive.jdbc.Utils).log4j:WARN Please initialize the log4j system properly.log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.Connected to: Spark SQL (version 2.2.3)Driver: Hive JDBC (version 1.2.1.spark2)Transaction isolation: TRANSACTION_REPEATABLE_READBeeline version 1.2.1.spark2 by Apache Hive0: jdbc:hive2://localhost:10000&gt;\n\n查看emp表的内容。\n0: jdbc:hive2://localhost:10000&gt; SELECT * FROM emp;+--------+---------+------------+-------+-----------+---------+---------+---------+--+| empno  |  ename  |    job     |  mgr  | hiredate  |   sal   |  comm   | deptno  |+--------+---------+------------+-------+-----------+---------+---------+---------+--+| 7369   | SMITH   | CLERK      | 7902  | NULL      | 800.0   | NULL    | 20      || 7499   | ALLEN   | SALESMAN   | 7698  | NULL      | 1600.0  | 300.0   | 30      || 7521   | WARD    | SALESMAN   | 7698  | NULL      | 1250.0  | 500.0   | 30      || 7566   | JONES   | MANAGER    | 7839  | NULL      | 2975.0  | NULL    | 20      || 7654   | MARTIN  | SALESMAN   | 7698  | NULL      | 1250.0  | 1400.0  | 30      || 7698   | BLAKE   | MANAGER    | 7839  | NULL      | 2850.0  | NULL    | 30      || 7782   | CLARK   | MANAGER    | 7839  | NULL      | 2450.0  | NULL    | 10      || 7788   | SCOTT   | ANALYST    | 7566  | NULL      | 3000.0  | NULL    | 20      || 7839   | KING    | PRESIDENT  | NULL  | NULL      | 5000.0  | NULL    | 10      || 7844   | TURNER  | SALESMAN   | 7698  | NULL      | 1500.0  | 0.0     | 30      || 7876   | ADAMS   | CLERK      | 7788  | NULL      | 1100.0  | NULL    | 20      || 7900   | JAMES   | CLERK      | 7698  | NULL      | 950.0   | NULL    | 30      || 7902   | FORD    | ANALYST    | 7566  | NULL      | 3000.0  | NULL    | 20      || 7934   | MILLER  | CLERK      | 7782  | NULL      | 1300.0  | NULL    | 10      |+--------+---------+------------+-------+-----------+---------+---------+---------+--+14 rows selected (0.411 seconds)\n\n通过程序访问thriftserver添加依赖&lt;!-- Thriftserver 支持 --&gt;&lt;dependency&gt;    &lt;groupId&gt;org.apache.hive&lt;/groupId&gt;    &lt;artifactId&gt;hive-jdbc&lt;/artifactId&gt;    &lt;version&gt;1.1.0&lt;/version&gt;&lt;/dependency&gt;\n\nSparkSQLThriftServerApppackage gy.finolo.sparkimport java.sql.DriverManagerobject SparkSQLThriftServerApp &#123;  def main(args: Array[String]): Unit = &#123;    // 1. 加载驱动    Class.forName(&quot;org.apache.hive.jdbc.HiveDriver&quot;)    // 2. 获取连接    val conn = DriverManager.getConnection(&quot;jdbc:hive2://localhost:10000&quot;, &quot;root&quot;, &quot;&quot;)    val ps = conn.prepareStatement(&quot;SELECT empno, ename, sal FROM emp&quot;)    val rs = ps.executeQuery()    while (rs.next()) &#123;      println(&quot;empno: &quot; + rs.getInt(&quot;empno&quot;) +        &quot;, ename: &quot; + rs.getString(&quot;ename&quot;) +        &quot;, salary: &quot; + rs.getDouble(&quot;sal&quot;))    &#125;    rs.close()    ps.close()    conn.close()  &#125;&#125;\n\n运行结果empno: 7369, ename: SMITH, salary: 800.0empno: 7499, ename: ALLEN, salary: 1600.0empno: 7521, ename: WARD, salary: 1250.0empno: 7566, ename: JONES, salary: 2975.0empno: 7654, ename: MARTIN, salary: 1250.0empno: 7698, ename: BLAKE, salary: 2850.0empno: 7782, ename: CLARK, salary: 2450.0empno: 7788, ename: SCOTT, salary: 3000.0empno: 7839, ename: KING, salary: 5000.0empno: 7844, ename: TURNER, salary: 1500.0empno: 7876, ename: ADAMS, salary: 1100.0empno: 7900, ename: JAMES, salary: 950.0empno: 7902, ename: FORD, salary: 3000.0empno: 7934, ename: MILLER, salary: 1300.0\n\n注意\n通过程序访问thriftserver前，得保证thriftserver已经启动，不然运行程序时，会遇到如下错误：\n\nException in thread &quot;main&quot; java.sql.SQLException: Could not open client transport with JDBC Uri: jdbc:hive2://localhost:10000: java.net.ConnectException: Connection refused (Connection refused)\n\n\n我安装的Hive版本是hive-1.1.0-cdh5.7.0，但我在pom.xml里依赖的jar版本不对时，可能遇到如下异常。\n\nException in thread &quot;main&quot; java.sql.SQLException: Could not open client transport with JDBC Uri: jdbc:hive2://localhost:10000: Could not establish connection to jdbc:hive2://localhost:10000: Required field &#x27;client_protocol&#x27; is unset! Struct:TOpenSessionReq(client_protocol:null, configuration:&#123;use:database=default&#125;)","categories":["大数据","Spark","Spark SQL"],"tags":["Spark","Spark SQL"]},{"title":"Mac Docker安装配置","url":"/2019/07/Mac-Docker%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AE/","content":"Docker在生产环境中的运用越来越多，已经非常成熟。这篇文章将从入门级操作视角，来讲解如何入门学习Docker。\n下载Docker安装软件从Docker官网下载mac版本Docker软件。\n目前的链接是：https://download.docker.com/mac/stable/Docker.dmg\n双击Docker图标，运行程序。\n运行docker version命令，出现如下Client和Server信息，表明安装成功了。\ndocker versionClient: Docker Engine - Community Version:           18.09.2 API version:       1.39 Go version:        go1.10.8 Git commit:        6247962 Built:             Sun Feb 10 04:12:39 2019 OS/Arch:           darwin/amd64 Experimental:      falseServer: Docker Engine - Community Engine:  Version:          18.09.2  API version:      1.39 (minimum version 1.12)  Go version:       go1.10.6  Git commit:       6247962  Built:            Sun Feb 10 04:13:06 2019  OS/Arch:          linux/amd64  Experimental:     false\n\n拉取镜像执行命令docker pull hello-world\ndocker pull hello-worldUsing default tag: latestError response from daemon: Get https://registry-1.docker.io/v2/: net/http: request canceled while waiting forconnection (Client.Timeout exceeded while awaiting headers)\n\n出现上述网络不通的问题，所以要换成国内的镜像源。\n我目前使用的阿里云的容器镜像服务。镜像加速器地址为：https://w3344f23.mirror.aliyuncs.com。每个登录用户都不相同。\n在mac系统上任何栏点击Docker小图标，Preference -&gt; Daemon -&gt; Registry mirrors添加上述镜像url。重启Docker。\n更换国内镜像后再次执行命令，就成功了\ndocker pull hello-worldUsing default tag: latestlatest: Pulling from library/hello-world1b930d010525: Pull completeDigest: sha256:6540fc08ee6e6b7b63468dc3317e3303aae178cb8a45ed3123180328bcc1d20fStatus: Downloaded newer image for hello-world:latest\n\n查看镜像docker imagesREPOSITORY          TAG                 IMAGE ID            CREATED             SIZEhello-world         latest              fce289e99eb9        6 months ago        1.84kB\n\n运行镜像docker run hello-worldHello from Docker!This message shows that your installation appears to be working correctly.To generate this message, Docker took the following steps: 1. The Docker client contacted the Docker daemon. 2. The Docker daemon pulled the &quot;hello-world&quot; image from the Docker Hub.    (amd64) 3. The Docker daemon created a new container from that image which runs the    executable that produces the output you are currently reading. 4. The Docker daemon streamed that output to the Docker client, which sent it    to your terminal.To try something more ambitious, you can run an Ubuntu container with: $ docker run -it ubuntu bashShare images, automate workflows, and more with a free Docker ID: https://hub.docker.com/For more examples and ideas, visit: https://docs.docker.com/get-started/\n\n运行Nginx镜像拉取镜像docker pull hub.c.163.com/library/nginxUsing default tag: latestlatest: Pulling from library/nginx5de4b4d551f8: Pull completed4b36a5e9443: Pull complete0af1f0713557: Pull completeDigest: sha256:f84932f738583e0169f94af9b2d5201be2dbacc1578de73b09a6dfaaa07801d6Status: Downloaded newer image for hub.c.163.com/library/nginx:latest\n\n运行镜像docker run -d hub.c.163.com/library/nginx89979036016d6d09b5cf072e4ad72069e83c0c19929d439233393d0610eefb89\n\n查看运行的容器docker psCONTAINER ID        IMAGE                         COMMAND                  CREATED             STATUS              PORTS               NAMES89979036016d        hub.c.163.com/library/nginx   &quot;nginx -g &#x27;daemon of…&quot;   2 minutes ago       Up 2 minutes        80/tcp              epic_thompson\n\n进入容器容器就相当于一个虚拟机，我们进入到容器中看看。\ndocker exec -it 89979036016d bashroot@89979036016d:/#\n\n执行ps命令\nroot@89979036016d:/# psbash: ps: command not found\n\n命令找不到。\nroot@89979036016d:/# apt-get update &amp;&amp; apt-get install procps\n\n再次执行ps查看nginx进程是否启动成功。\nroot@89979036016d:/# ps -efUID        PID  PPID  C STIME TTY          TIME CMDroot         1     0  0 08:13 ?        00:00:00 nginx: master process nginx -g daemon off;nginx        7     1  0 08:13 ?        00:00:00 nginx: worker processroot         8     0  0 08:16 pts/0    00:00:00 bashroot       350     8  0 09:07 pts/0    00:00:00 ps -ef\n\n设定端口转发停止容器\ndocker stop 89979036016d\n\n重新运行镜像\n我们把主机8080端口的请求转发到容器内部的80端口，使用-p fromport:toport参数\ndocker run -d -p 8080:80 hub.c.163.com/library/nginx38068e229ea71e29de8cf1992dded3520d2334122bfbdc145958788b48de563a\n\n访问页面http://localhost:8080\n就能成功看到nginx的欢迎页面了。\n","categories":["Docker"],"tags":["Docker"]},{"title":"创建Spark SQL Hive程序","url":"/2019/02/%E5%88%9B%E5%BB%BASpark-SQL-Hive%E7%A8%8B%E5%BA%8F/","content":"在这篇文章中，我们将使用SparkSession，打印出Hive中的emp表的内容。\nSpark SQL最基础程序，可以参考博文 创建Spark SQL程序\n创建SparkSessionHiveApp程序pom.xml&lt;properties&gt;    &lt;scala.version&gt;2.11.12&lt;/scala.version&gt;    &lt;scala.compat.version&gt;2.11&lt;/scala.compat.version&gt;    &lt;spark.version&gt;2.2.3&lt;/spark.version&gt;&lt;/properties&gt;\n\n&lt;dependency&gt;    &lt;groupId&gt;org.scala-lang&lt;/groupId&gt;    &lt;artifactId&gt;scala-library&lt;/artifactId&gt;    &lt;version&gt;$&#123;scala.version&#125;&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt;    &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;    &lt;artifactId&gt;spark-sql_$&#123;scala.compat.version&#125;&lt;/artifactId&gt;    &lt;version&gt;$&#123;spark.version&#125;&lt;/version&gt;&lt;/dependency&gt;&lt;!-- hive support --&gt;&lt;dependency&gt;    &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;    &lt;artifactId&gt;spark-hive_$&#123;scala.compat.version&#125;&lt;/artifactId&gt;    &lt;version&gt;$&#123;spark.version&#125;&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt;    &lt;groupId&gt;mysql&lt;/groupId&gt;    &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt;    &lt;version&gt;5.1.46&lt;/version&gt;&lt;/dependency&gt;\n\nSparkSessionHiveApppackage gy.finolo.sparkimport org.apache.spark.sql.SparkSessionobject SparkSessionHiveApp &#123;  def main(args: Array[String]): Unit = &#123;    // 创建Spark Session    val sparkSession = SparkSession.builder()      .master(&quot;local[2]&quot;)      .appName(&quot;Spark Session Hive App&quot;)      .getOrCreate()    // 业务处理    sparkSession.table(&quot;emp&quot;).show()    // 关闭资源    sparkSession.stop()  &#125;&#125;\n\n创建emp表创建emp原始文件vi /usr/local/spark/data/emp.csv\n\n写入如下内容到emp.csv文件中。注意，有些字段为空值。\n7369,SMITH,CLERK,7902,1980/12/17,800,,207499,ALLEN,SALESMAN,7698,1981/2/20,1600,300,307521,WARD,SALESMAN,7698,1981/2/22,1250,500,307566,JONES,MANAGER,7839,1981/4/2,2975,,207654,MARTIN,SALESMAN,7698,1981/9/28,1250,1400,307698,BLAKE,MANAGER,7839,1981/5/1,2850,,307782,CLARK,MANAGER,7839,1981/6/9,2450,,107788,SCOTT,ANALYST,7566,1987/4/19,3000,,207839,KING,PRESIDENT,,1981/11/17,5000,,107844,TURNER,SALESMAN,7698,1981/9/8,1500,0,307876,ADAMS,CLERK,7788,1987/5/23,1100,,207900,JAMES,CLERK,7698,1981/12/3,950,,307902,FORD,ANALYST,7566,1981/12/3,3000,,207934,MILLER,CLERK,7782,1982/1/23,1300,,10\n\n在Hive中创建emp表Hive的基本使用，可以参考博文 Hive环境搭建\n语句后面必须要有分号。\nhive&gt;CREATE TABLE emp(empno   INT,ename   STRING,job     STRING,mgr     INT,hiredate DATE,sal     DOUBLE,comm    DOUBLE,deptno  INT) ROW FORMAT DELIMITED FIELDS TERMINATED BY ‘,’;\n导入数据到emp表中hive&gt; LOAD DATA LOCAL INPATH &#x27;/usr/local/spark/data/emp.csv&#x27; INTO TABLE emp;\n\nhiredate这里有一些问题，以后再解决。\n在IDEA中运行程序可以看到emp表中的结果被成功打印在了控制台上。\n19/02/12 00:00:23 INFO DAGScheduler: Job 0 finished: show at SparkSessionHiveApp.scala:21, took 0.340480 s+-----+------+---------+----+--------+------+------+------+|empno| ename|      job| mgr|hiredate|   sal|  comm|deptno|+-----+------+---------+----+--------+------+------+------+| 7369| SMITH|    CLERK|7902|    null| 800.0|  null|    20|| 7499| ALLEN| SALESMAN|7698|    null|1600.0| 300.0|    30|| 7521|  WARD| SALESMAN|7698|    null|1250.0| 500.0|    30|| 7566| JONES|  MANAGER|7839|    null|2975.0|  null|    20|| 7654|MARTIN| SALESMAN|7698|    null|1250.0|1400.0|    30|| 7698| BLAKE|  MANAGER|7839|    null|2850.0|  null|    30|| 7782| CLARK|  MANAGER|7839|    null|2450.0|  null|    10|| 7788| SCOTT|  ANALYST|7566|    null|3000.0|  null|    20|| 7839|  KING|PRESIDENT|null|    null|5000.0|  null|    10|| 7844|TURNER| SALESMAN|7698|    null|1500.0|   0.0|    30|| 7876| ADAMS|    CLERK|7788|    null|1100.0|  null|    20|| 7900| JAMES|    CLERK|7698|    null| 950.0|  null|    30|| 7902|  FORD|  ANALYST|7566|    null|3000.0|  null|    20|| 7934|MILLER|    CLERK|7782|    null|1300.0|  null|    10|+-----+------+---------+----+--------+------+------+------+\n\n打包并提交作业在测试环境上面已经可以成功运行，我们现在打好包，以spark-submit方式提交作业。\nbin/spark-submit \\--master local[2] \\--class gy.finolo.spark.SparkSessionHiveApp \\--jars /Users/simon/.m2/repository/mysql/mysql-connector-java/5.1.46/mysql-connector-java-5.1.46.jar \\/Users/simon/Development/workspace/scala/spark-sql-demo/target/spark-sql-demo-1.0-SNAPSHOT.jar\n\n需要通过jars参数指定JDBC驱动的位置。\n如果需要加载的jar很多怎么办？我将在以后的博文中讲到这个问题。\n可能遇到的问题JDBC驱动未添加Caused by: org.datanucleus.store.rdbms.connectionpool.DatastoreDriverNotFoundException: The specified datastore driver (&quot;com.mysql.jdbc.Driver&quot;) was not found in the CLASSPATH. Please check your CLASSPATH specification, and the name of the driver.\n\n解决方案：\n如果是在IDEA中开发运行的话，那需要添加驱动的jar包。\n&lt;dependency&gt;    &lt;groupId&gt;mysql&lt;/groupId&gt;    &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt;    &lt;version&gt;5.1.46&lt;/version&gt;&lt;/dependency&gt;\n\nDatastoreDriverNotFoundorg.datanucleus.store.rdbms.connectionpool.DatastoreDriverNotFoundException: The specified datastore driver (&quot;com.mysql.jdbc.Driver&quot;) was not found in the CLASSPATH. Please check your CLASSPATH specification, and the name of the driver.\n\n和上面那个错误是类似的，我们在提交作业时，必须要通过--jars指定mysql-connector-java.jar的位置。\nbin/spark-submit \\--master local[2] \\--class gy.finolo.spark.SparkSessionHiveApp \\--jars /Users/simon/.m2/repository/mysql/mysql-connector-java/5.1.46/mysql-connector-java-5.1.46.jar \\/Users/simon/Development/workspace/scala/spark-sql-demo/target/spark-sql-demo-1.0-SNAPSHOT.jar\n\nTable or view not foundException in thread &quot;main&quot; org.apache.spark.sql.AnalysisException:Table or view not found: emp;\n\n解决方案：\n把Hive conf下面的hive-site.xml拷贝到工程的resources目录下面。\ncp /usr/local/hive/hive-1.1.0-cdh5.7.0/conf/hive-site.xml spark-sql-demo/src/resources","categories":["大数据","Spark","Spark SQL"],"tags":["Spark","Spark SQL"]},{"title":"初识MongoDB","url":"/2019/02/%E5%88%9D%E8%AF%86MongoDB/","content":"MySQL和MongoDB概念对比在mongodb中基本的概念是文档、集合、数据库。\n\n\n\nSQL术语&#x2F;概念\nMongoDB术语&#x2F;概念\n解释&#x2F;说明\n\n\n\natabase\ndatabase\n数据库\n\n\ntable\ncollection\n数据库表&#x2F;集合\n\n\nrow\ndocument\n数据记录行&#x2F;文档\n\n\ncolumn\nfield\n数据字段&#x2F;域\n\n\nindex\nindex\n索引\n\n\ntable\njoins\n表连接, MongoDB不支持\n\n\nprimary key\nprimary key\n主键, MongoDB自动将_id字段设置为主键\n\n\n数据库进入MongoDB shell\nbin/mongoMongoDB shell version v4.0.2connecting to: mongodb://127.0.0.1:27017MongoDB server version: 4.0.2\n\n查看所有的数据库\n&gt; show dbsadmin      0.000GBconfig     0.000GBlocal      0.000GBmongo1-db  0.000GB\n\n执行db命令可以显示当前数据库\n&gt; dblocal\n\n运行use命令，可以连接到一个指定的数据库。\n&gt; use mongo1-dbswitched to db mongo1-db\n\nObjectIdObjectId 类似唯一主键，可以很快的去生成和排序，包含 12 bytes，含义是：\n前 4 个字节表示创建 unix 时间戳,格林尼治时间 UTC 时间，比北京时间晚了 8 个小时\n接下来的 3 个字节是机器标识码\n紧接的两个字节由进程 id 组成 PID\n最后三个字节是随机数\nMongoDB 中存储的文档必须有一个 _id 键。这个键的值可以是任何类型的，默认是个 ObjectId 对象\n由于 ObjectId 中保存了创建的时间戳，所以你不需要为你的文档保存时间戳字段，你可以通过 getTimestamp 函数来获取文档的创建时间:\n&gt; var newObj = ObjectId()&gt; newObj.getTimestamp()ISODate(&quot;2019-02-06T16:01:21Z&quot;)&gt; newObj.str5c5b04d1dd377f0cd58338aa\n\n","categories":["Database","MongoDB"],"tags":["MongoDB"]},{"title":"Spark的local和standalone模式运行","url":"/2019/02/Spark%E7%9A%84local%E5%92%8Cstandalone%E6%A8%A1%E5%BC%8F%E8%BF%90%E8%A1%8C/","content":"Spark运行模式Spark 有很多种模式，最简单就是单机本地模式，还有单机伪分布式模式，复杂的则运行在集群中，目前能很好的运行在 Yarn和 Mesos 中，当然 Spark 还有自带的 Standalone 模式，对于大多数情况 Standalone 模式就足够了，如果企业已经有 Yarn 或者 Mesos 环境，也是很方便部署的。\n\nlocal(本地模式)：常用于本地开发测试，本地还分为local单线程和local-cluster多线程;\nstandalone(集群模式)：典型的Mater&#x2F;slave模式，不过也能看出Master是有单点故障的；Spark支持ZooKeeper来实现 HA\non yarn(集群模式)： 运行在 yarn 资源管理器框架之上，由 yarn 负责资源管理，Spark 负责任务调度和计算\non mesos(集群模式)： 运行在 mesos 资源管理器框架之上，由 mesos 负责资源管理，Spark 负责任务调度和计算\non cloud(集群模式)：比如 AWS 的 EC2，使用这个模式能很方便的访问 Amazon的 S3;Spark 支持多种分布式存储系统：HDFS 和 S3\n\nlocal模式运行spark-shell --master local[2]\n\n2代表2个worker。\n如果local[*]，也是默认master选项，则自动获取机器cores数量。\nstandalone模式运行Spark Standalone模式的架构和Hadoop hdfs&#x2F;yarn很类似，1 master +  n workers\n配置conf&#x2F;spark-env.sh文件cd $SPARK_HOMEcd confcp spark-env.sh.template spark-env.shvi spark-env.sh\n\n添加如下内容：\nSPARK_MASTER_HOST=localhostSPARK_WORKER_CORES=1SPARK_WORKER_MEMORY=2g\n\n启动Sparkcd $SPARK_HOMEsbin/start-all.shstarting org.apache.spark.deploy.master.Master, logging to /usr/local/spark/spark-2.2.3-bin-2.6.0-cdh5.7.0/logs/spark-simon-org.apache.spark.deploy.master.Master-1-localhost.outlocalhost: starting org.apache.spark.deploy.worker.Worker, logging to /usr/local/spark/spark-2.2.3-bin-2.6.0-cdh5.7.0/logs/spark-simon-org.apache.spark.deploy.worker.Worker-1-localhost.out\n\n查看master日志\n19/02/10 13:11:09 INFO Master: I have been elected leader! New state: ALIVE19/02/10 13:11:12 INFO Master: Registering worker 192.168.1.6:51683 with 1 cores, 2.0 GB RAM\n\n查看worker日志\n19/02/10 13:11:12 INFO Worker: Successfully registered with master spark://localhost:7077\n\n执行jps命令，可以看到有Master和Worker进程\n3424 Master3459 Worker\n\n生成wordCount的输入文件新建/usr/local/spark/data/words文件\nvi /usr/local/spark/data/words\n\n添加如下内容\nhello,hello,worldhello,world,welcome\n\n启动Spark-shell以standalone模式启动\nbin/spark-shell --master spark://localhost:7077Using Spark&#x27;s default log4j profile: org/apache/spark/log4j-defaults.propertiesSetting default log level to &quot;WARN&quot;.To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).19/02/10 13:12:41 WARN Utils: Your hostname, localhost resolves to a loopback address: 127.0.0.1; using 192.168.1.6 instead (on interface en0)19/02/10 13:12:41 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address19/02/10 13:12:42 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicableSpark context Web UI available at http://192.168.1.6:4040Spark context available as &#x27;sc&#x27; (master = spark://localhost:7077, app id = app-20190210131243-0000).Spark session available as &#x27;spark&#x27;.Welcome to      ____              __     / __/__  ___ _____/ /__    _\\ \\/ _ \\/ _ `/ __/  &#x27;_/   /___/ .__/\\_,_/_/ /_/\\_\\   version 2.2.3      /_/Using Scala version 2.11.8 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_181)Type in expressions to have them evaluated.Type :help for more information.scala&gt;\n\n输入wordCount程序\nscala&gt; var file = spark.sparkContext.textFile(&quot;file:///usr/local/spark/data/words&quot;)file: org.apache.spark.rdd.RDD[String] = file:///usr/local/spark/data MapPartitionsRDD[6] at textFile at &lt;console&gt;:23scala&gt; val wordCounts = file.flatMap(line =&gt; line.split(&quot;,&quot;)).map(word =&gt; (word, 1)).reduceByKey(_ + _)wordCounts: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[9] at reduceByKey at &lt;console&gt;:25scala&gt; wordCounts.collectres1: Array[(String, Int)] = Array((hello,3), (welcome,1), (world,2))scala&gt;\n\n可以看到&#x2F;usr&#x2F;local&#x2F;spark&#x2F;data&#x2F;words文件里面的单词被成功的统计了。\n","categories":["大数据","Spark"],"tags":["Spark"]},{"title":"编译安装Spark","url":"/2019/02/%E7%BC%96%E8%AF%91%E5%AE%89%E8%A3%85Spark/","content":"Spark会依赖Hadoop版本，当我们使用cdh版本的hadoop时，在Spark官网上下载不了对应的版本，这时就需要编译Spark了。\n下载源码到Spark官网 http://spark.apache.org/downloads.html 下载Spark的源码，并非已经Build好的安装包。\nSpark release我选择的是2.2.3\npackage type选择Source Code\n下载并解压\ncd /usr/local/sparkwget https://archive.apache.org/dist/spark/spark-2.2.3/spark-2.2.3.tgztar -zxvf spark-2.2.3.tgz\n\n构建发布版本查看dev/make-distribution.sh源码，可以知道构建后的包的文件名为spark-$VERSION-bin-$NAME.tgz，所以--name参数设置为2.6.0-cdh5.7.0，-P是指定使用pom.xml中指定的profile，-D是指使用指定的Dependency。\ncd spark-2.2.3./dev/make-distribution.sh --name 2.6.0-cdh5.7.0 --tgz -Phadoop-2.6 -Phive -Phive-thriftserver -Pmesos -Pyarn -Dhadoop.version=2.6.0-cdh5.7.0\n\n构建过程中，我们会发现出现了以下错误：\n[INFO] ------------------------------------------------------------------------[INFO] BUILD FAILURE[INFO] ------------------------------------------------------------------------[INFO] Total time: 46.000 s (Wall Clock)[INFO] Finished at: 2019-02-10T11:28:39+08:00[INFO] ------------------------------------------------------------------------[ERROR] Failed to execute goal on project spark-launcher_2.11: Could not resolve dependencies for project org.apache.spark:spark-launcher_2.11:jar:2.2.3: Could not find artifact org.apache.hadoop:hadoop-client:jar:2.6.0-cdh5.7.0 in alimaven (http://maven.aliyun.com/nexus/content/groups/public/) -&gt; [Help 1]\n\n表明在现有的maven仓库中，找不到cdh版本的jar包。所以，我们得在pom.xml中的repositories中添加cdh的仓库地址。\n编辑pom.xml文件，在repositories标签下的maven central仓库后面添加cloudera的仓库。\n&lt;repository&gt;    &lt;id&gt;cloudera&lt;/id&gt;    &lt;url&gt;https://repository.cloudera.com/artifactory/cloudera-repos/&lt;/url&gt;&lt;/repository&gt;\n\n然后再执行刚才的命令，经过12分钟，spark-2.2.3-bin-2.6.0-cdh5.7.0.tgz安装包构建成功。\n[INFO] ------------------------------------------------------------------------[INFO] BUILD SUCCESS[INFO] ------------------------------------------------------------------------[INFO] Total time: 12:08 min (Wall Clock)[INFO] Finished at: 2019-02-10T11:59:57+08:00[INFO] ------------------------------------------------------------------------\n\n解压spark-2.2.3-bin-2.6.0-cdh5.7.0.tgz包到上级spark目录\ncd ..tar -zxvf spark-2.2.3/spark-2.2.3-bin-2.6.0-cdh5.7.0.tgz -C .\n\n设置环境变量vi ~/.bash_profile\n\n添加内容\nexport SPARK_HOME=/usr/local/spark/spark-2.2.3-bin-2.6.0-cdh5.7.0\n\n使设置生效\nsource ~/.bash_profile\n\n启动Sparkcd $SPARK_HOMEbin/spark-shell --master local[*]Using Spark&#x27;s default log4j profile: org/apache/spark/log4j-defaults.propertiesSetting default log level to &quot;WARN&quot;.To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).19/02/10 12:13:43 WARN Utils: Your hostname, localhost resolves to a loopback address: 127.0.0.1; using 192.168.1.6 instead (on interface en0)19/02/10 12:13:43 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address19/02/10 12:13:43 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicableSpark context Web UI available at http://192.168.1.6:4040Spark context available as &#x27;sc&#x27; (master = local[*], app id = local-1549772024897).Spark session available as &#x27;spark&#x27;.Welcome to      ____              __     / __/__  ___ _____/ /__    _\\ \\/ _ \\/ _ `/ __/  &#x27;_/   /___/ .__/\\_,_/_/ /_/\\_\\   version 2.2.3      /_/Using Scala version 2.11.8 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_181)Type in expressions to have them evaluated.Type :help for more information.scala&gt;\n\nSpark成功启动。\n","categories":["大数据","Spark"],"tags":["Spark"]},{"title":"macOS升级nodejs到最新版本","url":"/2019/07/macOS%E5%8D%87%E7%BA%A7nodejs%E5%88%B0%E6%9C%80%E6%96%B0%E7%89%88%E6%9C%AC/","content":"npm 从5.2版开始，增加了 npx 命令。Node 自带 npm 模块，所以可以直接使用 npx 命令。如果没有安装就手动安装一下。\nsudo npm install -g npx\n\n当我运行\nnpx create-react-app my-appnpx: 91 安装成功，用时 9.606 秒You are running Node 6.3.1.Create React App requires Node 8 or higher.Please update your version of Node.\n\n提示需要更新node。\n查看本机node版本node -vv6.3.1\n\n清除node的cachesudo npm cache clean -f\n\n安装n工具sudo npm install -g n/usr/local/bin/n -&gt; /usr/local/lib/node_modules/n/bin/n/usr/local/lib└── n@4.1.0\n\n安装最新版本的nodesudo n stableinstall : node-v10.16.0       mkdir : /usr/local/n/versions/node/10.16.0       fetch : https://nodejs.org/dist/v10.16.0/node-v10.16.0-darwin-x64.tar.gz################################################################################### 100.0%   installed : v10.16.0\n\n再次查看本机node版本node -vv10.16.0\n\n更新npm到最新版sudo npm install npm@latest -gPassword:/usr/local/bin/npm -&gt; /usr/local/lib/node_modules/npm/bin/npm-cli.js/usr/local/bin/npx -&gt; /usr/local/lib/node_modules/npm/bin/npx-cli.js+ npm@6.10.0added 14 packages from 10 contributors, removed 5 packages and updated 17 packages in 6.962s\n\n验证版本号node -vnpm -v","tags":["nodejs"]},{"title":"创建Spark SQL程序","url":"/2019/02/%E5%88%9B%E5%BB%BASpark-SQL%E7%A8%8B%E5%BA%8F/","content":"这篇博文将讲述如何创建一个Spark SQL的demo程序。\n创建Maven工程在pom.xml文件中需要两个依赖。\n&lt;properties&gt;    &lt;scala.version&gt;2.11.12&lt;/scala.version&gt;    &lt;scala.compat.version&gt;2.11&lt;/scala.compat.version&gt;    &lt;spark.version&gt;2.2.3&lt;/spark.version&gt;&lt;/properties&gt;&lt;dependency&gt;    &lt;groupId&gt;org.scala-lang&lt;/groupId&gt;    &lt;artifactId&gt;scala-library&lt;/artifactId&gt;    &lt;version&gt;$&#123;scala.version&#125;&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt;    &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;    &lt;artifactId&gt;spark-sql_$&#123;scala.compat.version&#125;&lt;/artifactId&gt;    &lt;version&gt;$&#123;spark.version&#125;&lt;/version&gt;&lt;/dependency&gt;\n\n创建SparkSessionApp在Spark 1.x时，是通过创建SQLContext()来获取context的。\n在Spark 2.x后，改为使用SparkSession了。\nargs(0)表示输入文件的地址。\npackage gy.finolo.sparkimport org.apache.spark.sql.SparkSessionobject SparkSessionApp &#123;  def main(args: Array[String]): Unit = &#123;    val path = args(0)    val sparkSession = SparkSession.builder()      .master(&quot;local[2]&quot;)      .appName(&quot;Spark Session App&quot;)      .getOrCreate()    val people = sparkSession.read.format(&quot;json&quot;).load(path)    people.printSchema()    people.show()    sparkSession.stop()  &#125;&#125;\n\n在IDEA中运行时，需要指定Program Arguments为/usr/local/spark/spark-2.2.3-bin-hadoop2.6/examples/src/main/resources/people.json\ncat /usr/local/spark/spark-2.2.3-bin-hadoop2.6/examples/src/main/resources/people.json&#123;&quot;name&quot;:&quot;Michael&quot;&#125;&#123;&quot;name&quot;:&quot;Andy&quot;, &quot;age&quot;:30&#125;&#123;&quot;name&quot;:&quot;Justin&quot;, &quot;age&quot;:19&#125;\n\n提交作业在本地测试完成后，可以打包，通过spark-submit提交作业运行。\nbin/spark-submit \\--master local[2] \\--class gy.finolo.spark.SparkSessionApp \\/Users/simon/Development/workspace/scala/spark-sql-demo/target/spark-sql-demo-1.0-SNAPSHOT.jar \\/usr/local/spark/spark-2.2.3-bin-hadoop2.6/examples/src/main/resources/people.json\n\n可以在console中看到输出的信息。\n其中printSchema()语句打印的内容：\nroot |-- age: long (nullable = true) |-- name: string (nullable = true)\n\npeople.show()输出的内容：\n+----+-------+| age|   name|+----+-------+|null|Michael||  30|   Andy||  19| Justin|+----+-------+","categories":["大数据","Spark"],"tags":["Spark"]},{"title":"Jupyter Notebook快捷键","url":"/2019/04/Jupyter-Notebook%E5%BF%AB%E6%8D%B7%E9%94%AE/","content":"Jupyter有两种模式\n命令模式按Esc切换到命令模式，单元格的边框是蓝色的。这个模式下的常用快捷键（Shift-Enter可能是我们最常用的命令）：\nEnter : 转入编辑模式Shift-Enter : 运行本单元，选中下个单元Ctrl-Enter : 运行本单元Alt-Enter : 运行本单元，在其下插入新单元Y : 单元转入代码状态M :单元转入markdown状态R : 单元转入raw状态1 : 设定 1 级标题2 : 设定 2 级标题3 : 设定 3 级标题4 : 设定 4 级标题5 : 设定 5 级标题6 : 设定 6 级标题Up : 选中上方单元K : 选中上方单元Down : 选中下方单元J : 选中下方单元Shift-K : 扩大选中上方单元Shift-J : 扩大选中下方单元A : 在上方插入新单元B : 在下方插入新单元X : 剪切选中的单元C : 复制选中的单元Shift-V : 粘贴到上方单元V : 粘贴到下方单元Z : 恢复删除的最后一个单元D,D : 删除选中的单元Shift-M : 合并选中的单元Ctrl-S : 文件存盘S : 文件存盘L : 转换行号O : 转换输出Shift-O : 转换输出滚动Esc : 关闭页面Q : 关闭页面H : 显示快捷键帮助I,I : 中断Notebook内核0,0 : 重启Notebook内核Shift : 忽略Shift-Space : 向上滚动Space : 向下滚动\n编辑模式编辑模式按 Enter 键切换，这模式下单元格颜色是绿的： \nTab : 代码补全或缩进Shift-Tab : 提示Ctrl-] : 缩进Ctrl-[ : 解除缩进Ctrl-A : 全选Ctrl-Z : 复原Ctrl-Shift-Z : 再做Ctrl-Y : 再做Ctrl-Home : 跳到单元开头Ctrl-Up : 跳到单元开头Ctrl-End : 跳到单元末尾Ctrl-Down : 跳到单元末尾Ctrl-Left : 跳到左边一个字首Ctrl-Right : 跳到右边一个字首Ctrl-Backspace : 删除前面一个字Ctrl-Delete : 删除后面一个字Esc : 进入命令模式Ctrl-M : 进入命令模式Shift-Enter : 运行本单元，选中下一单元Ctrl-Enter : 运行本单元Alt-Enter : 运行本单元，在下面插入一单元Ctrl-Shift– : 分割单元Ctrl-Shift-Subtract : 分割单元Ctrl-S : 文件存盘Shift : 忽略Up : 光标上移或转入上一单元Down :光标下移或转入下一单元\n","categories":["Python"],"tags":["python"]},{"title":"Java VisualVM添加Visual GC标签","url":"/2019/04/Java-VisualVM%E6%B7%BB%E5%8A%A0Visual-GC%E6%A0%87%E7%AD%BE/","content":"运行Java VisualVM执行命令\njvisualvm\n\nJVisualVM启动成功后，如图所示：\n\n\n安装Visual GC插件点击 工具 -&gt; 插件\n选择设置标签，更新中心配置，也就是设置从哪里下载插件。\n可以在 https://visualvm.github.io/pluginscenters.html 页面找到不同版本的插件源地址。\n设置好以后，在可用插件标签下面，我们选择安装Visual GC插件。\n安装好以后，重启Java VisualVM\n就可以看到Visual GC的标签了。如下图所示：\n\n\n","tags":["JVM"]},{"title":"Jupyter Notebook设置代码提示自动补全，主题和字体","url":"/2019/03/Jupyter-Notebook%E8%AE%BE%E7%BD%AE%E4%BB%A3%E7%A0%81%E6%8F%90%E7%A4%BA%E8%87%AA%E5%8A%A8%E8%A1%A5%E5%85%A8-%E4%B8%BB%E9%A2%98%E5%92%8C%E5%AD%97%E4%BD%93/","content":"启动Console按图示方式进入命令行。\n\n\n安装Jupyter扩展插件Nbextensionspip install jupyter_contrib_nbextensions -i https://pypi.mirrors.ustc.edu.cn/simplejupyter contrib nbextension install --user\n\n安装代码格式化插件conda install yapf\n\n\n重启Jupyter notebook，进入首页后，可以看到Nbextensions标签已经有了。\n选中Hinterland和Code prettify复选框。\n\n\n设置主题和字体安装主题\npip install jupyterthemes\n\n安装成功后可查看可用主题：\njt -l\n\n设置主题\n-t(主题) -f(字体) -fs(字体大小) -cellw(占屏比或宽度) -ofs(输出端的字号) -T(显示工具栏) -N(显示自己主机名)\njt -t monokai -f fira -fs 14 -cellw 94% -ofs 11 -dfs 11 -T -N\n\n\n输出多行结果from IPython.core.interactiveshell import InteractiveShellInteractiveShell.ast_node_interactivity = &#x27;all&#x27;1 * 21 + 223\n\n","categories":["Python","Jupyter Notebook"],"tags":["python","jupyter notebook"]},{"title":"Spark SQL处理外部数据源","url":"/2019/03/Spark-SQL%E5%A4%84%E7%90%86%E5%A4%96%E9%83%A8%E6%95%B0%E6%8D%AE%E6%BA%90/","content":"方便快速从不同的数据源(json, parquet, rdbms)，经过混合处理(json join parquet)再将处理结果以特定的格式(json, parquet)写回到指定的系统中(HDFS, S3)\nspark.read.format(format)\nformat\tbuilt-in json, parquet jdbc, csv(2.x)\tpackages: 外部的非SPark内置 spark-packages.org\t\t\t\tspark.write\n操作parquetParquetApp\nmain\nspark &#x3D; SparkSession\ndf &#x3D; spark.read.format(“parquet”).load(users.parquet”)df.printSchemadf.select*\nnew_df.write.format(“json”).save(new_path)\n默认处理parquet数据\nspark-sql\ncreate temporary view parquetTableUSING…Options(path: fsafsdfsafdsf)\nselect * from parquettable\nspark.read.format().option(path).load()\n处理HiveDf写入到hivewrite.writeAsTable()\n设置分区的数量，默认200\n处理MySQL(jdbc)","categories":["大数据","Spark","Spark SQL"],"tags":["Spark"]},{"title":"Mac下处理MAT(Memory Analyzer Tool)报错的问题","url":"/2019/04/Mac%E4%B8%8B%E5%A4%84%E7%90%86MAT-Memory-Analyzer-Tool-%E6%8A%A5%E9%94%99%E7%9A%84%E9%97%AE%E9%A2%98/","content":"当程序出现内存泄露(Out of memory)时，我们需要通过Eclipse的Memory Analyzer Tool去定位错误的代码。\n从Eclipse官网下载MAT，https://www.eclipse.org/mat/downloads.php\n选择不同操作的版本。\nMemory Analyzer 1.8.1 ReleaseVersion: 1.8.1.20180910 | Date: 19 September 2018 | Type: ReleasedUpdate Site: http://download.eclipse.org/mat/1.8.1/update-site/Archived Update Site: MemoryAnalyzer-1.8.1.201809100846.zipStand-alone Eclipse RCP Applications   Windows (x86)   Windows (x86_64)   Mac OSX (Mac/Cocoa/x86_64)   Linux (x86/GTK+)   Linux (x86_64/GTK+)   Linux (PPC64/GTK+)   Linux (PPC64le/GTK+)\n\n当我在Mac系统双击mat.app打开MAT时，系统报错，查看日志内容如下：\n!SESSION 2019-04-18 21:48:21.506 -----------------------------------------------eclipse.buildId=unknownjava.version=1.8.0_92java.vendor=Oracle CorporationBootLoader constants: OS=macosx, ARCH=x86_64, WS=cocoa, NL=zh_CNFramework arguments:  -keyring /Users/simon/.eclipse_keyringCommand-line arguments:  -os macosx -ws cocoa -arch x86_64 -keyring /Users/simon/.eclipse_keyring!ENTRY org.eclipse.osgi 4 0 2019-04-18 21:48:24.893!MESSAGE Application error!STACK 1java.lang.IllegalStateException: The platform metadata area could not be written: /private/var/folders/cl/kn5ppbpd28j0s6y5kcxm9rdm0000gn/T/AppTranslocation/8157A927-01B7-4CF8-8F8B-AA4530391F8A/d/mat.app/Contents/MacOS/workspace/.metadata.  By default the platform writes its contentunder the current working directory when the platform is launched.  Use the -data parameter tospecify a different content area for the platform.\tat org.eclipse.core.internal.runtime.DataArea.assertLocationInitialized(DataArea.java:70)...\n\n查看日志，分析原因，我们得知其中一个文件无写入权限。IllegalStateException: The platform metadata area could not be written.\n我们可以指定一个具有写入权限的文件夹，设置如下：\n编辑文件 /Applications/mat.app/Contents/Eclipse/MemoryAnalyzer.ini\n添加-data参数：\n-data/Users/simon/workspace\n\ndata参数和路径必须在两个不同的行，data参数必须放在Laucher之前。\n最终文件内容如下所示：\n-startup../Eclipse/plugins/org.eclipse.equinox.launcher_1.5.0.v20180512-1130.jar-data/Users/simon/workspace--launcher.library../Eclipse/plugins/org.eclipse.equinox.launcher.cocoa.macosx.x86_64_1.1.700.v20180518-1200-vmargs-Xmx1024m-Dorg.eclipse.swt.internal.carbon.smallFonts-XstartOnFirstThread\n\n保存文件后，再次双击启动MAT，软件可以成功打开了。\n","tags":["JVM"]},{"title":"Mac下安装zsh","url":"/2019/04/Mac%E4%B8%8B%E5%AE%89%E8%A3%85zsh/","content":"iTerm2首先推荐安装iTerm2终端，可在官网 http://iterm2.com/ 免费下载安装。\n安装oh-my-zsh查看系统的shellcat /etc/shells# List of acceptable shells for chpass(1).# Ftpd will not allow users to connect who are not using# one of these shells./bin/bash/bin/csh/bin/ksh/bin/sh/bin/tcsh/bin/zsh\n\nbash是mac中terminal自带的shell，把它换成zsh，这个的功能要多得多。拥有语法高亮，命令行tab补全，自动提示符，显示Git仓库状态等功能。\nchsh -s /bin/zsh\n\n安装oh-my-zsh使用curl安装oh-my-zsh\nsh -c &quot;$(curl -fsSL https://raw.github.com/robbyrussell/oh-my-zsh/master/tools/install.sh)&quot;Cloning Oh My Zsh...Cloning into &#x27;/Users/simon/.oh-my-zsh&#x27;...remote: Enumerating objects: 1023, done.remote: Counting objects: 100% (1023/1023), done.remote: Compressing objects: 100% (940/940), done.remote: Total 1023 (delta 23), reused 848 (delta 20), pack-reused 0Receiving objects: 100% (1023/1023), 681.01 KiB | 5.00 KiB/s, done.Resolving deltas: 100% (23/23), done.Looking for an existing zsh config...Using the Oh My Zsh template file and adding it to ~/.zshrc         __                                     __  ____  / /_     ____ ___  __  __   ____  _____/ /_ / __ \\/ __ \\   / __ `__ \\/ / / /  /_  / / ___/ __ \\/ /_/ / / / /  / / / / / / /_/ /    / /_(__  ) / / /\\____/_/ /_/  /_/ /_/ /_/\\__, /    /___/____/_/ /_/                        /____/                       ....is now installed!Please look over the ~/.zshrc file to select plugins, themes, and options.p.s. Follow us at https://twitter.com/ohmyzsh.p.p.s. Get stickers, shirts, and coffee mugs at https://shop.planetargon.com/collections/oh-my-zsh.\n\n安装成功。\n如何切换主题就不在这里详述了。\n","categories":["Tools","SSH"],"tags":["zsh"]},{"title":"解决java.lang.ClassNotFoundException: org.springframework.cloud.client.loadbalancer.LoadBalancedRetryFactory","url":"/2019/04/%E8%A7%A3%E5%86%B3java-lang-ClassNotFoundException-org-springframework-cloud-client-loadbalancer-LoadBalancedRetryFactory/","content":"使用Spring Cloud Feign时遇到如下Class Not Found Error\nCaused by: java.lang.NoClassDefFoundError: org/springframework/cloud/client/loadbalancer/LoadBalancedRetryFactory\tat java.lang.Class.getDeclaredMethods0(Native Method) ~[na:1.8.0_92]\tat java.lang.Class.privateGetDeclaredMethods(Class.java:2701) ~[na:1.8.0_92]\tat java.lang.Class.getDeclaredMethods(Class.java:1975) ~[na:1.8.0_92]\tat org.springframework.util.ReflectionUtils.getDeclaredMethods(ReflectionUtils.java:641) ~[spring-core-5.0.0.RC3.jar:5.0.0.RC3]\t... 20 common frames omittedCaused by: java.lang.ClassNotFoundException: org.springframework.cloud.client.loadbalancer.LoadBalancedRetryFactory\tat java.net.URLClassLoader.findClass(URLClassLoader.java:381) ~[na:1.8.0_92]\tat java.lang.ClassLoader.loadClass(ClassLoader.java:424) ~[na:1.8.0_92]\tat sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331) ~[na:1.8.0_92]\tat java.lang.ClassLoader.loadClass(ClassLoader.java:357) ~[na:1.8.0_92]\t... 24 common frames omitted\n\n我使用的软件版本信息如下：\nspringCloudVersion = &#x27;Finchley.M2&#x27;springBootVersion = &#x27;2.0.0.M3&#x27;\n\n解决办法\n把openfeign的版本从原来默认的springCloudVersion改为springBootVersion，也就是2.0.0.M3\n// Feigncompile(&quot;org.springframework.cloud:spring-cloud-starter-openfeign:$&#123;springBootVersion&#125;&quot;)"},{"title":"Docker镜像如何重命名","url":"/2019/10/Docker%E9%95%9C%E5%83%8F%E5%A6%82%E4%BD%95%E9%87%8D%E5%91%BD%E5%90%8D/","content":"Docker镜像制作好以后，发现有些名字命名得并不合理，需要修改。\n查看镜像列表docker imagesREPOSITORY             TAG                 IMAGE ID            CREATED             SIZEsimon/basic_centos     latest              3816db78c729        3 weeks ago         284MB\n\n修改镜像名字如果不知道怎么执行命令，可以先看看帮助文档。\ndocker tag&quot;docker tag&quot; requires exactly 2 arguments.See &#x27;docker tag --help&#x27;.Usage:  docker tag SOURCE_IMAGE[:TAG] TARGET_IMAGE[:TAG]Create a tag TARGET_IMAGE that refers to SOURCE_IMAGE\n\n修改名称\ndocker tag simon/basic_centos simon/basic-centos\n\n删除旧镜像我们再次查看一下镜像列表\ndocker imagesREPOSITORY             TAG                 IMAGE ID            CREATED             SIZEsimon/basic-centos     latest              3816db78c729        3 weeks ago         284MBsimon/basic_centos     latest              3816db78c729        3 weeks ago         284MB\n\n发现上面两个镜像都拥有相同的IMAGE ID。我们需要把旧的镜像删除掉。\ndocker rmi simon/basic_centosUntagged: simon/basic_centos:latest\n","categories":["Docker"],"tags":["Docker"]},{"title":"Docker容器中配置sshd服务","url":"/2019/10/Docker%E5%AE%B9%E5%99%A8%E4%B8%AD%E9%85%8D%E7%BD%AEsshd%E6%9C%8D%E5%8A%A1/","content":"以前我们要做一个实验，需要安装虚拟机，安装需要耗费一定的时间，就算是复制虚拟机，也会占用大量磁盘空间。现在我们只需要创建一个CentOS的Docker容器就可以了，比较方便。\n拉取操作系统镜像这里我们采用CentOS\ndocker pull centos\n\ncentos镜像拉取到本地后，我们查看一下\ndocker images REPOSITORY                    TAG                 IMAGE ID            CREATED             SIZEcentos                        latest              67fa590cfc1c        6 weeks ago         202MB\n\n运行容器并进入容器docker run -it centos bash[root@ca6a84a514c2 /]#\n\n修改yum源如果是在公司内网环境，需要改为内网的源。如果是国外的源，也可以换成国内的源。具体方法请参考 将Centos7的yum源更换为国内阿里云的源\n安装所需软件安装ip工具yum install -y iproute\n\n安装ssh软件yum install -y openssh-server openssh-clients\n\n会有ssh相关软件被拷贝到&#x2F;usr&#x2F;sbin目录\n需要软件sshd的绝对路径去启动sshd服务，不然会报如下错误。\n./sshdsshd re-exec requires execution with an absolute path\n\n输入全路径执行命令，继续出现错误，如下：\n/usr/sbin/sshdCould not load host key: /etc/ssh/ssh_host_rsa_keyCould not load host key: /etc/ssh/ssh_host_ecdsa_keyCould not load host key: /etc/ssh/ssh_host_ed25519_keysshd: no hostkeys available -- exiting.\n\n执行sshd-keygen命令解决上述问题\n/usr/sbin/sshd-keygen Generating SSH2 RSA host key:                              [  OK  ]Generating SSH2 ECDSA host key:                            [  OK  ]Generating SSH2 ED25519 host key:                          [  OK  ]\n\n给root生成一个密码\npasswd rootChanging password for user root.New password: BAD PASSWORD: The password is shorter than 8 charactersRetype new password: passwd: all authentication tokens updated successfully.\n\n启动sshd服务\n/usr/sbin/sshd[root@ca6a84a514c2 sbin]# ps -ef | grep sshdroot        108      1  0 09:14 ?        00:00:00 /usr/sbin/sshd\n\n验证sshd服务\nssh root@localhostroot@localhost&#x27;s password: Last failed login: Sun Sep 29 09:16:59 UTC 2019 from localhost on ssh:nottyThere were 3 failed login attempts since the last successful login.Connection to localhost closed.\n\n会遇到连不上去的问题，这时，需要修改/etc/ssh/sshd_config配置。\n把里面的UsePAM yes改为UsePAM no\n杀掉之前sshd进程后，再次启动\n/usr/sbin/sshdWARNING: &#x27;UsePAM no&#x27; is not supported in Red Hat Enterprise Linux and may cause several problems.\n\n这时就可以成功登录了。\nroot@localhostroot@localhost&#x27;s password: Last login: Sun Sep 29 09:20:41 2019 from localhost\n\n生成一个新的镜像退出容器，并查看容器iddocker ps -aCONTAINER ID        IMAGE                            COMMAND                  CREATED             STATUS                         PORTS                    NAMESca6a84a514c2        centos                           &quot;bash&quot;                   34 minutes ago      Exited (255) 11 seconds ago                             musing_goodall\n\n生成一个新的镜像docker commit ca6a84a514c2 basic_centos sha256:2f965d1db627968f66bf968db7c052f1dec02b84a0b508ef896b3c8d93f97fb7\n\n查看镜像docker imagesREPOSITORY                    TAG                 IMAGE ID            CREATED             SIZEbasic_centos                  latest              2f965d1db627        26 seconds ago      284MB\n\n运行镜像docker run -dit basic_centos /usr/sbin/sshd -D\n\n此时，就可以通过ssh client连接上去了。\n","categories":["Docker"],"tags":["Docker","Kubernetes","SSH"]},{"title":"CentOS如何修改Hostname主机名","url":"/2019/10/CentOS%E5%A6%82%E4%BD%95%E4%BF%AE%E6%94%B9Hostname%E4%B8%BB%E6%9C%BA%E5%90%8D/","content":"CentOS 7环境修改主机名修改主机名hostnamectl set-hostname node1\n\n立即生效，重启以后也不会失效。\n查看主机名hostnamenode1\n\n设置hosts设置hostname对应的ip，主要是在其他节点上面设置node1节点的IP\nvi /etc/hosts\n\nCentOS 6环境修改主机名在文件/etc/sysconfig/network里面添加\nHOSTNAME=node2\n\n重启生效。\n","categories":["OS","CentOS"],"tags":["centos","linux","hostname"]},{"title":"Docker CentOS容器里面安装GlusterFS","url":"/2019/10/Docker-CentOS%E5%AE%B9%E5%99%A8%E9%87%8C%E9%9D%A2%E5%AE%89%E8%A3%85GlusterFS/","content":"拉取CentOS镜像docker pull centos\n\n然后启动容器并进入容器。\n安装GlusterFS软件安装GlusterFS的源yum install centos-release-gluster\n\n安装这个的目的是接下来在安装glusterfs-server等包时，能找到源。\n安装GlusterFS相关软件yum install -y glusterfs glusterfs-server glusterfs-fuse glusterfs-rdma\n\n创建文件夹mkdir -p /gluster/data/\n\n可以把iproute等基本工具安装上。\nyum install -y iproute\n\n生成一个新的镜像，gluster-centos\n运行gluster-centos镜像实例测试最基础的glusterFS功能，启动两个容器就可以。\ndocker run -dit --privileged simon/gluster-centos bash\n\n这里需要注意的是需要使用--privileged这个参数，不然在创建volume时会有Setting extended attributes failed, reason: Operation not permitted.错误。\n\n\n\nnode\nhostname\nip\n\n\n\nnode1\n172.17.0.2\n172.17.0.2\n\n\nnode2\n172.17.0.3\n172.17.0.3\n\n\n创建GlusterFS卷进入node1节点进入node1, 172.17.0.2节点，peer probe另一个节点，执行命令\ngluster peer probe 172.17.0.3peer probe: success.\n\n查看状态gluster peer statusNumber of Peers: 1Hostname: 172.17.0.3Uuid: 6b04331d-6fe9-4d78-937e-b10c69f476faState: Peer in Cluster (Connected)\n\n创建复本卷gluster volume create gv0 replica 2 172.17.0.2:/gluster/data 172.17.0.3:/gluster/dataReplica 2 volumes are prone to split-brain. Use Arbiter or Replica 3 to avoid this. See: http://docs.gluster.org/en/latest/Administrator%20Guide/Split%20brain%20and%20ways%20to%20deal%20with%20it/.Do you still want to continue? (y/n) yvolume create: gv0: failed: The brick 172.17.0.3:/gluster/data is being created in the root partition. It is recommended that you don&#x27;t use the system&#x27;s root partition for storage backend. Or use &#x27;force&#x27; at the end of the command if you want to override this behavior.\n\n按提示，增加force参数如果在启动容器时，没有设置--privileged参数，则会报如下错误。\ngluster volume create gv0 replica 2 172.17.0.2:/gluster/data 172.17.0.3:/gluster/data forcevolume create: gv0: failed: Glusterfs is not supported on brick: 172.17.0.2:/gluster/data.Setting extended attributes failed, reason: Operation not permitted.\n\n所以只能删除容器，再次启动，并加入--privileged参数。\n再次执行gluster volume create命令，执行成功。\ngluster volume create gv0 replica 2 172.17.0.2:/gluster/data 172.17.0.3:/gluster/data forcevolume create: gv0: success: please start the volume to access data\n\n\n\n启动卷gluster volume start gv0volume start: gv0: success\n\n查看卷信息gluster volume info Volume Name: gv0Type: ReplicateVolume ID: 065914c6-38b0-4ef6-ad76-c190ea0cfb0cStatus: StartedSnapshot Count: 0Number of Bricks: 1 x 2 = 2Transport-type: tcpBricks:Brick1: 172.17.0.2:/gluster/dataBrick2: 172.17.0.3:/gluster/dataOptions Reconfigured:performance.client-io-threads: offnfs.disable: ontransport.address-family: inet\n\n创建GlusterFS客户端安装GlusterFS客户端软件主要就是安装glusterfs-fuse\nyum install -y glusterfs glusterfs-fuse\n\n启动一个gluster-centos容器也可以的。挂载点/gluster/data已经创建好了的。\n以glusterfs方式挂载mount -t glusterfs 172.17.0.2:gv0 /gluster/data\n\n此时，在客户端/gluster/data看到的文件，就是分布式系统的文件了。可以像操作本地文件一样操作文件了。\n","categories":["FileSystem"],"tags":["docker","glusterfs","kubernetes","k8s"]},{"title":"NIO的通道","url":"/2019/10/NIO%E7%9A%84%E9%80%9A%E9%81%93/","content":"通道(Channel)，用于源节点和目标节点的连接。在Java NIO中负责数据的传输。Channel本身不存储数据，所以传输时还需要缓冲区配合。\n更多缓冲区的内容，请参考 NIO的缓冲区\n利用通道完成文件复制（非直接缓冲区）可以拷贝超过2G的文件。关于大文件的拷贝，后面再写文章。\ntry (FileInputStream fis = new FileInputStream(&quot;/infile&quot;);     FileOutputStream fos = new FileOutputStream(&quot;/outfile&quot;);     // 获取通道     FileChannel inChannel = fis.getChannel();     FileChannel outChannel = fos.getChannel()) &#123;    // 分配指定大小的缓冲区    ByteBuffer buffer = ByteBuffer.allocate(10240);    // 将通道中的数据存入缓冲区    while (inChannel.read(buffer) != -1) &#123;        buffer.flip();        outChannel.write(buffer);        buffer.clear();    &#125;&#125; catch (IOException e) &#123;    e.printStackTrace();&#125;\n\n使用直接缓冲区完成文件复制文件大小不能超过2G。\ntry (FileChannel inChannel = FileChannel.open(Paths.get(&quot;/infile&quot;),        StandardOpenOption.READ);     FileChannel outChannel = FileChannel.open(Paths.get(&quot;/outfile&quot;),             StandardOpenOption.WRITE, StandardOpenOption.READ, StandardOpenOption.CREATE)) &#123;    // 内存映射文件    MappedByteBuffer inMappedByteBuffer = inChannel.map(FileChannel.MapMode.READ_ONLY, 0, inChannel.size());    MappedByteBuffer outMappedByteBuffer = outChannel.map(FileChannel.MapMode.READ_WRITE, 0, inChannel.size());    // 直接对缓冲区进行数据的读写操作    byte[] bytes = new byte[inMappedByteBuffer.limit()];    inMappedByteBuffer.get(bytes);    outMappedByteBuffer.put(bytes);&#125; catch (IOException e) &#123;    e.printStackTrace();&#125;\n\n通道之间的数据传输(直接缓冲区)文件大小不能超过2G。\n// transferFrom()// transferTo()try (FileChannel inChannel = FileChannel.open(Paths.get(&quot;/Users/simon/Documents/largefile.tar&quot;),        StandardOpenOption.READ);     FileChannel outChannel = FileChannel.open(Paths.get(&quot;/Users/simon/Documents/largefile.tar.copy&quot;),             StandardOpenOption.WRITE, StandardOpenOption.READ, StandardOpenOption.CREATE)) &#123;    inChannel.transferTo(0, inChannel.size(), outChannel);&#125; catch (IOException e) &#123;    e.printStackTrace();&#125;\n\n分散和聚集ByteBuffer[] bufs = new ByteBuffer[0];try (RandomAccessFile raf1 = new RandomAccessFile(&quot;/infile&quot;, &quot;rw&quot;);     // 1. 获取通道     FileChannel raf1Channel = raf1.getChannel()) &#123;    // 2. 分配指定大小的缓冲区数组    ByteBuffer buf1 = ByteBuffer.allocate(100);    ByteBuffer buf2 = ByteBuffer.allocate(1024);    bufs = new ByteBuffer[]&#123;buf1, buf2&#125;;    // 3. 分散读取    raf1Channel.read(bufs);    for (ByteBuffer buf : bufs) &#123;        buf.flip();    &#125;&#125; catch (IOException e) &#123;    e.printStackTrace();&#125;System.out.println(new String(bufs[0].array(), 0, bufs[0].limit()));System.out.println(&quot;---&quot;);System.out.println(new String(bufs[1].array(), 0, bufs[1].limit()));// 4. 聚集写入try (RandomAccessFile raf2 = new RandomAccessFile(&quot;/outfile&quot;, &quot;rw&quot;);     FileChannel raf2Channel = raf2.getChannel()) &#123;    raf2Channel.write(bufs);&#125; catch (IOException e) &#123;    e.printStackTrace();&#125;\n","categories":["Java","NIO"],"tags":["java","io","nio"]},{"title":"NIO的缓冲区","url":"/2019/10/NIO%E7%9A%84%E7%BC%93%E5%86%B2%E5%8C%BA/","content":"最近需要对文件做一些处理，之前接触得比较多的还是基于流的IO，但现在用得比较多的还是NIO(New IO)，可以理解为Non-blocking IO。\n缓冲区(Buffer)缓冲区主要是负责数据的存取的。\n除了boolean这个基础类型外，其他数据类型都对应一个缓冲Buffer。\nByteBufferCharBufferShortBufferIntBufferLongBufferFloatBufferDoubleBuffer\n\n两个核心方法put() 存入数据到缓冲区get() 从缓冲区获取数据\n\n四个核心属性\ncapacity\n\n容量，一旦声明，不可改变\n\nlimit\n\n界限，表示缓冲区可以操作数据的大小。limit后数据不能进行读写操作\n\nposition\n\n位置，表示缓冲区正在操作数据的位置 \n\nmark\n\n标记，记录当前position的位置，后续操作之后，可以通过reset()恢复到mark的位置。\n实例1. 分配一个指定大小的缓冲区String str = &quot;abcde&quot;;ByteBuffer buffer = ByteBuffer.allocate(1024);System.out.println(&quot;--- allocate() ---&quot;);System.out.println(buffer.position());System.out.println(buffer.limit());System.out.println(buffer.capacity());/** * 0 * 1024 * 1024 */\n\n2. 执行put()把数据存入缓冲区buffer.put(str.getBytes());System.out.println(&quot;--- put() ---&quot;);System.out.println(buffer.position());System.out.println(buffer.limit());System.out.println(buffer.capacity());/** * 5 * 1024 * 1024 */\n\n3. 切换到读取数据模式buffer.flip();System.out.println(&quot;--- flip() ---&quot;);System.out.println(buffer.position());System.out.println(buffer.limit());System.out.println(buffer.capacity());/** * 0 * 5 * 1024 */\n\n4. 执行get()读取缓冲区数据byte[] bytes = new byte[buffer.limit()];buffer.get(bytes);System.out.println(&quot;--- get() ---&quot;);System.out.println(new String(bytes));System.out.println(buffer.position());System.out.println(buffer.limit());System.out.println(buffer.capacity());/** * abcde * 5 * 5 * 1024 */\n\n5. 重新读取缓冲区buffer.rewind();System.out.println(&quot;--- rewind() ---&quot;);System.out.println(buffer.position());System.out.println(buffer.limit());System.out.println(buffer.capacity());/** * 0 * 5 * 1024 */\n\n6. mark&#x2F;resetbuffer.get(bytes, 0, 2);System.out.println(new String(bytes, 0, 2));buffer.mark();buffer.get(bytes, 2, 2);buffer.reset();System.out.println(&quot;--- reset() ---&quot;);System.out.println(buffer.position());System.out.println(buffer.limit());System.out.println(buffer.capacity());/** * 2 * 5 * 1024 */if (buffer.hasRemaining()) &#123;    System.out.println(buffer.remaining());    /**     * 3     */&#125;\n\n7. 清空缓冲区buffer.clear();System.out.println(&quot;--- clear() ---&quot;);System.out.println(buffer.position());System.out.println(buffer.limit());System.out.println(buffer.capacity());/** * 0 * 1024 * 1024 */\n","categories":["Java","NIO"],"tags":["java","io","nio"]},{"title":"Ubuntu安装sshd服务","url":"/2019/10/Ubuntu%E5%AE%89%E8%A3%85sshd%E6%9C%8D%E5%8A%A1/","content":"查看是否已经安装sshsudo ps -ef | grep sshsimon      1720   1625  0 09:54 ?        00:00:00 /usr/bin/ssh-agent /usr/bin/im-launch env GNOME_SHELL_SESSION_MODE=ubuntu gnome-session --session=ubuntusimon      2604   2192  0 10:08 pts/0    00:00:00 grep --color=auto ssh\n\n可以看出并没有sshd服务，所以需要安装。\n安装sshdsudo apt install openssh-server\n\n再次查看有没有安装成功。\nsudo ps -ef | grep sshroot       2960      1  0 10:11 ?        00:00:00 /usr/sbin/sshd -D\n\n说明已经安装成功了。\n查看Ubuntu的IP地址ip a1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00    inet 127.0.0.1/8 scope host lo       valid_lft forever preferred_lft forever    inet6 ::1/128 scope host        valid_lft forever preferred_lft forever2: ens33: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc fq_codel state UP group default qlen 1000    link/ether 00:0c:29:69:47:e5 brd ff:ff:ff:ff:ff:ff    inet 172.16.64.224/24 brd 172.16.64.255 scope global dynamic noprefixroute ens33       valid_lft 1513sec preferred_lft 1513sec    inet6 fe80::3e0a:bd57:1c36:3425/64 scope link noprefixroute        valid_lft forever preferred_lft forever\n\n宿主机连接到Ubuntu虚拟机ssh root@172.16.64.224\n\n当然，实际环境中，最好不要用root用户。\n","categories":["Tools","SSH"],"tags":["SSH"]},{"title":"Ubuntu搭建Maven环境","url":"/2019/10/Ubuntu%E6%90%AD%E5%BB%BAMaven%E7%8E%AF%E5%A2%83/","content":"此文介绍的是在Ubuntu OS下搭建Maven环境。若想在MacOS下搭建，请阅读 macOS安装java jdk和maven并配置环境变量\n下载Mavenwget https://archive.apache.org/dist/maven/maven-3/3.6.1/binaries/apache-maven-3.6.1-bin.tar.gz\n\n解压sudo tar -zxvf apache-maven-3.6.1-bin.tar.gz\n\n配置国内镜像源sudo vi apache-maven-3.6.1/conf/settings.xml\n\n先修改仓库地址，也就是下载jar到哪个位置。\n添加localRepository标签。\n&lt;localRepository&gt;/usr/local/maven/repository&lt;/localRepository&gt;\n\n找到mirrors标签，并在标签里面添加如下内容\n&lt;mirror&gt;  &lt;id&gt;alimaven&lt;/id&gt;  &lt;name&gt;aliyun maven&lt;/name&gt;  &lt;url&gt;http://maven.aliyun.com/nexus/content/groups/public/&lt;/url&gt;  &lt;mirrorOf&gt;central&lt;/mirrorOf&gt;&lt;/mirror&gt;&lt;mirror&gt;  &lt;id&gt;alimaven&lt;/id&gt;  &lt;mirrorOf&gt;central&lt;/mirrorOf&gt;  &lt;name&gt;aliyun maven&lt;/name&gt;  &lt;url&gt;http://maven.aliyun.com/nexus/content/repositories/central/&lt;/url&gt;&lt;/mirror&gt;\n\n设置环境变量sudo vi /etc/bash.bashrc\n\n添加以下内容\nexport M2_HOME=/usr/local/maven/apache-maven-3.6.1export PATH=$&#123;JAVA_HOME&#125;/bin:$&#123;M2_HOME&#125;/bin:$PATH\n\n最后让设置生效\nsource /etc/bash.bashrc\n","categories":["Java","Maven"],"tags":["maven"]},{"title":"Ubuntu环境下安装Docker","url":"/2019/10/Ubuntu%E7%8E%AF%E5%A2%83%E4%B8%8B%E5%AE%89%E8%A3%85Docker/","content":"安装Dockersudo apt install docker.io\n\n添加账户到Docker组Docker安装好以后，我们执行docker command时，前面都要加一个sudo，极为不方便。\n现在把登录用户添加到docker用户组中。\nsudo gpasswd -a $&#123;USER&#125; dockerAdding user simon to group docker\n\n重新启动systemctl restart docker==== AUTHENTICATING FOR org.freedesktop.systemd1.manage-units ===Authentication is required to restart &#x27;docker.service&#x27;.Authenticating as: Simon,,, (simon)Password:==== AUTHENTICATION COMPLETE ===\n\n重新登录newgrp docker\n\n或者重新登录一下。\n这时，你会发现在运行Docker命令时，已经不再需要sudo了。\n设置镜像源vi /etc/docker/daemon.json\n\n添加如下内容：\n&#123;    &quot;registry-mirrors&quot;: [&quot;http://hub-mirror.c.163.com&quot;]&#125;\n\n重启Docker\nsystemctl restart docker\n\n查看Docker信息\ndocker info\n\n可以看到如下内容，说明已经设置成功了。\nRegistry Mirrors: http://hub-mirror.c.163.com/\n\n导入镜像导出镜像先从其他环境导出镜像。比如在MacOS上执行命令。\ndocker save simon/gluster-centos &gt; gluster-centos.tar\n\n迁移再把gluster-centos.tar拷贝到Ubuntu里面。\nscp gluster-centos.tar simon@172.16.64.225:/tmp\n\n导入镜像docker load -i /tmp/gluster-centos.tar877b494a9f30: Loading layer [==================================================&gt;]  209.6MB/209.6MBad844b10918d: Loading layer [==================================================&gt;]  198.2MB/198.2MBLoaded image: simon/gluster-centos:latest\n\n验证docker imagesREPOSITORY             TAG                 IMAGE ID            CREATED             SIZEsimon/gluster-centos   latest              140348a9e5de        2 weeks ago         396MB\n","categories":["Docker"],"tags":["Docker"]},{"title":"Ubuntu设置国内源","url":"/2019/10/Ubuntu%E8%AE%BE%E7%BD%AE%E5%9B%BD%E5%86%85%E6%BA%90/","content":"安装完操作系统后，首要的第一件事就是换成国内的源，这样下载软件速度才会有保障。\n备份文件sudo cp /etc/apt/sources.list /etc/apt/sources.list.bak\n\n编译文件编译sources.list，把http://cn.archive.ubuntu.com/ubuntu/和http://security.ubuntu.com/ubuntu替换为国内的阿里的地址http://mirrors.aliyun.com/ubuntu。\nvi /etc/apt/sources.list\n\n使用vi的替换命令\n:s%/cn.archive.ubuntu.com/mirrors.aliyun.com/g\n\n:s%/security.ubuntu.com/mirrors.aliyun.com/g\n\n更新软件列表sudo apt update\n\n更新软件包sudo apt upgrade\n","categories":["OS","Ubuntu"],"tags":["Ubuntu"]},{"title":"Ubuntu配置Java开发环境","url":"/2019/10/Ubuntu%E9%85%8D%E7%BD%AEJava%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83/","content":"下载软件下载jdk，存放到/usr/local/software目录下。\n解压先要创建好/usr/local/java目录。\nsudo tar -zxvf jdk-8u231-linux-x64.tar.gz -C /usr/local/java\n\n添加环境变量sudo vi /etc/bash.bashrc\n\n在文件末添加如下内容\nexport JAVA_HOME=/usr/local/java/jdk1.8.0_231export JRE_HOME=$&#123;JAVA_HOME&#125;/jreexport CLASSPATH=.:$&#123;JAVA_HOME&#125;/lib:$&#123;JRE_HOME&#125;/libexport PATH=$&#123;JAVA_HOME&#125;/bin:$PATH\n\n使配置文件生效source /etc/bash.bashrc\n\n检验java -versionjava version &quot;1.8.0_231&quot;Java(TM) SE Runtime Environment (build 1.8.0_231-b11)Java HotSpot(TM) 64-Bit Server VM (build 25.231-b11, mixed mode)\n","categories":["Java"],"tags":["java","ubuntu"]},{"title":"nginx如何配置https","url":"/2019/10/nginx%E5%A6%82%E4%BD%95%E9%85%8D%E7%BD%AEhttps/","content":"nginx如何配置https？我们今天通过Docker容器启动一台Centos服务器，从自签证书，到nginx安装配置，最终完成配置通过https访问web服务。\n运行Centos容器拉取镜像docker pull centosUsing default tag: latestlatest: Pulling from library/centos729ec3a6ada3: Pull completeDigest: sha256:f94c1d992c193b3dc09e297ffd54d8a4f1dc946c37cbeceb26d35ce1647f88d9Status: Downloaded newer image for centos:latest\n\n如果拉取得慢的话，可以设置国内的registry镜像。\n启动Centos容器docker run -dit centos759dee099156155c7b55b1e4c67bc7a89a52eed1acf865a9fdc1b299d57c33b1\n\n进入容器docker exec -it 759dee099156 bash\n\n生成自签名证书安装openssl工具\nyum install openssl\n\n实验原因，我们统一在&#x2F;opt目录下生成证书。\ncd /opt\n\n生成CA证书生成CA私钥ca.key\nopenssl genrsa -out ca.key 2048Generating RSA private key, 2048 bit long modulus (2 primes)........................................................................+++++...............+++++e is 65537 (0x010001)\n\n注意，centos版本如果是CentOS Linux release 8.0.1905 (Core)版本，私钥长度不能设置成1024位，必须2048位。不然再最后启动nginx时会出如下错误。\nnginx: [emerg] SSL_CTX_use_certificate(&quot;/opt/server.crt&quot;) failed (SSL: error:140AB18F:SSL routines:SSL_CTX_use_certificate:ee key too small)\n\n生成ca.csr\nopenssl req -new -key ca.key -out ca.csrYou are about to be asked to enter information that will be incorporatedinto your certificate request.What you are about to enter is what is called a Distinguished Name or a DN.There are quite a few fields but you can leave some blankFor some fields there will be a default value,If you enter &#x27;.&#x27;, the field will be left blank.-----Country Name (2 letter code) [XX]:CNState or Province Name (full name) []:ShanghaiLocality Name (eg, city) [Default City]:ShanghaiOrganization Name (eg, company) [Default Company Ltd]:Issuer Co., LtdOrganizational Unit Name (eg, section) []:Issuer SectionCommon Name (eg, your name or your server&#x27;s hostname) []:localhostEmail Address []:Please enter the following &#x27;extra&#x27; attributesto be sent with your certificate requestA challenge password []:An optional company name []:\n\n生成CA证书ca.crt\nopenssl x509 -req -in ca.csr -signkey ca.key -out ca.crtSignature oksubject=C = CN, ST = Shanghai, L = Shanghai, O = &quot;Issuer Co., Ltd&quot;, OU = Issuer Section, CN = localhostGetting Private key\n\n生成服务器证书生成服务器端私钥server.key\nopenssl genrsa -out server.key 2048Generating RSA private key, 2048 bit long modulus (2 primes).........+++++...............................................................................................+++++e is 65537 (0x010001)\n\n生成服务器端公钥server.pem\nopenssl rsa -in server.key -pubout -out server.pemwriting RSA key\n\n生成服务器端csr\n服务器端需要向自己的CA机构申请签名证书，在申请签名证书之前，先创建CSR文件。\nopenssl req -new -key server.key -out server.csrYou are about to be asked to enter information that will be incorporatedinto your certificate request.What you are about to enter is what is called a Distinguished Name or a DN.There are quite a few fields but you can leave some blankFor some fields there will be a default value,If you enter &#x27;.&#x27;, the field will be left blank.-----Country Name (2 letter code) [XX]:CNState or Province Name (full name) []:ShanghaiLocality Name (eg, city) [Default City]:ShanghaiOrganization Name (eg, company) [Default Company Ltd]:My CompanyOrganizational Unit Name (eg, section) []:My SectionCommon Name (eg, your name or your server&#x27;s hostname) []:localhostEmail Address []:Please enter the following &#x27;extra&#x27; attributesto be sent with your certificate requestA challenge password []:An optional company name []:\n\n向自己的CA机构申请证书，签名过程需要CA的证书和CA的私钥。最终生成服务器端的CA签名证书server.crt和ca.srl\nopenssl x509 -req -CA ca.crt -CAkey ca.key -CAcreateserial -in server.csr -out server.crtSignature oksubject=C = CN, ST = Shanghai, L = Shanghai, O = My Company, OU = My Section, CN = localhostGetting CA Private Key\n\n安装配置nginx 1.16下载nginx源码包如果没有wget命令需要执行yum install -y wget安装wget工具。\n下载源码包wget http://nginx.org/download/nginx-1.16.1.tar.gz\n\n解压tar -zxvf nginx-1.16.1.tar.gz\n\n更改文件所属用户及用户组在我在docker容器中做实验，所以才会改为root。真实环境改为登录用户帐户就可以了。\nchown -R root:root nginx-1.16.1\n\n进入目录cd nginx-1.16.1\n\n编译安装源码编译安装，需要带上--with-http_ssl_module参数以支持ssl。\n./configure --with-http_ssl_module --with-http_stub_status_module\n\n一路上，有可能会遇到如下问题\nchecking for OS + Linux 5.0.0-31-generic x86_64checking for C compiler ... not found./configure: error: C compiler cc is not found\n\n安装C编译器以解决\nyum install -y gcc\n\n找不到PCRE library\n./configure: error: the HTTP rewrite module requires the PCRE library.You can either disable the module by using --without-http_rewrite_moduleoption, or install the PCRE library into the system, or build the PCRE librarystatically from the source with nginx by using --with-pcre=&lt;path&gt; option.\n\n安装pcre-devel库\nyum install -y pcre-devel\n\n找不到OpenSSL library\n./configure: error: SSL modules require the OpenSSL library.You can either do not enable the modules, or install the OpenSSL libraryinto the system, or build the OpenSSL library statically from the sourcewith nginx by using --with-openssl=&lt;path&gt; option.\n\n还记得我们前面只安装了openssl么？这里还需要安装\nyum install -y openssl-devel\n\n上面三个问题解决以后，就可以成功安装nginx了\n执行make时，如果没有还需要安装\nyum install -y make\n\nmake &amp; make install\n\n配置nginxvi /usr/local/nginx/conf/nginx.conf\n\n按照HTTPS server段的模板内容，进行配置。\n配置的内容如下：\n# HTTPS server#server &#123;    listen       443 ssl;    server_name  localhost;    ssl_certificate      /opt/server.crt;    ssl_certificate_key  /opt/server.key;#    ssl_session_cache    shared:SSL:1m;#    ssl_session_timeout  5m;#    ssl_ciphers  HIGH:!aNULL:!MD5;#    ssl_prefer_server_ciphers  on;    location / &#123;        root   html;        index  index.html index.htm;    &#125;&#125;\n\n启动nginx/usr/local/nginx/sbin/nginx\n\n这个时候访问https://172.17.0.2/，就能看到nginx的欢迎页面了。\n当然，你也可以直接访问http://172.17.0.2/也是可以访问的。\n优化nginx配置先注释或删除掉默认的server段配置。这个时候，80端口就没有监听了。然后再添加如下配置。\nserver &#123;    listen 80;    server_name localhost;    rewrite ^(.*) https://$host$1 permanent;&#125;\n\n这个时候，访问http://172.17.0.2/就会自动跳转到https://172.17.0.2/。\n查看证书详情Issued ToCommon Name (CN)\tlocalhostOrganization (O)\tMy CompanyOrganizational Unit (OU)\tMy SectionIssued ByCommon Name (CN)\tlocalhostOrganization (O)\tIssuer CompanyOrganizational Unit (OU)\tIssuer Section\n","categories":["Tools","Nginx"],"tags":["nginx"]},{"title":"VMware虚拟机Ubuntu设置自适应屏幕大小","url":"/2019/10/VMware%E8%99%9A%E6%8B%9F%E6%9C%BAUbuntu%E8%AE%BE%E7%BD%AE%E8%87%AA%E9%80%82%E5%BA%94%E5%B1%8F%E5%B9%95%E5%A4%A7%E5%B0%8F/","content":"在VMware上安装了Ubuntu虚拟机后，发现屏幕显示得比较别扭，需要通过安装VMware Tools，以达到屏幕自适应宽度。\n点击菜单，虚拟机 -&gt; 安装VMware Tools\n此时CD&#x2F;DVD设置将自动连接到linux.iso镜像。\n把安装软件VMwareTools-10.0.10-4301679.tar.gz拷贝到磁盘中比如/tmp目录。\n解压\ncd /tmptar -zxvf VMwareTools-10.0.10-4301679.tar.gz\n\n运行vmware-install.pl文件\nsudo ./vmware-install.pl\n\n然后一路回车，默认选项，最后重新登录，就可以达到Ubuntu屏幕自适应大小了。\n","categories":["OS","Ubuntu"],"tags":["ubuntu","vmware"]},{"title":"MySQL分组Top N的问题","url":"/2019/09/MySQL%E5%88%86%E7%BB%84Top-N%E7%9A%84%E9%97%AE%E9%A2%98/","content":"经常会遇到分组求前几名的问题。比如，求每个班级中总分排名前三的学生。\n我们通过如下实验看看怎么解决这个问题。\n创建学生分数表CREATE TABLE `score` (  `id` int(11) NOT NULL AUTO_INCREMENT COMMENT &#x27;自增主键&#x27;,  `name` varchar(255) DEFAULT NULL COMMENT &#x27;学生姓名&#x27;,  `class` varchar(255) DEFAULT NULL COMMENT &#x27;班级&#x27;,  `score` int(11) DEFAULT NULL COMMENT &#x27;总分&#x27;,  PRIMARY KEY (`id`)) ENGINE=InnoDB DEFAULT CHARSET=utf8;\n\n创建若干条数据DELIMITER //DROP PROCEDURE IF EXISTS batchInsert;CREATE PROCEDURE batchInsert(n INT)BEGINDECLARE i INT;SET i = 0;WHILE i &lt; n DO\tINSERT INTO score(name, class, score) VALUES\t\t(substring(MD5(RAND()), 1, 10), CONCAT(&#x27;G&#x27;, ROUND(RAND() * 9)), ROUND(RAND() * 100));\tSET i = i + 1;END WHILE;END//DELIMITER ;CALL batchInsert(50);\n\nTOP NHAVING子句中的&lt; 3表示取前两条。\nSELECT s1.class, s1.score FROM score s1\tLEFT JOIN score s2 ON s1.class = s2.class AND s1.score &lt;= s2.score\tGROUP BY s1.class, s1.score\tHAVING COUNT(1) &lt; 3\tORDER BY s1.class, s1.score DESC;\n\n如果是取TOP 1，那就不能用这个自连接了，不然效率非常低下。还是得用MIN, MAX函数来解决。\nclass   scoreG0\t66G0\t17G1\t96G1\t92G2\t87G2\t83G3\t67G3\t32G4\t94G4\t37G5\t99G5\t74G6\t40G7\t98G7\t78G8\t100G8\t93G9\t99G9\t92\n","categories":["Database","MySQL"],"tags":["mysql"]},{"title":"FTP Java Client","url":"/2020/01/FTP-Java-Client/","content":"使用Apache Commons Net，从FTP服务器上下载文件。\n需要使用的依赖：\n&lt;dependency&gt;    &lt;groupId&gt;commons-net&lt;/groupId&gt;    &lt;artifactId&gt;commons-net&lt;/artifactId&gt;    &lt;version&gt;3.6&lt;/version&gt;&lt;/dependency&gt;\n\n有几点需要注意的：\n设置编码需要在connect之前。设置ftp模式需要在connect之后。\npackage gy.finolo;import org.apache.commons.net.ftp.FTPClient;import org.apache.commons.net.ftp.FTPFile;import org.apache.commons.net.ftp.FTPReply;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import java.io.IOException;import java.io.InputStream;import java.io.OutputStream;import java.nio.file.Files;import java.nio.file.Path;import java.nio.file.Paths;import java.nio.file.StandardOpenOption;public class FTPUtils &#123;    private static Logger log = LoggerFactory.getLogger(FTPUtils.class);    public static FTPClient loginFTP(String hostname, int port, String username, String password) throws IOException &#123;        FTPClient ftpClient = new FTPClient();        // 中文文件读取支持, 必须在connect之前设置        ftpClient.setControlEncoding(&quot;UTF-8&quot;);        ftpClient.connect(hostname, port);        // 被动模式设置, 必须在connect之后        ftpClient.enterLocalPassiveMode();        ftpClient.login(username, password);        // 设置文件类型为二进制, 若不设置, 压缩文件可能失败//        ftpClient.setFileType(FTPClient.BINARY_FILE_TYPE);        if (FTPReply.isPositiveCompletion(ftpClient.getReplyCode())) &#123;            log.info(&quot;ftp connection succeed&quot;);        &#125; else &#123;            String message = &quot;ftp server connection error: &quot; + ftpClient.getReplyString();            log.error(message);            throw new IOException(message);        &#125;        return ftpClient;    &#125;    public static void downloadFromFTP(FTPClient ftpClient, String ftpFile, String outDir) throws IOException &#123;        // 在这个方法里把该转的路径都转为Path        boolean isDirectory = ftpClient.changeWorkingDirectory(ftpFile);        Path outDirPath = Paths.get(outDir);        if (isDirectory) &#123;            downloadFTPDirectory2Directory(ftpClient, ftpFile, outDirPath);        &#125; else &#123;            downloadFTPFile2Directory(ftpClient, ftpFile, outDirPath);        &#125;    &#125;    // downlaod directory from ftp to directory (outDir)    private static void downloadFTPDirectory2Directory(FTPClient ftpClient, String ftpFileDir, Path outDirPath) throws IOException &#123;        // 保证输出文件夹要存在        if (!Files.exists(outDirPath)) &#123;            Files.createDirectories(outDirPath);        &#125;        FTPFile[] ftpFiles = ftpClient.listFiles(ftpFileDir);        for (FTPFile file: ftpFiles) &#123;            if (file.isFile()) &#123;                // ftp file full path                Path ftpFileFullPath = Paths.get(ftpFileDir).resolve(file.getName());                downloadFTPFile2Directory(ftpClient, ftpFileFullPath.toString(), outDirPath);                // download files under the directory to the outDir            &#125; else &#123;                String dirName = file.getName();                Path subOutDirPath = outDirPath.resolve(dirName);                Path subFtpDirPath = Paths.get(ftpFileDir).resolve(dirName);                downloadFTPDirectory2Directory(ftpClient, subFtpDirPath.toString(), subOutDirPath);            &#125;        &#125;    &#125;    // download ftp file (ftpFile) to directory (outDir)    private static void downloadFTPFile2Directory(FTPClient ftpClient, String ftpFile, Path outDirPath) throws IOException &#123;        // 保证输出文件夹要存在        if (!Files.exists(outDirPath)) &#123;            Files.createDirectories(outDirPath);        &#125;        // ftpFile is full path        try (InputStream is = ftpClient.retrieveFileStream(ftpFile)) &#123;            if (is != null) &#123;                Path ftpFileNamePath = Paths.get(ftpFile).getFileName();                Path outPath = outDirPath.resolve(ftpFileNamePath);                                int bufSize = 8 * 1024;                byte[] buffer = new byte[bufSize];                int len;                                                    try (OutputStream os = Files.newOutputStream(outPath, StandardOpenOption.TRUNCATE_EXISTING,                        StandardOpenOption.WRITE, StandardOpenOption.CREATE)) &#123;                                        while ((len = is.read(buffer)) &gt; 0) &#123;                        os.write(buffer, 0, len);                    &#125;                &#125;            &#125;        &#125;        ftpClient.completePendingCommand();    &#125;    public static void main(String[] args) throws IOException &#123;        String hostname = &quot;192.168.1.2&quot;;        int port = 21;        String username = &quot;ftpuser&quot;;        String password = &quot;ftppass&quot;;                FTPClient ftpClient;        try &#123;            ftpClient = FTPUtils.loginFTP(hostname, port, username, password);            downloadFromFTP(ftpClient, &quot;.&quot;, &quot;./dir&quot;);            // log out and disconnect from the server            ftpClient.logout();            ftpClient.disconnect();        &#125; catch (IOException e) &#123;            e.printStackTrace();        &#125;    &#125;&#125;\n\n","categories":["Java"],"tags":["java","ftp"]},{"title":"CentOS7搭建本地yum源","url":"/2020/01/CentOS7%E6%90%AD%E5%BB%BA%E6%9C%AC%E5%9C%B0yum%E6%BA%90/","content":"加载光盘iso首先在VMware里面，把CentOS-7-x86_64-Everything-1908.iso镜像文件加载到CD&#x2F;DVD设备中。\n加载成功后，查看状态：\nll /dev/cdrom lrwxrwxrwx. 1 root root 3 Jan  7 09:54 /dev/cdrom -&gt; sr0\n\n\n\n创建挂载目录此目录用于显示光盘内容\nmkdir /mnt/cdrom\n\n\n\n挂载查看挂载类型\ncat /proc/filesystemsnodev\tsysfsnodev\trootfsnodev\tramfs...nodev\tselinuxfs\t    xfs\t    iso9660\n\n我们看到类型有iso9660\nmount -t iso9660 -o ro /dev/cdrom /mnt/cdrom\n\n把参数-t省略也是可以的，即：\nmount /dev/cdrom /mnt/cdrom/\n\n系统会提示按只读方式挂载。\n查看挂载状态：\ndf -hFilesystem               Size  Used Avail Use% Mounted on.../dev/sr0                  11G   11G     0 100% /mnt/cdrom...\n\n\n\n添加yum配置文件备份CentOS-Base.repo文件\nsudo mv /etc/yum.repos.d/CentOS-Base.repo /etc/yum.repos.d/CentOS-Base.repo.bak\n\n创建文件/etc/yum.repos.d/CentOS-Base-Local.repo\n内容如下：\n[base-local]name=CentOS-$releasever - Base-Localbaseurl=file:///mnt/cdromgpgcheck=0  enabled=1              gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-7\n\n清理yum缓存\nyum clean all\n\n查看本地yum源是否生效\nyum repolistLoaded plugins: fastestmirrorLoading mirror speeds from cached hostfilerepo id                                                        repo name                                                                  statusbase-local                                                     CentOS-7 - Base-Local                                                      10,097repolist: 10,097\n\n到此，我们就可以通过yum install安装软件了。\n","categories":["OS","CentOS"],"tags":["centos","yum"]},{"title":"macOS安装Postgresql","url":"/2019/09/macOS%E5%AE%89%E8%A3%85Postgresql/","content":"pip安装依赖的时候报错，pg_config executable not found, 所以需要在mac上安装postgresql。\nbrew install postgresql\n\n如果出现Updating Homebrew并且卡住了，请参考 mac执行brew install时卡在Updating Homebrew的解决方案\n安装过程中，有可能会遇到操作文件权限不够的情况，添加一下权限就可以了。\n安装成功，将有如下提示：\nTo migrate existing data from a previous major version of PostgreSQL run:  brew postgresql-upgrade-databaseTo have launchd start postgresql now and restart at login:  brew services start postgresqlOr, if you don&#x27;t want/need a background service you can just run:  pg_ctl -D /usr/local/var/postgres start==&gt; Summary🍺  /usr/local/Cellar/postgresql/11.5_1: 3,189 files, 35.6MB==&gt; `brew cleanup` has not been run in 30 days, running now...Pruned 4 symbolic links and 58 directories from /usr/local\n","categories":["Database","Postgresql"],"tags":["macOS","Postgresql"]},{"title":"CentOS7搭建SVN服务及备份与恢复","url":"/2020/01/CentOS7%E6%90%AD%E5%BB%BASVN%E6%9C%8D%E5%8A%A1%E5%8F%8A%E5%A4%87%E4%BB%BD%E4%B8%8E%E6%81%A2%E5%A4%8D/","content":"安装SVNyum -y install subversion\n\n创建SVN仓库先创建仓库目录，用来放仓库文件\nmkdir -p /usr/local/svn/repo\n\n在上面目录下创建SVN仓库\nsvnadmin create /usr/local/svn/repo\n\n配置SVN进入SVN配置目录\ncd /usr/local/svn/repo/conf\n\n配置svnserve.conf至少有三个地方需要取消注释。\nauth-access = writepassword-db = passwdauthz-db = authz\n\n配置passwd比如增加一个admin用户\n[users]# harry = harryssecret# sally = sallyssecretadmin = admin\n\n配置authz给admin用户设置根目录的读写权限\n[/]admin = rw\n\n启动SVN服务svnserve -d -r /usr/local/svn/repo/\n\n-d参数是daemon\n-r参数是root，也即是仓库的根目录\n服务启动后，SVN服务在3690端口进行监听\nss -tunlp | grep 3690tcp    LISTEN     0      7         *:3690                  *:*\n\nSVN备份全量备份把/usr/local/svn/repo仓库的内容备份到/usr/local/svn-bak/目录下。\nsudo svnadmin dump /usr/local/svn/repo/ &gt; /usr/local/svn-bak/repobak-all\\* Dumped revision 0.\\* Dumped revision 1.\\* Dumped revision 2.\n\n增量备份查看svn最新的版本号svnlook youngest /usr/local/svn/repo1\n\n可得知svn的版本号为1。\n初始备份sudo svnadmin dump /usr/local/svn/repo/ -r 0:1 --incremental &gt; /usr/local/svn-bak/repobak0-1\\* Dumped revision 0.\\* Dumped revision 1.\n\n-r参数：&lt;start-revision-no.&gt;:&lt;end-revision-no.&gt;，上面例子是0:1，指定备份的开始与结束版本号。\n--incremental参数：表示增量备份。\n再次备份下次备份时，查看最新的svn版本号：\nsvnlook youngest /usr/local/svn/repo2\n\n说明这两次备份之间，只更新了一个版本。\n执行增量备份：\nsudo svnadmin dump /usr/local/svn/repo/ -r 2:2 --incremental &gt; /usr/local/svn-bak/repobak2-2\\* Dumped revision 2.\n\n恢复灾难过后，新建svn仓库，如：/usr/local/svn/recovery-repo/。\n全量恢复sudo svnadmin load /usr/local/svn/recovery-repo/ &lt; /usr/local/svn-bak/repobak\n\n增量恢复先恢复最早的版本。\nsudo svnadmin load /usr/local/svn/recovery-repo/ &lt; /usr/local/svn-bak/repobak0-1 &lt;&lt;&lt; Started new transaction, based on original revision 1​     \\* adding path : project1 ... done.​     \\* adding path : project1/new ... done.------- Committed revision 1 &gt;&gt;&gt;\n\n再恢复最近的版本。\nsudo svnadmin load /usr/local/svn/recovery-repo/ &lt; /usr/local/svn-bak/repobak2-2 &lt;&lt;&lt; Started new transaction, based on original revision 2​     \\* editing path : project1/new ... done.------- Committed revision 2 &gt;&gt;&gt;\n\n恢复后，设置好密码、权限等配置，启动svnserve服务，就算是恢复完成了。\n","categories":["Tools","SVN"],"tags":["centos","SVN"]},{"title":"Java复制文件或目录","url":"/2020/01/Java%E5%A4%8D%E5%88%B6%E6%96%87%E4%BB%B6%E6%88%96%E7%9B%AE%E5%BD%95/","content":"利用java复制文件或目录，要么使用递归的方法，要么使用walkFileTree的方法。大家可以比较一下。\npackage gy.finolo;import java.io.IOException;import java.io.InputStream;import java.io.OutputStream;import java.nio.file.*;import java.nio.file.attribute.BasicFileAttributes;import static java.nio.file.FileVisitResult.CONTINUE;public class CopyUtils &#123;    public static void copy2Directory(Path inPath, Path outDirPath) throws IOException &#123;        // 确保输出目录存在, 如果inPath是File的话, 这里就执行一次, 所以不用写到copyFile2Diretory()方法里        if (!Files.exists(outDirPath)) &#123;            Files.createDirectories(outDirPath);        &#125;        if (Files.isDirectory(inPath)) &#123;            copyFilesInDirectory2Directory(inPath, outDirPath);        &#125; else &#123;            copyFile2Directory(inPath, outDirPath);        &#125;    &#125;    private static void copyFilesInDirectory2Directory(Path inDirPath, Path outDirPath) throws IOException &#123;        DirectoryStream&lt;Path&gt; paths = Files.newDirectoryStream(inDirPath);        for (Path path : paths) &#123;            if (Files.isDirectory(path)) &#123;                Path subOutDirPath = outDirPath.resolve(inDirPath.relativize(path));                // 确保输出目录存在                if (!Files.exists(subOutDirPath)) &#123;                    Files.createDirectories(subOutDirPath);                &#125;                copyFilesInDirectory2Directory(path, subOutDirPath);            &#125; else &#123;                Path subOutDirPath = outDirPath.resolve(inDirPath.relativize(path.getParent()));                copyFile2Directory(path, subOutDirPath);            &#125;        &#125;    &#125;    private static void copyFile2Directory(Path inFilePath, Path outDirPath) throws IOException &#123;        Path fileName = inFilePath.getFileName();        Path outFilePath = outDirPath.resolve(fileName);        int bufSize = 8 * 1024;        byte[] buffer = new byte[bufSize];        int len;        try (InputStream is = Files.newInputStream(inFilePath);             OutputStream os = Files.newOutputStream(outFilePath, StandardOpenOption.TRUNCATE_EXISTING,                     StandardOpenOption.WRITE,                     StandardOpenOption.CREATE)) &#123;            while ((len = is.read(buffer)) &gt; 0) &#123;                os.write(buffer, 0, len);            &#125;        &#125;    &#125;        /**     * 注意: visitFile方法中, 每次需要做一个if (inPath.equals(file))的判断,      * 如果inPath是文件, 那返回为true, 如果inPath为目录, 那永远返回false     * 我是为了统计, 才在里面加了一个判断, 在实际运用中, 有了这个判断效率就不高了。     * @param inPath     * @param outDirPath     * @throws IOException     */    public static void copy2DirectoryV2(Path inPath, Path outDirPath) throws IOException &#123;        // 确保输出目录存在        if (!Files.exists(outDirPath)) &#123;            Files.createDirectories(outDirPath);        &#125;        Files.walkFileTree(inPath, new SimpleFileVisitor&lt;Path&gt;() &#123;            @Override            public FileVisitResult preVisitDirectory(Path dir, BasicFileAttributes attrs)                    throws IOException &#123;                System.out.println(&quot;processing DIR: &quot; + dir);                Path subOutDirPath = outDirPath.resolve(inPath.relativize(dir));                if (!Files.exists(subOutDirPath)) &#123;                    Files.createDirectories(subOutDirPath);                &#125;                return CONTINUE;            &#125;            @Override            public FileVisitResult visitFile(Path file, BasicFileAttributes attrs)                    throws IOException &#123;                System.out.println(&quot;processing FILE: &quot; + file);                Path outPath;                if (inPath.equals(file)) &#123;                    // 说明source是一个文件, 只需要单独复制一个文件即可                    outPath = outDirPath.resolve(file.getFileName());                &#125; else &#123;                    outPath = outDirPath.resolve(inPath.relativize(file));                &#125;                Files.copy(file, outPath);                return CONTINUE;            &#125;        &#125;);    &#125;    public static void main(String[] args) &#123;        Path inPath = Paths.get(&quot;/usr/local&quot;);        Path outDirPath = Paths.get(&quot;/opt&quot;);        try &#123;            // CopyUtils.copy2Directory(inPath, outDirPath);            CopyUtils.copy2DirectoryV2(inPath, outDirPath);        &#125; catch (IOException e) &#123;            e.printStackTrace();        &#125;    &#125;&#125;\n","categories":["Java"],"tags":["java","nio"]},{"title":"Linux批量删除指定后缀名的文件","url":"/2020/01/Linux%E6%89%B9%E9%87%8F%E5%88%A0%E9%99%A4%E6%8C%87%E5%AE%9A%E5%90%8E%E7%BC%80%E5%90%8D%E7%9A%84%E6%96%87%E4%BB%B6/","content":"批量递归删除当前目录下，以.class为后缀的文件。\nfind . -name &#x27;*.class&#x27; -type f -print -exec rm -rf &#123;&#125; \\;\n\n.表示从当前目录开始递归查找\n-name &#39;*.class&#39;根据名称来查找，查找指定目录下以.class结尾的文件\n-type f查找的类型为文件\n-print输出查找到的文件全路径名\n-exec后面写要执行的命令。\n","tags":["Linux"]},{"title":"Ubuntu ens33网卡无ip ","url":"/2020/01/Ubuntu-ens33%E7%BD%91%E5%8D%A1%E6%97%A0ip/","content":"Ubuntu 19.04\nip a1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00    inet 127.0.0.1/8 scope host lo       valid_lft forever preferred_lft forever    inet6 ::1/128 scope host        valid_lft forever preferred_lft forever2: ens33: &lt;BROADCAST,MULTICAST&gt; mtu 1500 qdisc fq_codel state DOWN group default qlen 1000    link/ether 00:0c:29:15:61:1c brd ff:ff:ff:ff:ff:ff\n\n可以看到ens33网卡是DOWN的状态，使用ifup ens33命令说是未知网卡。\nifup ens33ifup: unknown interface ens33\n\n在CentOS上使用的方法在Ubuntu上面不管用。\n重启网络也是不行的。\nsystemctl restart networking\n\n最后使用如下命令：\nsudo dhclient ens33ip a1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00    inet 127.0.0.1/8 scope host lo       valid_lft forever preferred_lft forever    inet6 ::1/128 scope host        valid_lft forever preferred_lft forever2: ens33: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc fq_codel state UP group default qlen 1000    link/ether 00:0c:29:15:61:1c brd ff:ff:ff:ff:ff:ff    inet 192.168.229.129/24 brd 192.168.229.255 scope global dynamic ens33       valid_lft 1798sec preferred_lft 1798sec    inet6 fe80::20c:29ff:fe15:611c/64 scope link        valid_lft forever preferred_lft forever\n\nens33状态为UP了，ip也显示出来了。\n","categories":["OS","Ubuntu"],"tags":["Ubuntu"]},{"title":"Springboot报错:No converter found for return value of type: class","url":"/2020/01/Springboot%E6%8A%A5%E9%94%99-No-converter-found-for-return-value-of-type-class/","content":"调用springboot的api接口时，报错如下：\nWARN 12573 --- [nio-8080-exec-1] .w.s.m.s.DefaultHandlerExceptionResolver : Failed to write HTTP message: org.springframework.http.converter.HttpMessageNotWritableException: No converter found for return value of type: class gy.finolo.common.Response\n\nNo converter found for return value of type: class gy.finolo.common.Response\n主要原因是gy.finolo.common.Response这个自定义类中的属性，少了getter方法。\n在这个类里面，自动生成getter方法就可以了，当然最好也把setter也生成出来。\n","categories":["Java","Spring Boot"],"tags":["java","spring boot"]},{"title":"Windows主机环境CentOS7虚拟机安装VMware Tools","url":"/2020/01/Windows%E4%B8%BB%E6%9C%BA%E7%8E%AF%E5%A2%83CentOS7%E8%99%9A%E6%8B%9F%E6%9C%BA%E5%AE%89%E8%A3%85VMware-Tools/","content":"在Windows主机环境下，给CentOS7虚拟机安装VMware Tools。如果是在MacOS主机环境下，给CentOS7虚拟安装VMware Tools，请参考 mac主机待机后和虚拟机时间不统一的问题\n加载linux.iso镜像在VMware里，点击”安装VMware Tools“\n\n\n光盘挂载的iso文件会自动变为：linux.iso\n挂载光盘到本地目录sudo mount /dev/cdrom /mnt/cdrom\n\n从光盘拷贝文件到工作目录\nsudo cp /mnt/cdrom/VMwareTools-10.3.10-12406962.tar.gz /usr/local/\n\n解压：\ncd /usr/local/sudo tar -zxvf VMwareTools-10.3.10-12406962.tar.gz\n\n安装VMware Toolscd vmware-tools-distrib./vmware-install.pl -bash: ./vmware-install.pl: /usr/bin/perl: bad interpreter: No such file or directory\n\n安装perl, gcc\nsudo yum install perl gcc kernel-devel\n\nsudo ./vmware-install.pl open-vm-tools packages are available from the OS vendor and VMware recommends using open-vm-tools packages. See http://kb.vmware.com/kb/2073803 for more information.Do you still want to proceed with this installation? [no]\n\n输入yes，然后一路回车，默认选项安装。\nTo enable advanced X features (e.g., guest resolution fit, drag and drop, and file and text copy/paste), you will need to do one (or more) of the following:1. Manually start /usr/bin/vmware-user2. Log out and log back into your desktop session3. Restart your X session.\n\nvmware-user: could not open /proc/fs/vmblock/dev 15484 pts/1    R      0:00 /usr/lib/vmware-tools/sbin64/vmtoolsd -n vmusr\n\n此处理错误未找到解决方案，不过已经不影响鼠标的移动了。\n","categories":["OS","CentOS"],"tags":["centos","vmware","windows"]},{"title":"Windows解压安装MySQL","url":"/2020/01/Windows%E8%A7%A3%E5%8E%8B%E5%AE%89%E8%A3%85MySQL/","content":"在Windows10下面解压安装MySQL5.7\n下载安装包下载zip包，mysql-5.7.29-winx64.zip\nhttps://dev.mysql.com/downloads/mysql/5.7.html#downloads\n选择x86, 64-bit\n解压解压到D:\\mysql\\mysql-5.7.29-winx64。\n编写配置文件D:\\mysql\\mysql-5.7.29-winx64\\my.ini\n内容如下：\n[mysql]# 设置mysql客户端默认字符集default-character-set=utf8[mysqld]#设置3306端口port = 3306# 设置mysql的安装目录 basedir=D:\\mysql\\mysql-5.7.29-winx64# 设置mysql数据库的数据的存放目录datadir=D:\\mysql\\data # 允许最大连接数max_connections=200# 服务端使用的字符集默认为UTF8character-set-server=utf8# 创建新表时将使用的默认存储引擎default-storage-engine=INNODB\n\n注册环境变量MYSQL_HOME D:\\mysql\\mysql-5.7.29-winx64\n在Path中加入%MYSQL_HOME%\\bin\n初始化MySQL以管理员方式打开cmd\nD:\\mysql\\mysql-5.7.29-winx64&gt;mysqld --initialize\n\n执行后，data目录下会生成很多文件。\nD:\\mysql\\mysql-5.7.29-winx64&gt;mysqld installService successfully installed.\n\n启动服务D:\\mysql\\mysql-5.7.29-winx64&gt;net start mysqlMySQL 服务正在启动 .MySQL 服务已经启动成功。\n\n登录D:\\mysql\\mysql-5.7.29-winx64&gt;mysql -u root -pEnter password: \n\n需要输入密码，这个初始密码，可以在data/*.err文件里面找到。\n2020-01-19T08:15:03.393313Z 1 [Note] A temporary password is generated for root@localhost: (waO?FJ/E8mW\n\n输入密码，登录成功后，修改root用户的密码。\nALTER USER &#x27;root&#x27;@&#x27;localhost&#x27; IDENTIFIED BY &#x27;root&#x27;;\n","categories":["Database","MySQL"],"tags":["Windows","MySQL"]},{"title":"javascript中的Promise","url":"/2020/01/javascript%E4%B8%AD%E7%9A%84Promise/","content":"Promise最大的好处是在异步执行过程中，把执行代码逻辑和处理请求后的结果分离了，解偶合了。\n\n\n执行代码逻辑，比如请求api后，返回http code 200怎么做，返回400又怎么做。\n可以把如下代码复制到Chrome的Console里面执行。\nnew Promise(function(resolve, reject) &#123;    var return_code = 400 // 任意设置    if (return_code == 200) &#123;        resolve(&#x27;OK&#x27;)    &#125; else &#123;        reject(&#123;code: 400&#125;)    &#125;&#125;).then((res) =&gt; &#123;    console.log(res)&#125;).catch((res) =&gt; &#123;    console.log(res)&#125;);\n","categories":["javascript"],"tags":["javascript","js","es6"]},{"title":"kubernetes中的静态Pod","url":"/2020/01/kubernetes%E4%B8%AD%E7%9A%84%E9%9D%99%E6%80%81Pod/","content":"平常我们提得比较多的Pod，都是通过Deployment，DaemonSet，StatefulSet等方式创建管理的。今天我们介绍一种特殊的Pod，叫静态(Static) Pod。\n什么是静态Pod静态Pod是由kubelet进行管理，仅存在于特定Node上的Pod，这些Pod是不能通过API Server进行管理的，无法与ReplicationController，Deployment或DaemonSet关联。\n静态Pod的创建通过配置yaml文件可以创建静态Pod。只要有kubelet进程，就可以在所在节点运行静态Pod。\n如果通过二进制启动的kubelet，可以在kubelet执行时添加配置参数--pod-manifest-path=&lt;yaml directory&gt;，kubelet会定期扫描目录，应用目录下面的yaml文件来创建静态Pod。\n我是通过kubeadm安装的集群，我们以kube-scheduler这个静态Pod为例，看看它是如何运行的。\nps -ef | grep kubeletroot       6088      1  3 Dec23 ?        01:45:47 /usr/bin/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf --config=/var/lib/kubelet/config.yaml --cgroup-driver=systemd --network-plugin=cni --pod-infra-container-image=k8s.gcr.io/pause:3.1\n\n看到--config的目录为/var/lib/kubelet/config.yaml，查看此文件内容，我们可以看到参数staticPodPath的值：\ncat /var/lib/kubelet/config.yaml | grep staticPodPathstaticPodPath: /etc/kubernetes/manifests\n\n查看目录内容\nll /etc/kubernetes/manifeststotal 16-rw------- 1 root root 1928 Dec 12 10:38 etcd.yaml-rw------- 1 root root 2610 Dec 12 10:38 kube-apiserver.yaml-rw------- 1 root root 2486 Dec 12 10:38 kube-controller-manager.yaml-rw------- 1 root root  990 Dec 12 10:38 kube-scheduler.yaml\n\n由此得知，kubelet会扫描staticPodPath，检测到这个目录下有yaml文件，就创建Pod了。如果要删除Pod，把这些配置文件删除即可。\n","categories":["Kubernetes"],"tags":["Kubernetes","k8s","pod"]},{"title":"java时间Date,timestamp,timezone","url":"/2020/01/java%E6%97%B6%E9%97%B4Date-timestamp-timezone/","content":"文中输出的时间，都是相同的，便于读者观看。\nDate转TimestampDate date = new Date();long currentMillis = date.getTime()System.out.println(currentMillis);1578064861125\n\njava.util.Date, 这个类跟时区是无关的。\nTimestamp转Datelong timeMillis = System.currentTimeMillis();Date date = new Date(timeMillis);System.out.println(date);Fri Jan 03 23:21:01 CST 2020\n\nDate转日期字符串当我们打印出Date时，显示的时间，就是带有时区的字符串了。那为什么是CST（China Standard Time)呢？\nString timeZoneId = TimeZone.getDefault().getID();System.out.println(timeZoneId);Asia/Shanghai\n\n发现系统默认的时区id为Asia/Shanghai。\n如何查看有哪些时区id呢？\nString[] availableIDs = TimeZone.getAvailableIDs();for (String availableID : availableIDs) &#123;    System.out.println(availableID);&#125;Africa/AbidjanAfrica/AccraAfrica/Addis_AbabaAfrica/Algiers...Asia/ShanghaiAsia/SingaporeAsia/SrednekolymskAsia/Taipei\n\n如果我们想查看日本东京的时间呢？\nDate date = new Date();SimpleDateFormat sdf = new SimpleDateFormat(&quot;yyyy-MM-dd HH:mm:ss.SSS Z&quot;);sdf.setTimeZone(TimeZone.getTimeZone(&quot;Asia/Tokyo&quot;));String dateStr = sdf.format(date);System.out.println(dateStr);2020-01-04 00:21:01.125 +0900\n","categories":["Java"],"tags":["java","date"]},{"title":"maven打包时跳过测试用例","url":"/2020/01/maven%E6%89%93%E5%8C%85%E6%97%B6%E8%B7%B3%E8%BF%87%E6%B5%8B%E8%AF%95%E7%94%A8%E4%BE%8B/","content":"mvn clean package -Dmaven.test.skip&#x3D;true\n\n\n[INFO] --- maven-resources-plugin:3.0.1:testResources (default-testResources) @ springboot-mybatis ---[INFO] Not copying test resources[INFO] [INFO] --- maven-compiler-plugin:3.7.0:testCompile (default-testCompile) @ springboot-mybatis ---[INFO] Not compiling test sources[INFO] [INFO] --- maven-surefire-plugin:2.20.1:test (default-test) @ springboot-mybatis ---[INFO] Tests are skipped.\n\n不拷贝，不编译test文件夹里面的内容了。\n","categories":["Java","Maven"],"tags":["java","maven","skiptests"]},{"title":"解决TypeError: this.cliEngine is not a constructor","url":"/2020/01/%E8%A7%A3%E5%86%B3TypeError-this-cliEngine-is-not-a-constructor/","content":"用create-react-app工具创建了一个项目，使用IDEA打开项目，出现如下错误提示：\n\n\n点击Detail查看：\nTypeError: this.cliEngine is not a constructorTypeError: this.cliEngine is not a constructor    at ESLintPlugin.invokeESLint (/Applications/IntelliJ IDEA.app/Contents/plugins/JavaScriptLanguage/languageService/eslint/bin/eslint-plugin.js:97:25)    at ESLintPlugin.getErrors (/Applications/IntelliJ IDEA.app/Contents/plugins/JavaScriptLanguage/languageService/eslint/bin/eslint-plugin.js:76:21)    at ESLintPlugin.onMessage (/Applications/IntelliJ IDEA.app/Contents/plugins/JavaScriptLanguage/languageService/eslint/bin/eslint-plugin.js:42:29)    at Interface.&lt;anonymous&gt; (/Applications/IntelliJ IDEA.app/Contents/plugins/JavaScriptLanguage/jsLanguageServicesImpl/js-language-service.js:105:39)    at Interface.emit (events.js:198:13)    at Interface._onLine (readline.js:308:10)    at Interface._normalWrite (readline.js:451:12)    at Socket.ondata (readline.js:165:10)    at Socket.emit (events.js:198:13)    at addChunk (_stream_readable.js:288:12)Process finished with exit code -1\n\n编辑文件\n把如下内容：\nthis.cliEngine = require(packagePath + &quot;lib/cli-engine&quot;);\n\n修改为：\nthis.cliEngine = require(packagePath + &quot;lib/cli-engine&quot;).CLIEngine;\n\n关闭项目，重新打开项目就可以了。\n","categories":["React"],"tags":["reactjs","react","intellij idea"]},{"title":"彻底清除yum缓存","url":"/2020/01/%E5%BD%BB%E5%BA%95%E6%B8%85%E9%99%A4yum%E7%BC%93%E5%AD%98/","content":"yum clean all\n\nrm -rf /var/cache/yum/*\n","categories":["OS","CentOS"],"tags":["centos","yum"]},{"title":"百度地图开放平台天气预报查询API","url":"/2020/01/%E7%99%BE%E5%BA%A6%E5%9C%B0%E5%9B%BE%E5%BC%80%E6%94%BE%E5%B9%B3%E5%8F%B0%E5%A4%A9%E6%B0%94%E9%A2%84%E6%8A%A5%E6%9F%A5%E8%AF%A2API/","content":"获取天气查询服务的文档地址\nhttp://lbsyun.baidu.com/index.php?title=car/api/weather\n\n\n首先得拿到到ak参数的值，也就是密钥。\n点击”获取密钥”链接，进入页面，注册成为百度地图开放平台开发者。\n\n\n获取密钥\n查询api:\nhttp://api.map.baidu.com/telematics/v3/weather?location=上海&amp;output=json&amp;ak=[ak密钥]\n这个api，已经不开放给新注册的用户了。\n如果是新注册的开发者用户，得看看我下一篇文章了，换成高德开放平台了。\n高德开放平台天气查询API\n","categories":["Tools","API"],"tags":["开放平台"]},{"title":"搭建reactjs和antd的原始项目","url":"/2020/01/%E6%90%AD%E5%BB%BAreactjs%E5%92%8Cantd%E7%9A%84%E5%8E%9F%E5%A7%8B%E9%A1%B9%E7%9B%AE/","content":"安装node, npm查看安装的版本信息\nnode -vv10.16.0npm -v6.10.0\n\n安装react脚手架工具npm install -g create-react-app\n\n查看安装版本\ncreate-react-app -V3.3.0\n\n新建一个项目create-react-app my-app\n\n启动项目\ncd my-appyarn start\n\n安装react-router-domreact-router 4.0版本以后，就只需要安装react-router-dom了，dom代表有dom元素在里面。\nyarn add react-router-dom\n\n安装less-loader, less如果写一个.less的样式文件，react脚手架工具创建出来的项目默认是不支持的，所以需要安装less-loader模块。\nyarn add less-loader less\n\n安装好less-loader后，需要做一些配置才能生效。\n执行yarn eject，把配置文件把暴露出来。系统可能会提示让把package.json和yarn.lockcommit掉。\n可以看到工程中多出了scripts和config目录\n编译config/webpack.config.js\noneOf: [    // &quot;url&quot; loader works like &quot;file&quot; loader except that it embeds assets    // smaller than specified limit in bytes as data URLs to avoid requests.    // A missing `test` is equivalent to a match.    // 添加如下内容    &#123;      test: /\\.less$/,      use: [&#123;        loader: &#x27;style-loader&#x27;,      &#125;, &#123;        loader: &#x27;css-loader&#x27;, // translates CSS into CommonJS      &#125;, &#123;        loader: &#x27;less-loader&#x27;, // compiles Less to CSS        options: &#123;          sourceMap: true,          modifyVars: &#123;            // &#x27;@primary-color&#x27;: &#x27;#F0CF13&#x27;,          &#125;,          javascriptEnabled: true,        &#125;      &#125;]    &#125;,    // 添加上面内容    &#123;      test: [/\\.bmp$/, /\\.gif$/, /\\.jpe?g$/, /\\.png$/],      loader: require.resolve(&#x27;url-loader&#x27;),      options: &#123;        limit: imageInlineSizeLimit,        name: &#x27;static/media/[name].[hash:8].[ext]&#x27;,      &#125;,    &#125;    ...]\n\n@primary-color可以不用设置。\n因为修改了配置，所以需要重启项目。\n这时我们发现，less的样式已经生效了。\n安装axiosyarn add axios\n\n安装antdyarn add antd\n\n在使用时，引入css文件。如果要使用Button组件，引入即可。\nimport &#x27;antd/dist/antd.css&#x27;import &#123;Button&#125; from &quot;antd&quot;;\n\nantd是用less来写的，我们加载的antd.css是全部的样式，这样加载的内容太多了。\n我们如果要达到按需加载的目的，还得安装babel-plugin-import\nyarn add babel-plugin-import\n\n安装好以后，在package.json里面的babel属性里面添加如下内容：\n&quot;babel&quot;: &#123;  &quot;presets&quot;: [    &quot;react-app&quot;  ],  // 添加下面的内容  &quot;plugins&quot;: [    [      &quot;import&quot;,      &#123;        &quot;libraryName&quot;: &quot;antd&quot;,        &quot;style&quot;: true      &#125;    ]  ]  // 添加上面的内容&#125;\n\n重启项目，我们发现，当我们注释掉css以后，antd的样式还是可以正常输出。\n// import &#x27;antd/dist/antd.css&#x27;import &#123;Button&#125; from &quot;antd&quot;;\n\n","categories":["React"],"tags":["react","antd"]},{"title":"通过yum源安装nginx","url":"/2020/01/%E9%80%9A%E8%BF%87yum%E6%BA%90%E5%AE%89%E8%A3%85nginx/","content":"前置依赖，安装这个的目的是可以使用sudo yum-config-manager --enable nginx-mainline命令\nsudo yum install yum-utils\n\n创建yum仓库配置文件\n/etc/yum.repo.d/nginx.repo\n[nginx-stable]name=nginx stable repobaseurl=http://nginx.org/packages/centos/$releasever/$basearch/gpgcheck=1enabled=1gpgkey=https://nginx.org/keys/nginx_signing.keymodule_hotfixes=true[nginx-mainlne]name=nginx mainline repobaseurl=http://nginx.org/packages/mainline/centos/$releasever/$basearch/gpgcheck=1enabled=0gpgkey=https://nginx.org/keys/nginx_signing.keymodule_hotfixes=true\n\n安装nginx\nsudo yum install nginx\n","categories":["Tools","Nginx"],"tags":["centos","nginx","yum"]},{"title":"高德开放平台天气查询API","url":"/2020/01/%E9%AB%98%E5%BE%B7%E5%BC%80%E6%94%BE%E5%B9%B3%E5%8F%B0%E5%A4%A9%E6%B0%94%E6%9F%A5%E8%AF%A2API/","content":"高德开放平台下的天气查询接口文档\nhttps://lbs.amap.com/api/webservice/guide/api/weatherinfo\n注册开发者账号，获取Key\nhttps://restapi.amap.com/v3/weather/weatherInfo?parameters\n\nparameters代表的参数包括必填参数和可选参数。\nhttps://restapi.amap.com/v3/weather/weatherInfo?key=[key]&amp;city=310115\n返回结果：\n&#123;    &quot;status&quot;: &quot;1&quot;,    &quot;count&quot;: &quot;1&quot;,    &quot;info&quot;: &quot;OK&quot;,    &quot;infocode&quot;: &quot;10000&quot;,    &quot;lives&quot;: [        &#123;            &quot;province&quot;: &quot;上海&quot;,            &quot;city&quot;: &quot;浦东新区&quot;,            &quot;adcode&quot;: &quot;310115&quot;,            &quot;weather&quot;: &quot;多云&quot;,            &quot;temperature&quot;: &quot;6&quot;,            &quot;winddirection&quot;: &quot;北&quot;,            &quot;windpower&quot;: &quot;≤3&quot;,            &quot;humidity&quot;: &quot;56&quot;,            &quot;reporttime&quot;: &quot;2020-01-12 17:57:21&quot;        &#125;    ]&#125;\n\n百度api是jsonp的方式获取数据，而高德地图的api是允许跨域的。\n\n","categories":["Tools","API"],"tags":["开放平台","天气","API","高德"]},{"title":"在CentOS上以二进制方式安装Docker","url":"/2019/11/%E5%9C%A8CentOS%E4%B8%8A%E4%BB%A5%E4%BA%8C%E8%BF%9B%E5%88%B6%E6%96%B9%E5%BC%8F%E5%AE%89%E8%A3%85Docker/","content":"之前有介绍过，在MacOS下安装Docker Mac Docker安装配置 和在Ubuntu环境下安装Docker Ubuntu环境下安装Docker ，可以参考。\n不过在生产环境，大多数是没有外网环境的，所以还是需要以二进制方式来安装。\n这篇文章将介绍如何下载Docker二进制文件来安装配置Docker。\n查看当前环境uname -r3.10.0-957.el7.x86_64\n\n下载Docker安装包Docker稳定版的程序可进入如下链接进行下载。\nhttps://download.docker.com/linux/static/stable/x86_64/\nwget https://download.docker.com/linux/static/stable/x86_64/docker-18.06.3-ce.tgz\n\n解压和拷贝tar -xvf docker-18.06.3-ce.tgzcp docker/* /usr/local/bin\n\n制作Docker.service创建文件/usr/lib/systemd/system/docker.service并添加如下内容：\n[Unit]Description=Docker Application Container EngineDocumentation=http://docs.docker.io[Service]ExecStart=/usr/local/bin/dockerdExecReload=/bin/kill -s HUP $MAINPIDRestart=on-failureRestartSec=5LimitNOFILE=infinityLimitNPROC=infinityLimitCORE=infinityDelegate=yesKillMode=process[Install]WantedBy=multi-user.target\n\n当然，里面还可能会有其他一些配置，大家可以根据自己需要来配置。\n启动Dockerchmod +x /usr/lib/systemd/system/docker.servicesudo systemctl daemon-reloadsudo systemctl start docker\n\n查看Docker版本信息如果安装成功，则可以看到如下信息。\ndocker -vDocker version 18.06.3-ce, build d7080c1\n\ndocker-compose二进制安装下载docker-compose文件curl -L https://github.com/docker/compose/releases/download/1.25.0/docker-compose-`uname -s`-`uname -m` -o /usr/local/bin/docker-composechmod +x /usr/local/bin/docker-compose\n\nuname -s执行结果是Linux\nuname -m执行结果是x86_64\n查看docker-compose的版本信息docker-compose -vdocker-compose version 1.25.0, build 0a186604\n\n设置Docker镜像代理等信息vi /etc/docker/daemon.json\n&#123;    &quot;registry-mirrors&quot;: [&quot;http://hub-mirror.c.163.com&quot;]&#125;\n\n通过docker info来查看是否设置成功。\n","categories":["Docker"],"tags":["centos","docker"]},{"title":"CentOS环境安装Docker","url":"/2019/11/CentOS%E7%8E%AF%E5%A2%83%E5%AE%89%E8%A3%85Docker/","content":"在CentOS7.6环境下通过yum安装Docker通过二进制方式安装请参考 在CentOS上以二进制方式安装Docker\n下载repocd /etc/yum.repos.d/wget http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo\n\n安装dockeryum install -y docker-ce\n\n启动dockersystemctl start docker\n\nsystemd下的docker服务安装并启动docker后，我们可以看到有两个service unit被生成了。\ndocker.service服务/usr/lib/systemd/system/docker.service\n[Unit]Description=Docker Application Container EngineDocumentation=https://docs.docker.comBindsTo=containerd.serviceAfter=network-online.target firewalld.service containerd.serviceWants=network-online.targetRequires=docker.socket[Service]Type=notify# the default is not to use systemd for cgroups because the delegate issues still# exists and systemd currently does not support the cgroup feature set required# for containers run by dockerExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sockExecReload=/bin/kill -s HUP $MAINPIDTimeoutSec=0RestartSec=2Restart=always# Note that StartLimit* options were moved from &quot;Service&quot; to &quot;Unit&quot; in systemd 229.# Both the old, and new location are accepted by systemd 229 and up, so using the old location# to make them work for either version of systemd.StartLimitBurst=3# Note that StartLimitInterval was renamed to StartLimitIntervalSec in systemd 230.# Both the old, and new name are accepted by systemd 230 and up, so using the old name to make# this option work for either version of systemd.StartLimitInterval=60s# Having non-zero Limit*s causes performance problems due to accounting overhead# in the kernel. We recommend using cgroups to do container-local accounting.LimitNOFILE=infinityLimitNPROC=infinityLimitCORE=infinity# Comment TasksMax if your systemd version does not support it.# Only systemd 226 and above support this option.TasksMax=infinity# set delegate yes so that systemd does not reset the cgroups of docker containersDelegate=yes# kill only the docker process, not all processes in the cgroupKillMode=process[Install]WantedBy=multi-user.target\n\ndocker.socket服务/usr/lib/systemd/system/docker.socket\n[Unit]Description=Docker Socket for the APIPartOf=docker.service[Socket]ListenStream=/var/run/docker.sockSocketMode=0660SocketUser=rootSocketGroup=docker[Install]WantedBy=sockets.target\n\n启动docker的进程root       1660      1  0 23:02 ?        00:00:03 /usr/bin/containerdroot       1820      1  0 23:13 ?        00:00:00 /usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock\n","categories":["Docker"],"tags":["centos","docker"]},{"title":"Docker容器中使用systemctl报错Failed to get D-Bus connection: Operation not permitted","url":"/2019/11/Docker%E5%AE%B9%E5%99%A8%E4%B8%AD%E4%BD%BF%E7%94%A8systemctl%E6%8A%A5%E9%94%99Failed-to-get-D-Bus-connection-Operation-not-permitted/","content":"在Docker容器中执行systemctl命令时，无论是启动nginx, 或者是vsftpd等服务，都会报如下错误。\nsystemctl status vsftpdFailed to get D-Bus connection: Operation not permitted\n\n解决方法\n\n在启动容器时，需要加上--privileged参数来添加权限。\n不能使用默认的bash，而需要执行/usr/sbin/init\n\n执行命令：\ndocker run -dit --privileged &lt;image_id&gt; /usr/sbin/init\n\n以下我们以安装启动vsftpd服务来例子来讲解。\n启动centos的容器\ndocker run -dit --privileged 3816db78c729 /usr/sbin/init\n\n进入刚才启动的容器\ndocker exec -it d9c3cf1ad7a bash\n\nyum安装vsftpd服务\nyum install -y vsftpd\n\n启动vsftpd服务\nsystemctl start vsftpd\n\n查看vsftpd服务的状态\nsystemctl status vsftpd● vsftpd.service - Vsftpd ftp daemon   Loaded: loaded (/usr/lib/systemd/system/vsftpd.service; disabled; vendor preset: disabled)   Active: active (running) since Sat 2019-11-16 03:55:51 UTC; 2s ago  Process: 176 ExecStart=/usr/sbin/vsftpd /etc/vsftpd/vsftpd.conf (code=exited, status=0/SUCCESS) Main PID: 177 (vsftpd)   CGroup: /docker/d9c3cf1ad7a9084f7563169cbac71b7a5687ebc25703c43f31b6f33a59e686e6/system.slice/vsftpd.service           └─177 /usr/sbin/vsftpd /etc/vsftpd/vsftpd.conf           ‣ 177 /usr/sbin/vsftpd /etc/vsftpd/vsftpd.confNov 16 03:55:51 d9c3cf1ad7a9 systemd[1]: Starting Vsftpd ftp daemon...Nov 16 03:55:51 d9c3cf1ad7a9 systemd[1]: Started Vsftpd ftp daemon.\n","categories":["Docker"],"tags":["centos","docker"]},{"title":"解决sudo docker报错command not found","url":"/2019/11/%E8%A7%A3%E5%86%B3sudo-docker%E6%8A%A5%E9%94%99command-not-found/","content":"我们在执行docker命令时，比如docker ps时，可能会遇到如下command not found的错误。\nsudo docker ps[sudo] password for simon: sudo: docker: command not found\n\n这是什么原因呢？我们看看如下两条命令。\nenv | grep PATHPATH=/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/home/simon/.local/bin:/home/simon/binsudo env | grep PATHPATH=/sbin:/bin:/usr/sbin:/usr/bin\n\n发现环境变量PATH是不相同的。之所以出现sudo: docker: commmand not found的问题，是因为在PATH=/sbin:/bin:/usr/sbin:/usr/bin下面找不到docker程序。\n可以添加一个docker组来解决。\n如果是通过二进制安装的话，就不会自动生成docker组，所以需要我们自己生成。\nsudo groupadd docker\n\n同时把用户加入到这个组中，并重启docker。\nsudo gpasswd -a simon dockerAdding user simon to group dockersudo systemctl restart docker\n\n用户以docker组重新登录一下\nnewgrp docker\n\n这时，就可以正常使用docker命令了。\n那我们会问，如果不把用户加入docker组，不加sudo执行docker命令又会怎样呢？\ndocker psGot permission denied while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock: Get http://%2Fvar%2Frun%2Fdocker.sock/v1.38/containers/json: dial unix /var/run/docker.sock: connect: permission denied\n\n会发现当我们连接socket的时候，没法访问/var/run/docker.sock这个文件。\n原因是，这个文件所属用户和用户组都是root的。我们用普通用户是没法访问的。\nls -l /var/run/docker.socksrw-rw----. 1 root root 0 Nov 21 21:27 /var/run/docker.sock\n\n但是，当我们把普通用户加入到docker这个用户组以后，那生成的/var/run/docker.sock的用户组已经变成docker了，所以就可以连接了。\nls -l /var/run/docker.socksrw-rw----.  1 root docker    0 Nov 22 09:48 docker.sock\n\n当然，我们没有直接解决sudo: docker: command not found的问题，但我觉得以上是解决docker命令没法使用的最佳实践。\n","categories":["Docker"],"tags":["centos","docker"]},{"title":"如何为Ubuntu虚拟机扩展磁盘","url":"/2019/11/%E5%A6%82%E4%BD%95%E4%B8%BAUbuntu%E8%99%9A%E6%8B%9F%E6%9C%BA%E6%89%A9%E5%B1%95%E7%A3%81%E7%9B%98/","content":"VMWare安装了Ubuntu1904虚拟机，空间不足了，此篇文章将讲解如何扩容磁盘。\n查看磁盘使用情况df -hFilesystem                              Size  Used Avail Use% Mounted onudev                                    3.9G     0  3.9G   0% /devtmpfs                                   796M  1.9M  794M   1% /run/dev/sda1                                20G   18G  921M  96% /tmpfs                                   3.9G     0  3.9G   0% /dev/shmtmpfs                                   5.0M  4.0K  5.0M   1% /run/locktmpfs                                   3.9G     0  3.9G   0% /sys/fs/cgrouptmpfs                                   796M   40K  796M   1% /run/user/1000\n\n可以看出20G的磁盘使用率达到96%了，需要赶紧扩容了。\n在VMWare中设置虚拟机属性首先把Ubuntu虚拟机关机。\n在VMWare中可以看到磁盘大小为20G。\n\n\n点击”扩展”按钮。\n\n\n磁盘从原始的20G扩展到30G\n\n\n这里操作完成以后，还需要进入到Ubuntu系统中进行分区和扩展操作。\n\n\n重启Ubuntu虚拟机再次查看硬盘使用情况。\ndf -hFilesystem      Size  Used Avail Use% Mounted onudev            3.9G     0  3.9G   0% /devtmpfs           796M  1.8M  794M   1% /run/dev/sda1        20G   18G  978M  95% /tmpfs           3.9G     0  3.9G   0% /dev/shmtmpfs           5.0M  4.0K  5.0M   1% /run/locktmpfs           3.9G     0  3.9G   0% /sys/fs/cgrouptmpfs           796M   28K  796M   1% /run/user/1000\n\n可以发现，实质上是没有变化的。但通过fdisk可以看到&#x2F;dev&#x2F;sda设备已经有30G了。\nsudo fdisk -lDisk /dev/sda: 30 GiB, 32212254720 bytes, 62914560 sectorsDisk model: VMware Virtual SUnits: sectors of 1 * 512 = 512 bytesSector size (logical/physical): 512 bytes / 512 bytesI/O size (minimum/optimal): 512 bytes / 512 bytesDisklabel type: dosDisk identifier: 0xa4334a8dDevice     Boot Start      End  Sectors Size Id Type/dev/sda1  *     2048 41940991 41938944  20G 83 Linux\n\n备份数据为了防止数据丢失，为虚拟机做一个快照吧。\n\n\n扩容操作sudo fdisk /dev/sda[sudo] password for simon: Welcome to fdisk (util-linux 2.33.1).Changes will remain in memory only, until you decide to write them.Be careful before using the write command.Command (m for help): mHelp:  DOS (MBR)   a   toggle a bootable flag   b   edit nested BSD disklabel   c   toggle the dos compatibility flag  Generic   d   delete a partition   F   list free unpartitioned space   l   list known partition types   n   add a new partition   p   print the partition table   t   change a partition type   v   verify the partition table   i   print information about a partition  Misc   m   print this menu   u   change display/entry units   x   extra functionality (experts only)  Script   I   load disk layout from sfdisk script file   O   dump disk layout to sfdisk script file  Save &amp; Exit   w   write table to disk and exit   q   quit without saving changes  Create a new label   g   create a new empty GPT partition table   G   create a new empty SGI (IRIX) partition table   o   create a new empty DOS partition table   s   create a new empty Sun partition tableCommand (m for help): \n\n可以先打印分区表看看基本信息\nCommand (m for help): pDisk /dev/sda: 30 GiB, 32212254720 bytes, 62914560 sectorsDisk model: VMware Virtual SUnits: sectors of 1 * 512 = 512 bytesSector size (logical/physical): 512 bytes / 512 bytesI/O size (minimum/optimal): 512 bytes / 512 bytesDisklabel type: dosDisk identifier: 0xa4334a8dDevice     Boot Start      End  Sectors Size Id Type/dev/sda1  *     2048 41940991 41938944  20G 83 LinuxCommand (m for help):\n\n如果有交换分区(Linux swap)，比如&#x2F;dev&#x2F;sdan，则需要关闭。\nsudo swapoff /dev/sdan\n\n然后删除分区\nCommand (m for help): dSelected partition 1Partition 1 has been deleted.Command (m for help): \n\n新建分区\nCommand (m for help): nPartition type   p   primary (0 primary, 0 extended, 4 free)   e   extended (container for logical partitions)Select (default p): pPartition number (1-4, default 1): First sector (2048-62914559, default 2048): Last sector, +/-sectors or +/-size&#123;K,M,G,T,P&#125; (2048-62914559, default 62914559): Created a new partition 1 of type &#x27;Linux&#x27; and of size 30 GiB.Partition #1 contains a ext4 signature.Do you want to remove the signature? [Y]es/[N]o: n\n\n打印看一下\nCommand (m for help): pDisk /dev/sda: 30 GiB, 32212254720 bytes, 62914560 sectorsDisk model: VMware Virtual SUnits: sectors of 1 * 512 = 512 bytesSector size (logical/physical): 512 bytes / 512 bytesI/O size (minimum/optimal): 512 bytes / 512 bytesDisklabel type: dosDisk identifier: 0xa4334a8dDevice     Boot Start      End  Sectors Size Id Type/dev/sda1        2048 62914559 62912512  30G 83 Linux\n\n确认无误\nCommand (m for help): aSelected partition 1The bootable flag on partition 1 is enabled now.\n\n写入\nCommand (m for help): wThe partition table has been altered.Syncing disks.\n\n重启虚拟机\nsudo reboot\n\n不要着急，这个时候查看磁盘，还是会显示20G的。\n执行扩容操作\nsudo resize2fs /dev/sda1[sudo] password for simon: resize2fs 1.44.6 (5-Mar-2019)Filesystem at /dev/sda1 is mounted on /; on-line resizing requiredold_desc_blocks = 3, new_desc_blocks = 4The filesystem on /dev/sda1 is now 7864064 (4k) blocks long.\n\n最后再次查看磁盘信息\ndf -hFilesystem      Size  Used Avail Use% Mounted onudev            3.9G     0  3.9G   0% /devtmpfs           796M  1.7M  794M   1% /run/dev/sda1        30G   18G   11G  63% /tmpfs           3.9G     0  3.9G   0% /dev/shmtmpfs           5.0M  4.0K  5.0M   1% /run/locktmpfs           3.9G     0  3.9G   0% /sys/fs/cgrouptmpfs           796M  1.2M  794M   1% /run/user/123tmpfs           796M     0  796M   0% /run/user/1000\n\n我们发现&#x2F;dev&#x2F;sda1已经有30G了，扩容成功。\n","categories":["OS","Ubuntu"],"tags":["Ubuntu","fdisk","VMWare"]},{"title":"无法挂载NFS共享目录到Docker容器的解决方法","url":"/2019/11/%E6%97%A0%E6%B3%95%E6%8C%82%E8%BD%BDNFS%E5%85%B1%E4%BA%AB%E7%9B%AE%E5%BD%95%E5%88%B0Docker%E5%AE%B9%E5%99%A8%E7%9A%84%E8%A7%A3%E5%86%B3%E6%96%B9%E6%B3%95/","content":"为了测试把NFS文件系统的共享目录挂载到本地，特意启动了Docker容器来做实验。\n执行命令时，必须加入--privileged这个参数，不然在Mount时会报permission denied的错误。\ndocker run -d --privileged &lt;image_id&gt;\n\n查看NFS的共享目录\nshowmount -e 172.17.0.2Export list for 172.17.0.2:/nfs/share *\n\n也可以到NFS服务器上查看/etc/exports文件的内容。\n执行挂载命令\nmount -t nfs 172.17.0.2:/nfs/share /mnt/nfs/\n\n会报如下错误：\nmount: wrong fs type, bad option, bad superblock on 172.17.0.2:/nfs/share,       missing codepage or helper program, or other error       (for several filesystems (e.g. nfs, cifs) you might       need a /sbin/mount.&lt;type&gt; helper program)       In some cases useful info is found in syslog - try       dmesg | tail or so.\n\n解决办法即是安装NFS的依赖包。\nyum install -y nfs-utils\n\n安装成功后，再次mount就可以成功了。\n","categories":["FileSystem","NFS"],"tags":["centos","docker","showmount","mount","nfs"]},{"title":"解决ubuntu上回环设备/dev/loop0占用100%的问题","url":"/2019/11/%E8%A7%A3%E5%86%B3ubuntu%E4%B8%8A%E5%9B%9E%E7%8E%AF%E8%AE%BE%E5%A4%87-dev-loop0%E5%8D%A0%E7%94%A8100-%E7%9A%84%E9%97%AE%E9%A2%98/","content":"当前ubuntu环境\ncat /etc/issueUbuntu 19.04 \\n \\l\n\n当我们查看文件系统占用情况时，发现有&#x2F;dev&#x2F;loop0至&#x2F;dev&#x2F;loop7占用率为100%。\ndf -hFilesystem      Size  Used Avail Use% Mounted onudev            3.9G     0  3.9G   0% /devtmpfs           796M  1.9M  794M   1% /run/dev/sda1        20G   18G  1.1G  95% /tmpfs           3.9G     0  3.9G   0% /dev/shmtmpfs           5.0M  4.0K  5.0M   1% /run/locktmpfs           3.9G     0  3.9G   0% /sys/fs/cgroup/dev/loop0       15M   15M     0 100% /snap/gnome-characters/254/dev/loop2       90M   90M     0 100% /snap/core/6673/dev/loop1      3.8M  3.8M     0 100% /snap/gnome-system-monitor/77/dev/loop3      152M  152M     0 100% /snap/gnome-3-28-1804/31/dev/loop4       36M   36M     0 100% /snap/gtk-common-themes/1198/dev/loop7      1.0M  1.0M     0 100% /snap/gnome-logs/61/dev/loop6       54M   54M     0 100% /snap/core18/941/dev/loop5      4.2M  4.2M     0 100% /snap/gnome-calculator/406tmpfs           796M   36K  795M   1% /run/user/1000\n\n&#x2F;dev&#x2F;loopn这些设备在Linux下被称为回环设备。\n可以通过执行losetup命令查看所有回环设备。\nlosetupNAME       SIZELIMIT OFFSET AUTOCLEAR RO BACK-FILE                                         DIO LOG-SEC/dev/loop1         0      0         1  1 /var/lib/snapd/snaps/gnome-system-monitor_77.snap   0     512/dev/loop6         0      0         1  1 /var/lib/snapd/snaps/core18_941.snap                0     512/dev/loop4         0      0         1  1 /var/lib/snapd/snaps/gtk-common-themes_1198.snap    0     512/dev/loop2         0      0         1  1 /var/lib/snapd/snaps/core_6673.snap                 0     512/dev/loop0         0      0         1  1 /var/lib/snapd/snaps/gnome-characters_254.snap      0     512/dev/loop7         0      0         1  1 /var/lib/snapd/snaps/gnome-logs_61.snap             0     512/dev/loop5         0      0         1  1 /var/lib/snapd/snaps/gnome-calculator_406.snap      0     512/dev/loop3         0      0         1  1 /var/lib/snapd/snaps/gnome-3-28-1804_31.snap        0     512\n\n我们可以把xxx.iso镜像文件挂载到一个目录下。\nmount -o loop xxx.iso /media\n\n等同于执行：\nlosetup /dev/loop0 xxx.isomount /dev/loop0 /media\n\n清理方法\nsudo apt autoremove --purge snapd[sudo] password for simon: Reading package lists... DoneBuilding dependency tree       Reading state information... DoneThe following additional packages will be installed:  gnome-software gnome-software-commonSuggested packages:  apt-config-icons-hidpi gnome-software-plugin-flatpakRecommended packages:  gnome-software-plugin-snapThe following packages will be REMOVED:  apg* gnome-control-center-faces* gnome-online-accounts* gnome-software-plugin-snap* libclutter-imcontext-0.1-0* libclutter-imcontext-0.1-bin*  libcolord-gtk1* libgsound0* libgtop-2.0-11* libgtop2-common* mobile-broadband-provider-info* network-manager-gnome* python3-macaroonbakery*  python3-protobuf* python3-rfc3339* python3-tz* snapd* ubuntu-system-service*The following packages will be upgraded:  gnome-software gnome-software-common2 upgraded, 0 newly installed, 18 to remove and 171 not upgraded.Need to get 2,982 kB of archives.After this operation, 73.7 MB disk space will be freed.Do you want to continue? [Y/n] y\n\n再次查看磁盘使用情况：\ndf -hFilesystem      Size  Used Avail Use% Mounted onudev            3.9G     0  3.9G   0% /devtmpfs           796M  1.9M  794M   1% /run/dev/sda1        20G   18G  1.6G  92% /tmpfs           3.9G     0  3.9G   0% /dev/shmtmpfs           5.0M  4.0K  5.0M   1% /run/locktmpfs           3.9G     0  3.9G   0% /sys/fs/cgrouptmpfs           796M   36K  795M   1% /run/user/1000\n","categories":["FileSystem"],"tags":["Ubuntu"]},{"title":"CentOS7安装json格式化工具jq","url":"/2019/12/CentOS7%E5%AE%89%E8%A3%85json%E6%A0%BC%E5%BC%8F%E5%8C%96%E5%B7%A5%E5%85%B7jq/","content":"cd /etc/yum.repos.d/wget http://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpmrpm -ivh epel-release-latest-7.noarch.rpmyum install jq...============================================================================================================================================== Package                            架构                            版本                                  源                             大小==============================================================================================================================================正在安装: jq                                 x86_64                          1.5-1.el7                             epel                          153 k为依赖而安装: oniguruma                          x86_64                          5.9.5-3.el7                           epel                          129 k...\n","categories":["OS","CentOS"],"tags":["centos","json"]},{"title":"Qihoo360 Wayne backend启动时报错cannot load github.com/modern-go/reflect2","url":"/2019/08/Qihoo360-Wayne-backend%E5%90%AF%E5%8A%A8%E6%97%B6%E6%8A%A5%E9%94%99cannot-load-github-com-modern-go-reflect2/","content":"Qihoo360 Wayne的文件目录为/Development/wayne。\n运行bee run命令时，会遇到如下错误。\ncd src/backendbee run -runargs=apiserver______| ___ \\| |_/ /  ___   ___| ___ \\ / _ \\ / _ \\| |_/ /|  __/|  __/\\____/  \\___| \\___| v1.10.02019/08/10 08:17:53 WARN     ▶ 0001 Running application outside of GOPATH2019/08/10 08:17:53 INFO     ▶ 0002 Using &#x27;backend&#x27; as &#x27;appname&#x27;2019/08/10 08:17:53 INFO     ▶ 0003 Initializing watcher...Fetching https://goproxy.io/github.com/modern-go/reflect2/@v/listFetching https://goproxy.io/sigs.k8s.io/yaml/@v/listFetching https://goproxy.io/sigs.k8s.io/@v/listFetching https://goproxy.io/github.com/modern-go/@v/listFetching https://goproxy.io/github.com/@v/listbuild github.com/Qihoo360/wayne/src/backend: cannot load github.com/modern-go/reflect2: cannot find module providing package github.com/modern-go/reflect22019/08/10 08:17:55 ERROR    ▶ 0004 Failed to build the application: build github.com/Qihoo360/wayne/src/backend: cannot load github.com/modern-go/reflect2: cannot find module providing package github.com/modern-go/reflect2\n\n解决方法，在终端先执行如下命令，以GO MODULE的方式加载包，即可以正常运行。\nexport GO111MODULE=on   // mac, linuxset $GO111MODULE=on     // windows\n\nbee run -runargs=apiserver______| ___ \\| |_/ /  ___   ___| ___ \\ / _ \\ / _ \\| |_/ /|  __/|  __/\\____/  \\___| \\___| v1.10.02019/08/10 08:19:26 WARN     ▶ 0001 Running application outside of GOPATH2019/08/10 08:19:26 INFO     ▶ 0002 Using &#x27;backend&#x27; as &#x27;appname&#x27;2019/08/10 08:19:26 INFO     ▶ 0003 Initializing watcher...Fetching https://goproxy.io/sigs.k8s.io/yaml/@v/listFetching https://goproxy.io/github.com/modern-go/reflect2/@v/list2019/08/10 08:19:35 SUCCESS  ▶ 0004 Built Successfully!2019/08/10 08:19:35 INFO     ▶ 0005 Restarting &#x27;backend&#x27;...2019/08/10 08:19:35 SUCCESS  ▶ 0006 &#x27;./backend&#x27; is running...2019/08/10 08:19:35.057 [D] [db.go:75]  Initialize database connection: ****:root@tcp(127.0.0.1:3306)/2019/08/10 08:19:35.076 [I] [asm_amd64.s:1337]  http server Running on http://:80802019/08/10 08:19:35.076 [I] [asm_amd64.s:1337]  Admin server Running on :8088\n\n如果有些包下载不了，还可以设置GOPROXY环境变量。\nexport GOPROXY=https://goproxy.io       // mac, linuxset $GOPROXY=on                         // windows\n","categories":["Kubernetes","Wayne"],"tags":["kubernetes","k8s","wayne"]},{"title":"Grafana匿名登录Kubernetes设置","url":"/2019/12/Grafana%E5%8C%BF%E5%90%8D%E7%99%BB%E5%BD%95Kubernetes%E8%AE%BE%E7%BD%AE/","content":"在Kubernetes中，通过yaml文件安装Grafana。要实现匿名登录查看Dashboard，则需要在&#96;spec.template.spec.containers[]下面添加如下内容：\nenv:- name: GF_AUTH_ANONYMOUS_ENABLED  value: &quot;true&quot;- name: GF_AUTH_ANONYMOUS_ORG_ROLE  value: Viewer\n\n部分内容如下：\nspec:  replicas: 1  selector:    matchLabels:      app: grafana  template:    metadata:      labels:        app: grafana    spec:      containers:      - image: grafana/grafana:6.3.3        name: grafana        ports:        - containerPort: 3000          name: http        readinessProbe:          httpGet:            path: /api/health            port: http        resources:          limits:            cpu: 200m            memory: 200Mi          requests:            cpu: 100m            memory: 100Mi        env:        - name: GF_AUTH_ANONYMOUS_ENABLED          value: &quot;true&quot;        - name: GF_AUTH_ANONYMOUS_ORG_ROLE          value: Viewer\n","categories":["Kubernetes","Grafana"],"tags":["kubernetes","k8s","grafana","monitor","监控"]},{"title":"K8S国内的镜像源","url":"/2019/12/K8S%E5%9B%BD%E5%86%85%E7%9A%84%E9%95%9C%E5%83%8F%E6%BA%90/","content":"2020-04 更新：\nAzure 近期将国内的公共镜像仓库代理 gcr.azk8s.cn 关闭了。gcr.azk8s.cn/google-containers 不能再用了。KubeSphere 之前在 gcr.azk8s.cn 的镜像都迁移到了KubeSphere 官方的 DockerHub。可以尝试使用 mirrorgooglecontainers，如果你需要的版本不存在，可以上 https://hub.docker.com 查找。\n\n安装K8S需要的一些镜像，在大陆不方便下载，这里提供几个国内的镜像。\n\n\n\n官方源\n大陆镜像\n\n\n\ndockerhub(docker.io)\ndockerhub.azk8s.cn\n\n\ngcr.io\ngcr.azk8s.cn\n\n\nk8s.gcr.io\ngcr.azk8s.cn&#x2F;google-containers\n\n\nquay.io\nquay.azk8s.cn\n\n\n","categories":["Kubernetes"],"tags":["kubernetes","k8s"]},{"title":"CentOS7开机后无IP4地址","url":"/2019/12/CentOS7%E5%BC%80%E6%9C%BA%E5%90%8E%E6%97%A0IP4%E5%9C%B0%E5%9D%80/","content":"我们在VMware或VirtualBox中启动CentOS7的虚拟机，登录后，查看网卡的IP4地址，发现ens33的IP地址并未显示出来。\nip a1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00    inet 127.0.0.1/8 scope host lo       valid_lft forever preferred_lft forever    inet6 ::1/128 scope host       valid_lft forever preferred_lft forever2: ens33: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000    link/ether 00:0c:29:d5:43:ca brd ff:ff:ff:ff:ff:ff\n\n这时，需要执行ifup ens33命令，让网卡连接起来。\n连接好以后，再次查看IP地址。\nip a1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00    inet 127.0.0.1/8 scope host lo       valid_lft forever preferred_lft forever    inet6 ::1/128 scope host       valid_lft forever preferred_lft forever2: ens33: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000    link/ether 00:0c:29:d5:43:ca brd ff:ff:ff:ff:ff:ff    inet 172.16.64.233/24 brd 172.16.64.255 scope global noprefixroute dynamic ens33       valid_lft 1778sec preferred_lft 1778sec    inet6 fe80::e8ec:c304:253f:44b7/64 scope link noprefixroute       valid_lft forever preferred_lft forever\n\n发现已经有IP地址了。\n如果想让重启后配置也能生效，还需要修改/etc/sysconfig/network-scripts/ifcfg-ens33网卡配置。\n把ONBOOT=no改为ONBOOT=yes\nsed -i s/ONBOOT=no/ONBOOT=yes/g /etc/sysconfig/network-scripts/ifcfg-ens33\n","categories":["OS","CentOS"],"tags":["centos","vmware"]},{"title":"Ubuntu环境搭建Go语言开发环境","url":"/2019/12/Ubuntu%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BAGo%E8%AF%AD%E8%A8%80%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83/","content":"下载Golang安装文件官方下载页面：https://golang.org/dl/\n也可以到国内下载地址下载：https://studygolang.com/dl\n我下载的版本是go1.12.14，下载到/usr/local/go1.12.14.linux-amd64.tar.gz\n解压sudo tar -zxvf go1.12.14.linux-amd64.tar.gz\n\n配置环境变量sudo vi /etc/bash.bashrc\n\n在文件末尾添加如下内容：\n# goexport GOROOT=/usr/local/go/ # golang installation pathexport GOPATH=/usr/local/development/gopath # go working pathexport GOBIN=$GOPATH/bin/export PATH=$GOBIN:$GOROOT/bin:$PATH\n\n使配置生效\nsource /etc/bash.bashrc\n\n安装成功，查看go版本信息\ngo versiongo version go1.12.14 linux/amd64\n\n安装intellij Go插件如果在线下载不了，可以进入https://plugins.jetbrains.com/plugin/9568-go下载，注意选择对应的版本。\n进入Configure -&gt; Settings -&gt; Plugings -&gt; Install Plugin from Disk…，选择刚才下载好的插件安装。\n安装成功后，如下图所示，重启IDEA。\n\n\n设置go语言sdkConfigure -&gt; Settings -&gt; Language &amp; Frameworks -&gt; Go\n\n\n设置GOPATH\n\n从本地，或者git上面导入工程代码。\n配置GO依赖代理如果在网络受限的情况下，可能是需要设置GO依赖代理的。\n# go proxyexport GO111MODULE=onexport GOPROXY=https://goproxy.io\n\n如果公司有启用go代理的话，则把GOPROXY设置成相应的地址就行了。\n当执行go build或go run时，依赖包会自动下载到$GOPATH/pkg/mod下面。\n设置git代理如果网络受限不能访问github.com，那还需要设置git代理。\ngit config --global url.&quot;http://&lt;server-address&gt;/&quot;.insteadOf &quot;git@github.com:&quot;\n\n设置好以后，查看结果：\ncat ~/.gitconfig[url &quot;http://&lt;server-address&gt;/&quot;]\tinsteadOf = git@github.com:\n\n或者\ngit config --global -lurl.http://&lt;server-address&gt;/.insteadof=git@github.com:\n","categories":["Golang"],"tags":["ubuntu","intellij idea","go","golang"]},{"title":"kubernetes集群卸载flannel网络","url":"/2019/12/kubernetes%E9%9B%86%E7%BE%A4%E5%8D%B8%E8%BD%BDflannel%E7%BD%91%E7%BB%9C/","content":"在生产环境中，最好还是要提前规划好网络。\n测试环境中，我们可以尝试以这种方式来卸载flannel网络。\n找到最初安装flannel时的yaml文件：\nkubectl delete -f kube-flannel.yml\n\n在所有master、worker节点上执行：\nip link delete cni0ip link delete flannel.1rm -rf /var/lib/cni/rm -f /etc/cni/net.d/*\n\n重启kubelet\nsystemctl restart kubelet\n","categories":["Kubernetes","Network"],"tags":["kubernetes","k8s","flannel","network"]},{"title":"Kubernetes监控方案kube-prometheus(prometheus, node-exporter, grafana)","url":"/2019/12/Kubernetes%E7%9B%91%E6%8E%A7%E6%96%B9%E6%A1%88kube-prometheus-prometheus-node-exporter-grafana/","content":"环境kubernetes: v1.15.6\nkube-prometheus: v0.2.0\n部署到github上克隆kube-prometheus: v0.2.0。\n进入yaml文件目录：\ncd kube-prometheus-0.2.0/manifests/\n\n新建目录，把yaml文件分门别类整理一下。\nmkdir -p operator node-exporter alertmanager grafana kube-state-metrics prometheus serviceMonitor adapter add-servicemv *-serviceMonitor* serviceMonitor/mv 0prometheus-operator* operator/mv grafana-* grafana/mv kube-state-metrics-* kube-state-metrics/mv alertmanager-* alertmanager/mv node-exporter-* node-exporter/mv prometheus-adapter* adapter/mv prometheus-* prometheus/\n\n整理好以后，目录结构是这样的：\ntree ..├── 00namespace-namespace.yaml├── adapter│   ├── prometheus-adapter-apiService.yaml│   ├── prometheus-adapter-clusterRoleAggregatedMetricsReader.yaml│   ├── prometheus-adapter-clusterRoleBindingDelegator.yaml│   ├── prometheus-adapter-clusterRoleBinding.yaml│   ├── prometheus-adapter-clusterRoleServerResources.yaml│   ├── prometheus-adapter-clusterRole.yaml│   ├── prometheus-adapter-configMap.yaml│   ├── prometheus-adapter-deployment.yaml│   ├── prometheus-adapter-roleBindingAuthReader.yaml│   ├── prometheus-adapter-serviceAccount.yaml│   └── prometheus-adapter-service.yaml├── add-service├── alertmanager│   ├── alertmanager-alertmanager.yaml│   ├── alertmanager-secret.yaml│   ├── alertmanager-serviceAccount.yaml│   └── alertmanager-service.yaml├── grafana│   ├── grafana-dashboardDatasources.yaml│   ├── grafana-dashboardDefinitions.yaml│   ├── grafana-dashboardSources.yaml│   ├── grafana-deployment.yaml│   ├── grafana-serviceAccount.yaml│   └── grafana-service.yaml├── kube-state-metrics│   ├── kube-state-metrics-clusterRoleBinding.yaml│   ├── kube-state-metrics-clusterRole.yaml│   ├── kube-state-metrics-deployment.yaml│   ├── kube-state-metrics-roleBinding.yaml│   ├── kube-state-metrics-role.yaml│   ├── kube-state-metrics-serviceAccount.yaml│   └── kube-state-metrics-service.yaml├── node-exporter│   ├── node-exporter-clusterRoleBinding.yaml│   ├── node-exporter-clusterRole.yaml│   ├── node-exporter-daemonset.yaml│   ├── node-exporter-serviceAccount.yaml│   └── node-exporter-service.yaml├── operator│   ├── 0prometheus-operator-0alertmanagerCustomResourceDefinition.yaml│   ├── 0prometheus-operator-0podmonitorCustomResourceDefinition.yaml│   ├── 0prometheus-operator-0prometheusCustomResourceDefinition.yaml│   ├── 0prometheus-operator-0prometheusruleCustomResourceDefinition.yaml│   ├── 0prometheus-operator-0servicemonitorCustomResourceDefinition.yaml│   ├── 0prometheus-operator-clusterRoleBinding.yaml│   ├── 0prometheus-operator-clusterRole.yaml│   ├── 0prometheus-operator-deployment.yaml│   ├── 0prometheus-operator-serviceAccount.yaml│   └── 0prometheus-operator-service.yaml├── prometheus│   ├── prometheus-clusterRoleBinding.yaml│   ├── prometheus-clusterRole.yaml│   ├── prometheus-prometheus.yaml│   ├── prometheus-roleBindingConfig.yaml│   ├── prometheus-roleBindingSpecificNamespaces.yaml│   ├── prometheus-roleConfig.yaml│   ├── prometheus-roleSpecificNamespaces.yaml│   ├── prometheus-rules.yaml│   ├── prometheus-serviceAccount.yaml│   └── prometheus-service.yaml└── serviceMonitor    ├── 0prometheus-operator-serviceMonitor.yaml    ├── alertmanager-serviceMonitor.yaml    ├── grafana-serviceMonitor.yaml    ├── kube-state-metrics-serviceMonitor.yaml    ├── node-exporter-serviceMonitor.yaml    ├── prometheus-serviceMonitorApiserver.yaml    ├── prometheus-serviceMonitorCoreDNS.yaml    ├── prometheus-serviceMonitorKubeControllerManager.yaml    ├── prometheus-serviceMonitorKubelet.yaml    ├── prometheus-serviceMonitorKubeScheduler.yaml    └── prometheus-serviceMonitor.yaml\n\n在alertmanager-service.yaml增加nodePort 30093\n在grafana-service.yaml增加nodePort 32000\n添加两行\napiVersion: v1kind: Servicemetadata:  labels:    app: grafana  name: grafana  namespace: monitoringspec:  type: NodePort  ports:  - name: http    port: 3000    targetPort: http    nodePort: 32000  selector:    app: grafana\n\n在prometheus-service.yaml增加nodePort 30090\n执行命令：\nkubectl apply -f .namespace/monitoring createdkubectl apply -f operator/customresourcedefinition.apiextensions.k8s.io/alertmanagers.monitoring.coreos.com createdcustomresourcedefinition.apiextensions.k8s.io/podmonitors.monitoring.coreos.com createdcustomresourcedefinition.apiextensions.k8s.io/prometheuses.monitoring.coreos.com createdcustomresourcedefinition.apiextensions.k8s.io/prometheusrules.monitoring.coreos.com createdcustomresourcedefinition.apiextensions.k8s.io/servicemonitors.monitoring.coreos.com createdclusterrole.rbac.authorization.k8s.io/prometheus-operator createdclusterrolebinding.rbac.authorization.k8s.io/prometheus-operator createddeployment.apps/prometheus-operator createdservice/prometheus-operator createdserviceaccount/prometheus-operator createdkubectl apply -f adapter/apiservice.apiregistration.k8s.io/v1beta1.metrics.k8s.io createdclusterrole.rbac.authorization.k8s.io/prometheus-adapter createdclusterrole.rbac.authorization.k8s.io/system:aggregated-metrics-reader createdclusterrolebinding.rbac.authorization.k8s.io/prometheus-adapter createdclusterrolebinding.rbac.authorization.k8s.io/resource-metrics:system:auth-delegator createdclusterrole.rbac.authorization.k8s.io/resource-metrics-server-resources createdconfigmap/adapter-config createddeployment.apps/prometheus-adapter createdrolebinding.rbac.authorization.k8s.io/resource-metrics-auth-reader createdservice/prometheus-adapter createdserviceaccount/prometheus-adapter createdkubectl apply -f alertmanager/alertmanager.monitoring.coreos.com/main createdsecret/alertmanager-main createdservice/alertmanager-main createdserviceaccount/alertmanager-main createdkubectl apply -f node-exporter/clusterrole.rbac.authorization.k8s.io/node-exporter createdclusterrolebinding.rbac.authorization.k8s.io/node-exporter createddaemonset.apps/node-exporter createdservice/node-exporter createdserviceaccount/node-exporter createdkubectl apply -f kube-state-metrics/clusterrole.rbac.authorization.k8s.io/kube-state-metrics createdclusterrolebinding.rbac.authorization.k8s.io/kube-state-metrics createddeployment.apps/kube-state-metrics createdrole.rbac.authorization.k8s.io/kube-state-metrics createdrolebinding.rbac.authorization.k8s.io/kube-state-metrics createdservice/kube-state-metrics createdserviceaccount/kube-state-metrics createdkubectl apply -f grafana/secret/grafana-datasources createdconfigmap/grafana-dashboard-apiserver createdconfigmap/grafana-dashboard-controller-manager createdconfigmap/grafana-dashboard-k8s-resources-cluster createdconfigmap/grafana-dashboard-k8s-resources-namespace createdconfigmap/grafana-dashboard-k8s-resources-pod createdconfigmap/grafana-dashboard-k8s-resources-workload createdconfigmap/grafana-dashboard-k8s-resources-workloads-namespace createdconfigmap/grafana-dashboard-kubelet createdconfigmap/grafana-dashboard-node-cluster-rsrc-use createdconfigmap/grafana-dashboard-node-rsrc-use createdconfigmap/grafana-dashboard-nodes createdconfigmap/grafana-dashboard-persistentvolumesusage createdconfigmap/grafana-dashboard-pods createdconfigmap/grafana-dashboard-prometheus-remote-write createdconfigmap/grafana-dashboard-prometheus createdconfigmap/grafana-dashboard-proxy createdconfigmap/grafana-dashboard-scheduler createdconfigmap/grafana-dashboard-statefulset createdconfigmap/grafana-dashboards createddeployment.apps/grafana createdservice/grafana createdserviceaccount/grafana createdkubectl apply -f prometheus/clusterrole.rbac.authorization.k8s.io/prometheus-k8s createdclusterrolebinding.rbac.authorization.k8s.io/prometheus-k8s createdprometheus.monitoring.coreos.com/k8s createdrolebinding.rbac.authorization.k8s.io/prometheus-k8s-config createdrolebinding.rbac.authorization.k8s.io/prometheus-k8s createdrolebinding.rbac.authorization.k8s.io/prometheus-k8s createdrolebinding.rbac.authorization.k8s.io/prometheus-k8s createdrole.rbac.authorization.k8s.io/prometheus-k8s-config createdrole.rbac.authorization.k8s.io/prometheus-k8s createdrole.rbac.authorization.k8s.io/prometheus-k8s createdrole.rbac.authorization.k8s.io/prometheus-k8s createdprometheusrule.monitoring.coreos.com/prometheus-k8s-rules createdservice/prometheus-k8s createdserviceaccount/prometheus-k8s createdkubectl apply -f serviceMonitor/servicemonitor.monitoring.coreos.com/prometheus-operator createdservicemonitor.monitoring.coreos.com/alertmanager createdservicemonitor.monitoring.coreos.com/grafana createdservicemonitor.monitoring.coreos.com/kube-state-metrics createdservicemonitor.monitoring.coreos.com/node-exporter createdservicemonitor.monitoring.coreos.com/prometheus createdservicemonitor.monitoring.coreos.com/kube-apiserver createdservicemonitor.monitoring.coreos.com/coredns createdservicemonitor.monitoring.coreos.com/kube-controller-manager createdservicemonitor.monitoring.coreos.com/kube-scheduler createdservicemonitor.monitoring.coreos.com/kubelet created\n\n查看状态\nkubectl -n monitoring get allNAME                                      READY   STATUS    RESTARTS   AGEpod/alertmanager-main-0                   2/2     Running   0          24hpod/alertmanager-main-1                   2/2     Running   4          24hpod/alertmanager-main-2                   2/2     Running   0          24hpod/grafana-57bfdd47f8-k9wcd              1/1     Running   0          9hpod/kube-state-metrics-65d5b4b99d-gwfb8   4/4     Running   0          9hpod/node-exporter-jmhff                   2/2     Running   0          9hpod/node-exporter-ld2sg                   2/2     Running   0          9hpod/node-exporter-qqmzw                   2/2     Running   0          9hpod/prometheus-adapter-668748ddbd-h6x2w   1/1     Running   0          25hpod/prometheus-k8s-0                      3/3     Running   3          8hpod/prometheus-k8s-1                      3/3     Running   2          8hpod/prometheus-operator-55b978b89-lptfq   1/1     Running   0          26hNAME                            TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)                      AGEservice/alertmanager-main       NodePort    10.104.249.82   &lt;none&gt;        9093:30093/TCP               24hservice/alertmanager-operated   ClusterIP   None            &lt;none&gt;        9093/TCP,9094/TCP,9094/UDP   24hservice/grafana                 NodePort    10.99.157.162   &lt;none&gt;        3000:32000/TCP               9hservice/kube-state-metrics      ClusterIP   None            &lt;none&gt;        8443/TCP,9443/TCP            9hservice/node-exporter           ClusterIP   None            &lt;none&gt;        9100/TCP                     10hservice/prometheus-adapter      ClusterIP   10.105.30.68    &lt;none&gt;        443/TCP                      25hservice/prometheus-k8s          NodePort    10.102.127.30   &lt;none&gt;        9090:30090/TCP               8hservice/prometheus-operated     ClusterIP   None            &lt;none&gt;        9090/TCP                     8hservice/prometheus-operator     ClusterIP   None            &lt;none&gt;        8080/TCP                     26hNAME                           DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR            AGEdaemonset.apps/node-exporter   3         3         3       3            3           kubernetes.io/os=linux   10hNAME                                  READY   UP-TO-DATE   AVAILABLE   AGEdeployment.apps/grafana               1/1     1            1           9hdeployment.apps/kube-state-metrics    1/1     1            1           9hdeployment.apps/prometheus-adapter    1/1     1            1           25hdeployment.apps/prometheus-operator   1/1     1            1           26hNAME                                            DESIRED   CURRENT   READY   AGEreplicaset.apps/grafana-57bfdd47f8              1         1         1       9hreplicaset.apps/kube-state-metrics-65d5b4b99d   1         1         1       9hreplicaset.apps/kube-state-metrics-77467ddf9b   0         0         0       9hreplicaset.apps/prometheus-adapter-668748ddbd   1         1         1       25hreplicaset.apps/prometheus-operator-55b978b89   1         1         1       26hNAME                                 READY   AGEstatefulset.apps/alertmanager-main   3/3     24hstatefulset.apps/prometheus-k8s      2/2     8h\n\n\n\n我们发现有两个targets下面没有内容，分别是kube-controller-manager和kube-scheduler。\n查看其ServiceMonitor资源的定义，内容如下：\ncat prometheus-serviceMonitorKubeControllerManager.yaml\n\napiVersion: monitoring.coreos.com/v1kind: ServiceMonitormetadata:  labels:    k8s-app: kube-controller-manager  name: kube-controller-manager  namespace: monitoringspec:  endpoints:  - interval: 30s    metricRelabelings:    - action: drop      regex: etcd_(debugging|disk|request|server).*      sourceLabels:      - __name__    port: http-metrics  jobLabel: k8s-app  namespaceSelector:    matchNames:    - kube-system  selector:    matchLabels:      k8s-app: kube-controller-manager\n\n上面是一个典型的ServiceMonitor资源文件的声明方式，上面我们通过selector .matchLabels在kube-system这个命名空间下面匹配具有k8s-app=kube-controller-manager这样的Service，但是我们系统中根本就没有对应的Service。\n所以，在add-service目录下面创建Service文件prometheus-kubeControllerManagerService.yaml\napiVersion: v1kind: Servicemetadata:  namespace: kube-system  name: kube-controller-manager  labels:    k8s-app: kube-controller-managerspec:  selector:    component: kube-controller-manager  ports:  - name: http-metrics    port: 10252    targetPort: 10252    protocol: TCP\n\nprometheus-kubeControllerManagerService.yaml文件也可以写成这样：\napiVersion: v1kind: Servicemetadata:  namespace: kube-system  name: kube-controller-manager  labels:    k8s-app: kube-controller-managerspec:  selector:    component: kube-controller-manager  type: ClusterIP  clusterIP: None  ports:  - name: http-metrics    port: 10252    targetPort: 10252    protocol: TCP---apiVersion: v1kind: Endpointsmetadata:  labels:    k8s-app: kube-controller-manager  name: kube-controller-manager  namespace: kube-systemsubsets:- addresses:  - ip: 172.16.64.233  # - ip: 10.0.0.15  # - ip: 10.0.0.20  ports:  - name: http-metrics    port: 10252    protocol: TCP\n\n同理，在add-service目录下面创建文件prometheus-kubeSchedulerService.yaml\napiVersion: v1kind: Servicemetadata:  namespace: kube-system  name: kube-scheduler  labels:    # matches the serviceMonitor selector    k8s-app: kube-schedulerspec:  selector:    # matches scheduler&#x27;s pod label    component: kube-scheduler  ports:  - name: http-metrics    port: 10251    targetPort: 10251    protocol: TCP\n\nprometheus-kubeSchedulerService.yaml也可以写成这样：\napiVersion: v1kind: Servicemetadata:  namespace: kube-system  name: kube-scheduler  labels:    k8s-app: kube-schedulerspec:  type: ClusterIP  clusterIP: None  ports:  - name: port    port: 10251    protocol: TCP---apiVersion: v1kind: Endpointsmetadata:  labels:    k8s-app: kube-scheduler  name: kube-scheduler  namespace: kube-systemsubsets:- addresses:  - ip: 172.16.64.233  # - ip: 10.0.0.15  # - ip: 10.0.0.20  ports:  - name: http-metrics    port: 10251    protocol: TCP\n\nkubectl apply -f add-service/\n\n这时，两个targets下面就会有内容产生了。\nPrometheus Configuration我们浏览Prometheus Dashboard上的Configuration页面，会有个疑问，这些配置内容是从哪里来的呢？\nprometheus-operator和直接部署prometheus区别是operator把prometheus, alertmanager server 的配置, 还有scape config, record&#x2F;alert rule 包装成了k8s中的CRD。\nkubectl get crd | grep  monitoringalertmanagers.monitoring.coreos.com           2019-12-29T06:39:49Zpodmonitors.monitoring.coreos.com             2019-12-29T06:39:49Zprometheuses.monitoring.coreos.com            2019-12-29T06:39:49Zprometheusrules.monitoring.coreos.com         2019-12-29T06:39:49Zservicemonitors.monitoring.coreos.com         2019-12-29T06:39:49Z\n\n修改CRD之后，operator监控到CRD的修改，生成一份prometheus的配置文件，gzip压缩后存成k8s Secret。\nkubectl get secret -n monitoring prometheus-k8s -o json | jq -r &#x27;.data.&quot;prometheus.yaml.gz&quot;&#x27; | base64 -d | gzip -d\n\njson文件内容是：\n&#123;    &quot;apiVersion&quot;: &quot;v1&quot;,    &quot;data&quot;: &#123;        &quot;prometheus.yaml.gz&quot;: &quot;xxxxxxx&quot;    &#125;,    ...&#125;\n\n我们需要取出data.&quot;prometheus.yaml.gz&quot;的值，再做base64解密和gzip还原。\n同理可得alertmanager的配置内容：\nkubectl get secret -n monitoring alertmanager-main -o json | jq -r &#x27;.data.&quot;alertmanager.yaml&quot;&#x27; | base64 -d\n\n&quot;global&quot;:  &quot;resolve_timeout&quot;: &quot;5m&quot;&quot;receivers&quot;:- &quot;name&quot;: &quot;null&quot;&quot;route&quot;:  &quot;group_by&quot;:  - &quot;job&quot;  &quot;group_interval&quot;: &quot;5m&quot;  &quot;group_wait&quot;: &quot;30s&quot;  &quot;receiver&quot;: &quot;null&quot;  &quot;repeat_interval&quot;: &quot;12h&quot;  &quot;routes&quot;:  - &quot;match&quot;:      &quot;alertname&quot;: &quot;Watchdog&quot;    &quot;receiver&quot;: &quot;null&quot;\n\nprometheus和alertmanager的pod中都有监控configmap&#x2F;secrets的container，如果挂载的配置文件发生了修改(k8s同步configmap需要1分钟左右)，会让server进程reload配置文件。\n","categories":["Kubernetes","Prometheus"],"tags":["kubernetes","grafana","kube-prometheus","prometheus","node-exporter"]},{"title":"在Prometheus Operator中添加自定义监控项，监控etcd集群","url":"/2019/12/%E5%9C%A8Prometheus-Operator%E4%B8%AD%E6%B7%BB%E5%8A%A0%E8%87%AA%E5%AE%9A%E4%B9%89%E7%9B%91%E6%8E%A7%E9%A1%B9%EF%BC%8C%E7%9B%91%E6%8E%A7etcd%E9%9B%86%E7%BE%A4/","content":"添加自定义监控项流程\n创建ServiceMonitor对象\n创建Service对象，提供metrics数据接口，并将其和ServiceMonitor关联\n确保Service对象可以正确获取metrics数据\n\n\n\n配置etcd证书查看etcd启动时的证书路径\nkubectl get po -n kube-system...etcd-k8s-master                            1/1     Running   1          6h28m...\n\nkubectl get po etcd-k8s-master -n kube-system -o yaml...spec:  containers:  - command:    - etcd    - --advertise-client-urls=https://192.168.229.134:2379    - --cert-file=/etc/kubernetes/pki/etcd/server.crt    - --client-cert-auth=true    - --data-dir=/var/lib/etcd    - --initial-advertise-peer-urls=https://192.168.229.134:2380    - --initial-cluster=k8s-master=https://192.168.229.134:2380    - --key-file=/etc/kubernetes/pki/etcd/server.key    - --listen-client-urls=https://127.0.0.1:2379,https://192.168.229.134:2379    - --listen-peer-urls=https://192.168.229.134:2380    - --name=k8s-master    - --peer-cert-file=/etc/kubernetes/pki/etcd/peer.crt    - --peer-client-cert-auth=true    - --peer-key-file=/etc/kubernetes/pki/etcd/peer.key    - --peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt    - --snapshot-count=10000    - --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt    image: k8s.gcr.io/etcd:3.3.10...\n\n可以看出etcd使用的证书都在对应节点的/etc/kubernetes/pki/etcd/目录下面。所以先将需要使用的证书通过secret对象保存到集群中：\nkubectl -n monitoring create secret generic etcd-certs \\--from-file=/etc/kubernetes/pki/etcd/healthcheck-client.crt \\--from-file=/etc/kubernetes/pki/etcd/healthcheck-client.key \\--from-file=/etc/kubernetes/pki/etcd/ca.crt secret/etcd-certs created\n\n将创建etcd-certs对象配置到prometheus资源对象中，直接更新：\nkubectl edit prometheus k8s -n monitoring\n\n添加secrets的如下属性：\nnodeSelector:  kubernetes.io/os: linuxpodMonitorSelector: &#123;&#125;replicas: 2# 添加如下两行secrets:- etcd-certs\n\n更新完成后，就可以在Prometheus的Pod中获取之前创建的etcd证书文件了。先查看一下pod名字。\nkubectl get po -n monitoring NAME                                  READY   STATUS    RESTARTS   AGE...prometheus-k8s-0                      3/3     Running   1          2m20sprometheus-k8s-1                      3/3     Running   1          3m19s...\n\n进入两个容器，查看一下证书的具体路径：\nkubectl exec -it prometheus-k8s-0 /bin/sh -n monitoringDefaulting container name to prometheus.Use &#x27;kubectl describe pod/prometheus-k8s-0 -n monitoring&#x27; to see all of the containers in this pod./prometheus $ ls /etc/prometheus/secrets/etcd-certs/ca.crt                  healthcheck-client.crt  healthcheck-client.key\n\n创建ServiceMonitor创建prometheus-serviceMonitorEtcd.yaml文件：\napiVersion: monitoring.coreos.com/v1kind: ServiceMonitormetadata:  labels:    k8s-app: etcd-k8s  name: etcd-k8s  namespace: monitoringspec:  endpoints:  - port: port    interval: 30s    scheme: https    tlsConfig:      caFile: /etc/prometheus/secrets/etcd-certs/ca.crt      certFile: /etc/prometheus/secrets/etcd-certs/healthcheck-client.crt      keyFile: /etc/prometheus/secrets/etcd-certs/healthcheck-client.key      insecureSkipVerify: true  jobLabel: k8s-app  namespaceSelector:    matchNames:    - kube-system  selector:    matchLabels:      k8s-app: etcd\n\n创建这个serviceMonitor对象：\nkubectl apply -f prometheus-serviceMonitorEtcd.yaml servicemonitor.monitoring.coreos.com/etcd-k8s created\n\n创建ServiceServiceMonitor已经创建完成了，需要创建一个对应的Service对象。prometheus-etcdService.yaml内容如下：\napiVersion: v1kind: Servicemetadata:  labels:    k8s-app: etcd  name: etcd-k8s  namespace: kube-systemspec:  ports:  - name: port    port: 2379    protocol: TCP  type: ClusterIP  clusterIP: None---apiVersion: v1kind: Endpointsmetadata:  name: etcd-k8s  namespace: kube-system  labels:    k8s-app: etcdsubsets:- addresses:  - ip: 192.168.229.134    nodeName: etcd-master#  - ip: 192.168.229.135#    nodeName: etcd02#  - ip: 192.168.229.136#    nodeName: etcd03  ports:  - name: port    port: 2379    protocol: TCP\n\n\n\netcd集群独立于集群之外，所以需要定义一个Endpoints。Endpoints的metadata区域的内容要和Service保持一致，并且将Service的clusterIP设置为None。\n在Endpoints的subsets中填写etcd的地址，如果是集群，则在addresses属性下面添加多个地址。\n创建Service, Endpoints资源：\nkubectl apply -f prometheus-etcdService.yaml service/etcd-k8s createdendpoints/etcd-k8s created\n\n\n\n采集到数据以后，在Grafana中导入编号为3070的Dashboard。\n","categories":["Kubernetes","Prometheus"],"tags":["kubernetes","k8s","prometheus"]},{"title":"VMware非正常关闭后启动，提示该虚拟机似乎正在使用中","url":"/2019/12/VMware%E9%9D%9E%E6%AD%A3%E5%B8%B8%E5%85%B3%E9%97%AD%E5%90%8E%E5%90%AF%E5%8A%A8%EF%BC%8C%E6%8F%90%E7%A4%BA%E8%AF%A5%E8%99%9A%E6%8B%9F%E6%9C%BA%E4%BC%BC%E4%B9%8E%E6%AD%A3%E5%9C%A8%E4%BD%BF%E7%94%A8%E4%B8%AD/","content":"VMware非正常关闭后启动，提示”该虚拟机似乎正在使用中“\n\n\n点击”获取所有权“按钮\n\n\n获取所有权失败，说明该虚拟机是非常正常关闭导致的，点击”取消”按钮。\n进入虚拟机存放目录，删除以.lck结尾的目录。\n\n\n删除以后，则可以正常开机了。\n","categories":["Tools","VMware"],"tags":["vmware"]},{"title":"解决Failed to load module canberra-gtk-module错误","url":"/2019/12/%E8%A7%A3%E5%86%B3Failed-to-load-module-canberra-gtk-module%E9%94%99%E8%AF%AF/","content":"在Ubuntu环境里，运行程序时，报错：\n\n\nGtk-Message: 09:10:26.571: Failed to load module &quot;canberra-gtk-module&quot;\n\n查看一下模块位置\nlocate libcanberra-gtk-module/usr/lib/x86_64-linux-gnu/gtk-3.0/modules/libcanberra-gtk-module.so\n\n发现已经安装了，却加载不了。\n重新安装一下吧。\nsudo apt-get install libcanberra-gtk-module[sudo] password for simon: Reading package lists... DoneBuilding dependency tree       Reading state information... DoneThe following additional packages will be installed:  libcanberra-gtk0The following NEW packages will be installed:  libcanberra-gtk-module libcanberra-gtk00 upgraded, 2 newly installed, 0 to remove and 175 not upgraded.Need to get 17.8 kB of archives.After this operation, 92.2 kB of additional disk space will be used.Do you want to continue? [Y/n] y\n\n安装成功，再次启动程序，就不会报错了。\n","categories":["OS","Ubuntu"],"tags":["Ubuntu","canberra-gtk-module"]},{"title":"解决Google Chrome浏览器无法打开Kubernetes(K8S) Dashboard页面","url":"/2019/12/%E8%A7%A3%E5%86%B3Google-Chrome%E6%B5%8F%E8%A7%88%E5%99%A8%E6%97%A0%E6%B3%95%E6%89%93%E5%BC%80Kubernetes-K8S-Dashboard%E9%A1%B5%E9%9D%A2/","content":"环境k8s version: v.1.16.3\ndashboard: dashboard:v2.0.0-beta6\n问题描述K8S Dashboard安装好以后，通过Firefox浏览器是可以打开的，但通过Google Chrome浏览器，无法成功浏览页面。如图：\n\n\n解决方案kubeadm自动生成的证书，很多浏览器不支持。所以我们需要自己创建证书。\n创建一个目录存放证书等文件\nmkdir keycd key\n\n生成证书openssl genrsa -out dashboard.key 2048Generating RSA private key, 2048 bit long modulus.................+++......+++e is 65537 (0x10001)# 172.16.64.229为master节点的IP地址openssl req -new -out dashboard.csr -key dashboard.key -subj &#x27;/CN=172.16.64.229&#x27;openssl x509 -req -in dashboard.csr -signkey dashboard.key -out dashboard.crtSignature oksubject=/CN=172.16.64.229Getting Private key\n\n删除原有证书注意新版的Dashboard的namespace已经变为kubernetes-dashboard了\nkubectl delete secret kubernetes-dashboard-certs -n kubernetes-dashboardsecret &quot;kubernetes-dashboard-certs&quot; deleted\n\n创建新证书的secretkubectl create secret generic kubernetes-dashboard-certs --from-file=dashboard.key --from-file=dashboard.crt -n kubernetes-dashboardsecret/kubernetes-dashboard-certs created\n\n查找正在运行的podkubectl get pod -n kubernetes-dashboardNAME                                         READY   STATUS    RESTARTS   AGEdashboard-metrics-scraper-76585494d8-dzgt9   1/1     Running   0          8m20skubernetes-dashboard-b65488c4-rcdjh          1/1     Running   0          8m20s\n\n删除podkubectl delete po kubernetes-dashboard-b65488c4-rcdjh -n kubernetes-dashboardpod &quot;kubernetes-dashboard-b65488c4-rcdjh&quot; deletedkubectl delete po dashboard-metrics-scraper-76585494d8-dzgt9 -n kubernetes-dashboardpod &quot;dashboard-metrics-scraper-76585494d8-dzgt9&quot; deleted\n\n如果pod比较多的时候，可以使用以下这条命令批量删除。\nkubectl get pod -n kubernetes-dashboard | grep -v NAME | awk &#x27;&#123;print &quot;kubectl delete po &quot; $1 &quot; -n kubernetes-dashboard&quot;&#125;&#x27; | sh\n\n删除后，新的pod会自动启动起来。\n这时，再次刷新Chrome浏览器的Dashboard页面，如图：\n\n\n点击继续前往，页面就可以正常显示了。\n","categories":["Kubernetes","Dashboard"],"tags":["kubernetes","k8s"]},{"title":"通过kubeadm离线安装k8s集群v1.15","url":"/2019/12/%E9%80%9A%E8%BF%87kubeadm%E7%A6%BB%E7%BA%BF%E5%AE%89%E8%A3%85k8s%E9%9B%86%E7%BE%A4v1-15/","content":"通过kubeadm离线安装k8s集群v1.15。最近有网友提到 gcr.azk8s.cn 被关闭的问题，解决方案请查阅 K8S国内的镜像源\n安装说明这篇文章将描述在生产环境中，如何搭建k8s集群。\n为什么使用kubeadm来安装kubeadm是官方社区推出的一个用于快速部署kubernetes集群的工具。这个工具能通过两条指令快速完成一个kubernetes集群的部署。\n网上很多人说通过二进制安装能了解到配置的细节，其实通过kubeadm安装也能查看到配置的细节。\n可以自动生成证书，对初学者带来了不少便利。\n网络环境我们完全模拟生产环境中，不可以访问外部互联网的情况。\n基础的yum源是有提供的，像什么docker-ce、kubernetes的源是没有的。\nk8s.gcr.io、quay.io这些域名也是不可以访问的。\n准备环境如果没有特殊提及，安装及操作需要在所有master及node节点上执行。\n机器网络及配置复制三台虚拟机。\n\n\n\n主机名\nIP\n节点类型\n最低配置\n\n\n\nk8s-master\n172.16.64.233\nmaster节点\nCPU 2Core, Memory 1GB\n\n\nk8s-node1\n172.16.64.232\nnode节点\nCPU 1Core, Memory 1GB\n\n\nk8s-node2\n172.16.64.235\nnode节点\nCPU 1Core, Memory 1GB\n\n\nmaster节点需要至少2个CPU，不然会报如错误：\nerror execution phase preflight: [preflight] Some fatal errors occurred:\t[ERROR NumCPU]: the number of available CPUs 1 is less than the required 2\n\n关闭防火墙systemctl stop firewalldsystemctl disable firewalldRemoved symlink /etc/systemd/system/multi-user.target.wants/firewalld.service.Removed symlink /etc/systemd/system/dbus-org.fedoraproject.FirewallD1.service.\n\n关闭selinux把SELINUX=enforcing替换成SELINUX=disabled\nsed -i s/SELINUX=enforcing/SELINUX=disabled/g /etc/selinux/configsetenforce 0\n\n查看一下selinux的状态。\ngetenforcePermissive\n\n关闭Swapswapoff -acp /etc/fstab /etc/fstab_bakcat /etc/fstab_bak | grep -v swap &gt; /etc/fstab\n\ngrep -v swap是查找不包含swap的行。\n查看一下swap的情况，Swap已经全部为0了。\nfree -m              total        used        free      shared  buff/cache   availableMem:            972         142         715           7         114         699Swap:             0           0           0\n\n设置主机名在master节点上设置主机名。\nhostnamectl set-hostname k8s-master\n\n在node1节点上设置主机名。\nhostnamectl set-hostname k8s-node1\n\n在node2节点上设置主机名。\nhostnamectl set-hostname k8s-node2\n\n在master上查看主机名。\nhostnamek8s-master\n\n设置hosts&gt;&gt;表示文件末尾追加记录。\ncat &gt;&gt; /etc/hosts &lt;&lt;EOF172.16.64.233   k8s-master172.16.64.232   k8s-node1172.16.64.235   k8s-node2EOF\n\n修改sysctl.conf暂时未修改，装docker的时候会自动修改。可以暂时先跳过这一步。\n如果未修改成功，在执行docker info命令时，会显示如下提示信息。\nWARNING: bridge-nf-call-iptables is disabledWARNING: bridge-nf-call-ip6tables is disabled\n\ncat /proc/sys/net/bridge/bridge-nf-call-iptables0cat /proc/sys/net/bridge/bridge-nf-call-ip6tables0\n\n可通过以下方法来做修改。\n# 修改 /etc/sysctl.conf# 如果有配置，则修改sed -i &quot;s#^net.ipv4.ip_forward.*#net.ipv4.ip_forward=1#g&quot;  /etc/sysctl.confsed -i &quot;s#^net.bridge.bridge-nf-call-ip6tables.*#net.bridge.bridge-nf-call-ip6tables=1#g&quot;  /etc/sysctl.confsed -i &quot;s#^net.bridge.bridge-nf-call-iptables.*#net.bridge.bridge-nf-call-iptables=1#g&quot;  /etc/sysctl.conf# 可能没有，追加echo &quot;net.ipv4.ip_forward = 1&quot; &gt;&gt; /etc/sysctl.confecho &quot;net.bridge.bridge-nf-call-ip6tables = 1&quot; &gt;&gt; /etc/sysctl.confecho &quot;net.bridge.bridge-nf-call-iptables = 1&quot; &gt;&gt; /etc/sysctl.confsysctl -p\n\n也就是在/etc/sysctl.conf末尾加上如下内容：\nnet.ipv4.ip_forward = 1net.bridge.bridge-nf-call-ip6tables = 1net.bridge.bridge-nf-call-iptables = 1\n\n同时让配置生效sysctl -p\n安装Docker下载docker由于我们在生产环境中是没法连接互联网的，所以要提前准备好docker rpm包。\n我们在另一台可以联网的机器上下载安装所需的软件。\n添加docker yum源在联网的机器上，下载docker\n配置docker-ce源\ncd /etc/yum.repos.d/wget https://download.docker.com/linux/centos/docker-ce.repo\n\n查看docker所有版本yum list docker-ce --showduplicates...docker-ce.x86_64                            18.06.3.ce-3.el7                                   docker-ce-stabledocker-ce.x86_64                            3:18.09.0-3.el7                                    docker-ce-stable...\n\n我们选择安装docker-ce.18.06.3.ce-3.el7\n下载yum install --downloadonly --downloaddir ~/k8s/docker docker-ce-18.06.3.ce-3.el7\n\ndocker及其依赖会下载到~&#x2F;docker文件夹中。\n我们可以看到只有docker-ce是来自docker-ce-stable源的。\n=============================================================================================================== Package                          架构             版本                       源                          大小===============================================================================================================正在安装: docker-ce                        x86_64           18.06.3.ce-3.el7           docker-ce-stable            41 M为依赖而安装: audit-libs-python                x86_64           2.8.5-4.el7                base                        76 k checkpolicy                      x86_64           2.5-8.el7                  base                       295 k container-selinux                noarch           2:2.107-3.el7              extras                      39 k libcgroup                        x86_64           0.41-21.el7                base                        66 k libseccomp                       x86_64           2.3.1-3.el7                base                        56 k libsemanage-python               x86_64           2.5-14.el7                 base                       113 k libtool-ltdl                     x86_64           2.4.2-22.el7_3             base                        49 k policycoreutils-python           x86_64           2.5-33.el7                 base                       457 k python-IPy                       noarch           0.75-6.el7                 base                        32 k setools-libs                     x86_64           3.3.8-4.el7                base                       620 k\n\n所以，我们只需要把docker-ce-18.06.3.ce-3.el7.x86_64.rpm拷贝到master及node节点里面。\n在master及node节点里创建~/k8s/docker目录，用于存放docker安装rpm包。\nmkdir -p ~/k8s/docker\n\n拷贝到k8s集群通过scp命令拷贝。\nscp docker-ce-18.06.3.ce-3.el7.x86_64.rpm root@172.16.64.233:~/k8s/docker/scp docker-ce-18.06.3.ce-3.el7.x86_64.rpm root@172.16.64.232:~/k8s/docker/scp docker-ce-18.06.3.ce-3.el7.x86_64.rpm root@172.16.64.235:~/k8s/docker/\n\n安装Dockeryum本地安装\nyum install k8s/docker/docker-ce-18.06.3.ce-3.el7.x86_64.rpm\n\n设置开机启动\nsystemctl enable dockerCreated symlink from /etc/systemd/system/multi-user.target.wants/docker.service to /usr/lib/systemd/system/docker.service.\n\n我们可以查看一下安装包到底生成了哪些文件。\nrpm -ql docker-ce\n\n或者\nrpm -qpl k8s/docker/docker-ce-18.06.3.ce-3.el7.x86_64.rpm\n\n启动Dockersystemctl start docker\n\n查看docker服务信息。\ndocker info...Cgroup Driver: cgroupfs...\n\n呆会儿我们还需要修改这个值。\n安装k8s组件由于kubeadm是依赖kubelet, kubectl的，所以我们只需要下载kubeadm的rpm，其依赖就自动下载下来了。但是版本可能不是我们想要的，所以可能需要单独下载。比如我下载kubeadm-1.15.6，它依赖的可能是kubelet-1.16.x。\n下载k8s组件我们需要安装kubeadm, kubelet, kubectl，版本需要一致。在可以连外网的机器上下载组件，同上面docker。\n添加kubernetes yum源cat &gt; /etc/yum.repos.d/kubernetes.repo &lt;&lt;EOF[kubernetes]name=Kubernetes Repobaseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/gpgcheck=1gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpgenabled=1EOF\n\n查看kubeadm版本yum list kubeadm --showduplicateskubeadm.x86_64                                                         1.15.6-0                                                           kubernetes\n\n下载下载kubeadm-1.15.6\nyum install --downloadonly --downloaddir ~/k8s/kubernetes kubeadm-1.15.6\n\n根据如下依赖关系\n==================================================================================================================================================== Package                                    架构                       版本                                    源                              大小====================================================================================================================================================正在安装: kubeadm                                    x86_64                     1.15.6-0                                kubernetes                     8.9 M为依赖而安装: conntrack-tools                            x86_64                     1.4.4-5.el7_7.2                         updates                        187 k cri-tools                                  x86_64                     1.13.0-0                                kubernetes                     5.1 M kubectl                                    x86_64                     1.16.3-0                                kubernetes                      10 M kubelet                                    x86_64                     1.16.3-0                                kubernetes                      22 M kubernetes-cni                             x86_64                     0.7.5-0                                 kubernetes                      10 M libnetfilter_cthelper                      x86_64                     1.0.0-10.el7_7.1                        updates                         18 k libnetfilter_cttimeout                     x86_64                     1.0.0-6.el7_7.1                         updates                         18 k libnetfilter_queue                         x86_64                     1.0.2-2.el7_2                           base                            23 k socat                                      x86_64                     1.7.3.2-2.el7                           base                           290 k\n\n我们只需要把来自kubernetes源的kubeadm和4个依赖cri-tools, kubectl, kubelet和kubernetes-cni拷贝到master和node节点。\n下载kubelet-1.15.6\nyum install --downloadonly --downloaddir ~/k8s/kubernetes kubelet-1.15.6\n\n下载kubectl-1.15.6\nyum install --downloadonly --downloaddir ~/k8s/kubernetes kubectl-1.15.6\n拷贝到k8s集群在master及node节点里创建~/k8s/kubernetes目录，用于存放k8s组件安装的rpm包。\nmkdir -p ~/k8s/kubernetes\n\nkubeadm\n\n\ncri-tools\nscp 14bfe6e75a9efc8eca3f638eb22c7e2ce759c67f95b43b16fae4ebabde1549f3-cri-tools-1.13.0-0.x86_64.rpm root@172.16.64.233:~/k8s/kubernetes/scp 14bfe6e75a9efc8eca3f638eb22c7e2ce759c67f95b43b16fae4ebabde1549f3-cri-tools-1.13.0-0.x86_64.rpm root@172.16.64.232:~/k8s/kubernetes/scp 14bfe6e75a9efc8eca3f638eb22c7e2ce759c67f95b43b16fae4ebabde1549f3-cri-tools-1.13.0-0.x86_64.rpm root@172.16.64.235:~/k8s/kubernetes/\n\nkubectl\nscp 5181c2b7eee876b8ce205f0eca87db2b3d00ffd46d541882620cb05b738d7a80-kubectl-1.15.6-0.x86_64.rpm root@172.16.64.233:~/k8s/kubernetes/scp 5181c2b7eee876b8ce205f0eca87db2b3d00ffd46d541882620cb05b738d7a80-kubectl-1.15.6-0.x86_64.rpm root@172.16.64.232:~/k8s/kubernetes/scp 5181c2b7eee876b8ce205f0eca87db2b3d00ffd46d541882620cb05b738d7a80-kubectl-1.15.6-0.x86_64.rpm root@172.16.64.235:~/k8s/kubernetes/\n\nkubelet\nscp e9e7cc53edd19d0ceb654d1bde95ec79f89d26de91d33af425ffe8464582b36e-kubelet-1.15.6-0.x86_64.rpm root@172.16.64.233:~/k8s/kubernetes/scp e9e7cc53edd19d0ceb654d1bde95ec79f89d26de91d33af425ffe8464582b36e-kubelet-1.15.6-0.x86_64.rpm root@172.16.64.232:~/k8s/kubernetes/scp e9e7cc53edd19d0ceb654d1bde95ec79f89d26de91d33af425ffe8464582b36e-kubelet-1.15.6-0.x86_64.rpm root@172.16.64.235:~/k8s/kubernetes/\n\nkubernetes-cni\nscp 548a0dcd865c16a50980420ddfa5fbccb8b59621179798e6dc905c9bf8af3b34-kubernetes-cni-0.7.5-0.x86_64.rpm root@172.16.64.235:~/k8s/kubernetes/\n\n安装k8s组件yum install ~/k8s/kubernetes/*.rpm\n\n这样，kubeadm, kubectl, kubelet就已经安装好了。\n设置kubelet的开机启动。我们并不需要启动kubelet，就算启动，也是不能成功的。执行kubeadm命令，会生成一些配置文件 ，这时才会让kubelet启动成功的。\nsystemctl enable kubeletCreated symlink from /etc/systemd/system/multi-user.target.wants/kubelet.service to /usr/lib/systemd/system/kubelet.service.\n\n拉取镜像执行kubeadm时，需要用到一些镜像，我们需要提前准备。\n查看需要依赖哪些镜像kubeadm config images listW1207 18:53:23.129020   10255 version.go:98] could not fetch a Kubernetes version from the internet: unable to get URL &quot;https://dl.k8s.io/release/stable-1.txt&quot;: Get https://dl.k8s.io/release/stable-1.txt: net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)W1207 18:53:23.129433   10255 version.go:99] falling back to the local client version: v1.15.6k8s.gcr.io/kube-apiserver:v1.15.6k8s.gcr.io/kube-controller-manager:v1.15.6k8s.gcr.io/kube-scheduler:v1.15.6k8s.gcr.io/kube-proxy:v1.15.6k8s.gcr.io/pause:3.1k8s.gcr.io/etcd:3.3.10k8s.gcr.io/coredns:1.3.1\n\n在生产环境，是肯定访问不了k8s.gcr.io这个地址的。在有大陆联网的机器上，也是无法访问的。所以我们需要使用国内镜像先下载下来。\n镜像地址，请参考 K8S国内的镜像源\n拉取镜像在三台机器上拉取如下镜像。\ndocker pull gcr.azk8s.cn/google-containers/kube-apiserver:v1.15.6docker pull gcr.azk8s.cn/google-containers/kube-controller-manager:v1.15.6docker pull gcr.azk8s.cn/google-containers/kube-scheduler:v1.15.6docker pull gcr.azk8s.cn/google-containers/kube-proxy:v1.15.6docker pull gcr.azk8s.cn/google-containers/pause:3.1docker pull gcr.azk8s.cn/google-containers/etcd:3.3.10docker pull gcr.azk8s.cn/google-containers/coredns:1.3.1\n\n查看拉取镜像。\ndocker imagesREPOSITORY                                               TAG                 IMAGE ID            CREATED             SIZEgcr.azk8s.cn/google-containers/kube-proxy                v1.15.6             d756327a2327        3 weeks ago         82.4MBgcr.azk8s.cn/google-containers/kube-apiserver            v1.15.6             9f612b9e9bbf        3 weeks ago         207MBgcr.azk8s.cn/google-containers/kube-controller-manager   v1.15.6             83ab61bd43ad        3 weeks ago         159MBgcr.azk8s.cn/google-containers/kube-scheduler            v1.15.6             502e54938456        3 weeks ago         81.1MBgcr.azk8s.cn/google-containers/coredns                   1.3.1               eb516548c180        10 months ago       40.3MBgcr.azk8s.cn/google-containers/etcd                      3.3.10              2c4adeb21b4f        12 months ago       258MBgcr.azk8s.cn/google-containers/pause                     3.1                 da86e6ba6ca1        23 months ago       742kB\n\ntag镜像为了让kubeadm程序能找到k8s.gcr.io下面的镜像，需要把刚才下载的镜像名称重新打一下tag。\ndocker images | grep gcr.azk8s.cn/google-containers | sed &#x27;s/gcr.azk8s.cn\\/google-containers/k8s.gcr.io/&#x27; | awk &#x27;&#123;print &quot;docker tag &quot; $3 &quot; &quot; $1 &quot;:&quot; $2&#125;&#x27; | sh\n\n删除旧的镜像，当然，你留着也不会占用太多空间。\ndocker images | grep gcr.azk8s.cn/google-containers | awk &#x27;&#123;print &quot;docker rmi &quot; $1 &quot;:&quot; $2&#125;&#x27; | sh\n\n查看镜像docker imagesREPOSITORY                           TAG                 IMAGE ID            CREATED             SIZEk8s.gcr.io/kube-proxy                v1.15.6             d756327a2327        3 weeks ago         82.4MBk8s.gcr.io/kube-apiserver            v1.15.6             9f612b9e9bbf        3 weeks ago         207MBk8s.gcr.io/kube-controller-manager   v1.15.6             83ab61bd43ad        3 weeks ago         159MBk8s.gcr.io/kube-scheduler            v1.15.6             502e54938456        3 weeks ago         81.1MBk8s.gcr.io/coredns                   1.3.1               eb516548c180        10 months ago       40.3MBk8s.gcr.io/etcd                      3.3.10              2c4adeb21b4f        12 months ago       258MBk8s.gcr.io/pause                     3.1                 da86e6ba6ca1        23 months ago       742kB\n\n镜像搞定了。\n部署k8s集群初始化master节点在master节点上执行kubeadm init命令。\n我们接下来首先会使用flannel网络，所以参数中必须设置--pod-network-cidr=10.244.0.0/16，这个IP地址是固定的。\nkubeadm init \\--apiserver-advertise-address=172.16.64.233 \\--pod-network-cidr=10.244.0.0/16W1207 21:18:48.257967   10859 version.go:98] could not fetch a Kubernetes version from the internet: unable to get URL &quot;https://dl.k8s.io/release/stable-1.txt&quot;: Get https://dl.k8s.io/release/stable-1.txt: net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)W1207 21:18:48.258448   10859 version.go:99] falling back to the local client version: v1.15.6[init] Using Kubernetes version: v1.15.6[preflight] Running pre-flight checks\t[WARNING IsDockerSystemdCheck]: detected &quot;cgroupfs&quot; as the Docker cgroup driver. The recommended driver is &quot;systemd&quot;. Please follow the guide at https://kubernetes.io/docs/setup/cri/[preflight] Pulling images required for setting up a Kubernetes cluster[preflight] This might take a minute or two, depending on the speed of your internet connection[preflight] You can also perform this action in beforehand using &#x27;kubeadm config images pull&#x27;[kubelet-start] Writing kubelet environment file with flags to file &quot;/var/lib/kubelet/kubeadm-flags.env&quot;[kubelet-start] Writing kubelet configuration to file &quot;/var/lib/kubelet/config.yaml&quot;[kubelet-start] Activating the kubelet service[certs] Using certificateDir folder &quot;/etc/kubernetes/pki&quot;[certs] Generating &quot;ca&quot; certificate and key[certs] Generating &quot;apiserver&quot; certificate and key[certs] apiserver serving cert is signed for DNS names [k8s-master kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 172.16.64.233][certs] Generating &quot;apiserver-kubelet-client&quot; certificate and key[certs] Generating &quot;etcd/ca&quot; certificate and key[certs] Generating &quot;etcd/peer&quot; certificate and key[certs] etcd/peer serving cert is signed for DNS names [k8s-master localhost] and IPs [172.16.64.233 127.0.0.1 ::1][certs] Generating &quot;etcd/healthcheck-client&quot; certificate and key[certs] Generating &quot;etcd/server&quot; certificate and key[certs] etcd/server serving cert is signed for DNS names [k8s-master localhost] and IPs [172.16.64.233 127.0.0.1 ::1][certs] Generating &quot;apiserver-etcd-client&quot; certificate and key[certs] Generating &quot;front-proxy-ca&quot; certificate and key[certs] Generating &quot;front-proxy-client&quot; certificate and key[certs] Generating &quot;sa&quot; key and public key[kubeconfig] Using kubeconfig folder &quot;/etc/kubernetes&quot;[kubeconfig] Writing &quot;admin.conf&quot; kubeconfig file[kubeconfig] Writing &quot;kubelet.conf&quot; kubeconfig file[kubeconfig] Writing &quot;controller-manager.conf&quot; kubeconfig file[kubeconfig] Writing &quot;scheduler.conf&quot; kubeconfig file[control-plane] Using manifest folder &quot;/etc/kubernetes/manifests&quot;[control-plane] Creating static Pod manifest for &quot;kube-apiserver&quot;[control-plane] Creating static Pod manifest for &quot;kube-controller-manager&quot;[control-plane] Creating static Pod manifest for &quot;kube-scheduler&quot;[etcd] Creating static Pod manifest for local etcd in &quot;/etc/kubernetes/manifests&quot;[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory &quot;/etc/kubernetes/manifests&quot;. This can take up to 4m0s[apiclient] All control plane components are healthy after 36.504799 seconds[upload-config] Storing the configuration used in ConfigMap &quot;kubeadm-config&quot; in the &quot;kube-system&quot; Namespace[kubelet] Creating a ConfigMap &quot;kubelet-config-1.15&quot; in namespace kube-system with the configuration for the kubelets in the cluster[upload-certs] Skipping phase. Please see --upload-certs[mark-control-plane] Marking the node k8s-master as control-plane by adding the label &quot;node-role.kubernetes.io/master=&#x27;&#x27;&quot;[mark-control-plane] Marking the node k8s-master as control-plane by adding the taints [node-role.kubernetes.io/master:NoSchedule][bootstrap-token] Using token: 1czxp7.4tt0x3lxdcus8wer[bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles[bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials[bootstrap-token] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token[bootstrap-token] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster[bootstrap-token] Creating the &quot;cluster-info&quot; ConfigMap in the &quot;kube-public&quot; namespace[addons] Applied essential addon: CoreDNS[addons] Applied essential addon: kube-proxyYour Kubernetes control-plane has initialized successfully!To start using your cluster, you need to run the following as a regular user:  mkdir -p $HOME/.kube  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config  sudo chown $(id -u):$(id -g) $HOME/.kube/configYou should now deploy a pod network to the cluster.Run &quot;kubectl apply -f [podnetwork].yaml&quot; with one of the options listed at:  https://kubernetes.io/docs/concepts/cluster-administration/addons/Then you can join any number of worker nodes by running the following on each as root:kubeadm join 172.16.64.233:6443 --token 1czxp7.4tt0x3lxdcus8wer \\    --discovery-token-ca-cert-hash sha256:abb676401e56a48f07675ff802f1abedd512cce0523190b2e0f636ee6d70d8b4\n\n解决WARNING我们看到上面的消息中有一句\n[WARNING IsDockerSystemdCheck]: detected &quot;cgroupfs&quot; as the Docker cgroup driver. The recommended driver is &quot;systemd&quot;. Please follow the guide at https://kubernetes.io/docs/setup/cri/\n\n还记得前面我在查看docker info时，有提到要修改cgroup driver么？现在就来修改吧。\n修改或创建/etc/docker/daemon.json，添加如下内容：\n&#123;\t&quot;exec-opts&quot;: [&quot;native.cgroupdriver=systemd&quot;]&#125;\n\n重启docker\nsystemctl restart docker\n\n查看修改结果，如果Cgroup Driver改为systemd后就表示成功了。\ndocker info...Cgroup Driver: systemd...\n\n重置\nkubeadm reset[reset] Reading configuration from the cluster...[reset] FYI: You can look at this config file with &#x27;kubectl -n kube-system get cm kubeadm-config -oyaml&#x27;W1207 22:12:18.285935   27649 reset.go:98] [reset] Unable to fetch the kubeadm-config ConfigMap from cluster: failed to get config map: Get https://172.16.64.233:6443/api/v1/namespaces/kube-system/configmaps/kubeadm-config: dial tcp 172.16.64.233:6443: connect: connection refused[reset] WARNING: Changes made to this host by &#x27;kubeadm init&#x27; or &#x27;kubeadm join&#x27; will be reverted.[reset] Are you sure you want to proceed? [y/N]: y[preflight] Running pre-flight checksW1207 22:12:19.569005   27649 removeetcdmember.go:79] [reset] No kubeadm config, using etcd pod spec to get data directory[reset] Stopping the kubelet service[reset] Unmounting mounted directories in &quot;/var/lib/kubelet&quot;[reset] Deleting contents of config directories: [/etc/kubernetes/manifests /etc/kubernetes/pki][reset] Deleting files: [/etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/bootstrap-kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf][reset] Deleting contents of stateful directories: [/var/lib/etcd /var/lib/kubelet /etc/cni/net.d /var/lib/dockershim /var/run/kubernetes]The reset process does not reset or clean up iptables rules or IPVS tables.If you wish to reset iptables, you must do so manually.For example:iptables -F &amp;&amp; iptables -t nat -F &amp;&amp; iptables -t mangle -F &amp;&amp; iptables -XIf your cluster was setup to utilize IPVS, run ipvsadm --clear (or similar)to reset your system&#x27;s IPVS tables.The reset process does not clean your kubeconfig files and you must remove them manually.Please, check the contents of the $HOME/.kube/config file.\n\n再次初始化Master节点apiserver-advertise-address和pod-network-cidr参数都可以省略掉。\nkubeadm init \\--apiserver-advertise-address=172.16.64.233 \\--pod-network-cidr=10.244.0.0/16...Your Kubernetes control-plane has initialized successfully!To start using your cluster, you need to run the following as a regular user:  mkdir -p $HOME/.kube  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config  sudo chown $(id -u):$(id -g) $HOME/.kube/configYou should now deploy a pod network to the cluster.Run &quot;kubectl apply -f [podnetwork].yaml&quot; with one of the options listed at:  https://kubernetes.io/docs/concepts/cluster-administration/addons/Then you can join any number of worker nodes by running the following on each as root:kubeadm join 172.16.64.233:6443 --token duof19.l9q3dsh4ccen4ya0 \\    --discovery-token-ca-cert-hash sha256:66fad0ed5f46f5ea9a394276a77db16d26d60465ec9930f8c21aa924a5df9bb5\n\n提示信息和上面初始化时的信息一样，只是少了刚才的WARNING。\n按照信息提示，执行如下命令，目前登录的就是root用户，所以也不需要用sudo了。\nmkdir -p $HOME/.kubesudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/configsudo chown $(id -u):$(id -g) $HOME/.kube/config\n\n查看节点信息，节点状态为NotReady:\nkubectl get noNAME         STATUS     ROLES    AGE     VERSIONk8s-master   NotReady   master   2m22s   v1.15.6\n\n往集群里面加入node节点在节点node1上，按上面的提示执行命令：\nkubeadm join 172.16.64.233:6443 --token duof19.l9q3dsh4ccen4ya0 \\    --discovery-token-ca-cert-hash sha256:66fad0ed5f46f5ea9a394276a77db16d26d60465ec9930f8c21aa924a5df9bb5    [preflight] Running pre-flight checks[preflight] Reading configuration from the cluster...[preflight] FYI: You can look at this config file with &#x27;kubectl -n kube-system get cm kubeadm-config -oyaml&#x27;[kubelet-start] Downloading configuration for the kubelet from the &quot;kubelet-config-1.15&quot; ConfigMap in the kube-system namespace[kubelet-start] Writing kubelet configuration to file &quot;/var/lib/kubelet/config.yaml&quot;[kubelet-start] Writing kubelet environment file with flags to file &quot;/var/lib/kubelet/kubeadm-flags.env&quot;[kubelet-start] Activating the kubelet service[kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap...This node has joined the cluster:* Certificate signing request was sent to apiserver and a response was received.* The Kubelet was informed of the new secure connection details.Run &#x27;kubectl get nodes&#x27; on the control-plane to see this node join the cluster.    \n\n在Master节点上（control-plane)上查看节点信息\nkubectl get noNAME         STATUS     ROLES    AGE   VERSIONk8s-master   NotReady   master   7m    v1.15.6k8s-node1    NotReady   &lt;none&gt;   65s   v1.15.6\n\n我们看到了多了一个节点，虽然现在都是NotReady状态。\nToken过期后再加入节点过了一段时间后，再加入节点，这个时候会提示token已经过期了。我们可以这样拿到token和hash值。\nkubeadm token createkubeadm token listopenssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outform der 2&gt;/dev/null | openssl dgst -sha256 -hex | sed &#x27;s/^.* //&#x27;\n\n安装Network插件这里我们先安装flannel网络插件。\n查看安装方法查看flannel的官网https://github.com/coreos/flannel，找到安装方法。\nFor Kubernetes v1.7+ kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml\n\n下载yml文件在有网络的机器上下载kube-flannel.yml文件\nwget https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml\n\n把下载好的yml文件分发到k8s集群的三台机器里面。\n下载镜像cat kube-flannel.yml | grep image        image: quay.io/coreos/flannel:v0.11.0-amd64        ...\n\n还记得前面方法么？不记得就回到上面再看看吧。\ndocker pull quay.azk8s.cn/coreos/flannel:v0.11.0-amd64docker tag ff281650a721 quay.io/coreos/flannel:v0.11.0-amd64docker rmi quay.azk8s.cn/coreos/flannel:v0.11.0-amd64\n\n安装flannel我们也可以选择安装Calico网络插件。\n在Master节点执行：\nkubectl apply -f kube-flannel.ymlpodsecuritypolicy.policy/psp.flannel.unprivileged createdclusterrole.rbac.authorization.k8s.io/flannel createdclusterrolebinding.rbac.authorization.k8s.io/flannel createdserviceaccount/flannel createdconfigmap/kube-flannel-cfg createddaemonset.apps/kube-flannel-ds-amd64 createddaemonset.apps/kube-flannel-ds-arm64 createddaemonset.apps/kube-flannel-ds-arm createddaemonset.apps/kube-flannel-ds-ppc64le createddaemonset.apps/kube-flannel-ds-s390x created\n\n查看节点信息kubectl get noNAME         STATUS   ROLES    AGE   VERSIONk8s-master   Ready    master   33m   v1.15.6k8s-node1    Ready    &lt;none&gt;   28m   v1.15.6k8s-node2    Ready    &lt;none&gt;   17m   v1.15.6\n\n这一下所有节点都已经ready了。\n查看进程Master节点ps -ef | grep kuberoot       1652      1  3 15:13 ?        00:00:04 /usr/bin/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf --config=/var/lib/kubelet/config.yaml --cgroup-driver=systemd --network-plugin=cni --pod-infra-container-image=k8s.gcr.io/pause:3.1root       1973   1909  8 15:13 ?        00:00:12 kube-apiserver --advertise-address=172.16.64.233 --allow-privileged=true --authorization-mode=Node,RBAC --client-ca-file=/etc/kubernetes/pki/ca.crt --enable-admission-plugins=NodeRestriction --enable-bootstrap-token-auth=true --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key --etcd-servers=https://127.0.0.1:2379 --insecure-port=0 --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key --requestheader-allowed-names=front-proxy-client --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt --requestheader-extra-headers-prefix=X-Remote-Extra- --requestheader-group-headers=X-Remote-Group --requestheader-username-headers=X-Remote-User --secure-port=6443 --service-account-key-file=/etc/kubernetes/pki/sa.pub --service-cluster-ip-range=10.96.0.0/12 --tls-cert-file=/etc/kubernetes/pki/apiserver.crt --tls-private-key-file=/etc/kubernetes/pki/apiserver.keyroot       1980   1915  1 15:13 ?        00:00:01 kube-scheduler --bind-address=127.0.0.1 --kubeconfig=/etc/kubernetes/scheduler.conf --leader-elect=trueroot       1995   1936  2 15:13 ?        00:00:03 etcd --advertise-client-urls=https://172.16.64.233:2379 --cert-file=/etc/kubernetes/pki/etcd/server.crt --client-cert-auth=true --data-dir=/var/lib/etcd --initial-advertise-peer-urls=https://172.16.64.233:2380 --initial-cluster=k8s-master=https://172.16.64.233:2380 --key-file=/etc/kubernetes/pki/etcd/server.key --listen-client-urls=https://127.0.0.1:2379,https://172.16.64.233:2379 --listen-peer-urls=https://172.16.64.233:2380 --name=k8s-master --peer-cert-file=/etc/kubernetes/pki/etcd/peer.crt --peer-client-cert-auth=true --peer-key-file=/etc/kubernetes/pki/etcd/peer.key --peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt --snapshot-count=10000 --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crtroot       2002   1951  2 15:13 ?        00:00:03 kube-controller-manager --authentication-kubeconfig=/etc/kubernetes/controller-manager.conf --authorization-kubeconfig=/etc/kubernetes/controller-manager.conf --bind-address=127.0.0.1 --client-ca-file=/etc/kubernetes/pki/ca.crt --cluster-signing-cert-file=/etc/kubernetes/pki/ca.crt --cluster-signing-key-file=/etc/kubernetes/pki/ca.key --controllers=*,bootstrapsigner,tokencleaner --kubeconfig=/etc/kubernetes/controller-manager.conf --leader-elect=true --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt --root-ca-file=/etc/kubernetes/pki/ca.crt --service-account-private-key-file=/etc/kubernetes/pki/sa.key --use-service-account-credentials=trueroot       2263   2244  0 15:13 ?        00:00:00 /usr/local/bin/kube-proxy --config=/var/lib/kube-proxy/config.conf --hostname-override=k8s-masterroot       3907   3888  0 15:14 ?        00:00:00 /usr/bin/kube-controllers\n\nWorker节点ps -ef | grep kuberoot       1355      1  1 15:05 ?        00:00:06 /usr/bin/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf --config=/var/lib/kubelet/config.yaml --cgroup-driver=systemd --network-plugin=cni --pod-infra-container-image=k8s.gcr.io/pause:3.1root       1669   1620  0 15:13 ?        00:00:00 /usr/local/bin/kube-proxy --config=/var/lib/kube-proxy/config.conf --hostname-override=k8s-node1\n\n测试k8s集群安装一个nginx。\n创建一个部署(deployment)在master节点（Control Plane）安装一个叫nginx-deployment的deployment：\nkubectl create deploy nginx-deployment --image=nginxdeployment.apps/nginx-deployment created\n\n查看deployment状态kubectl get deployNAME               READY   UP-TO-DATE   AVAILABLE   AGEnginx-deployment   0/1     1            0           44s\n\n发现没有READY。\n查看pod状态kubectl get poNAME                                READY   STATUS              RESTARTS   AGEnginx-deployment-6f77f65499-htdps   0/1     ContainerCreating   0          111s\n也是没有READY。继续查看详细信息。\nkubectl describe po nginx-deployment-6f77f65499-htdps...Events:  Type    Reason     Age    From                Message  ----    ------     ----   ----                -------  Normal  Scheduled  2m18s  default-scheduler   Successfully assigned default/nginx-deployment-6f77f65499-htdps to k8s-node1  Normal  Pulling    2m17s  kubelet, k8s-node1  Pulling image &quot;nginx&quot;\n\n初步判断应该是拉取镜像拉不下来，或者速度非常慢。\n配置docker源在生产环境，肯定是有内部的镜像源的，在这里，我就模拟把源配置为阿里的镜像源了。\n/etc/docker/daemon.json内容如下：\n&#123;\t&quot;exec-opts&quot;: [&quot;native.cgroupdriver=systemd&quot;],\t&quot;registry-mirrors&quot;: [&quot;http://hub-mirror.c.163.com&quot;]&#125;\n\n重启docker\nsystemctl restart docker\n\n这个时候，镜像就容易拉取了。\n测试pod再次查看deploy, pod，状态已经变为READY了。\nkubectl get po -o wideNAME                                READY   STATUS    RESTARTS   AGE   IP           NODE        NOMINATED NODE   READINESS GATESnginx-deployment-6f77f65499-htdps   1/1     Running   0          26m   10.244.1.3   k8s-node1   &lt;none&gt;           &lt;none&gt;\n\n我们看到pod的IP为10.244.1.3。还记得我们在初始化master节点时设置的参数--pod-network-cidr=10.244.0.0/16么？\n在集群内的三个节点访问nginx，能成功访问。\ncurl 10.244.1.3...&lt;body&gt;&lt;h1&gt;Welcome to nginx!&lt;/h1&gt;&lt;p&gt;If you see this page, the nginx web server is successfully installed andworking. Further configuration is required.&lt;/p&gt;...\n\n创建Service我们把deployment暴露出来。\nkubectl expose deploy nginx-deployment --port=80 --type=NodePortservice/nginx-deployment exposed\n\n查看状态\nkubectl get svcNAME               TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGEkubernetes         ClusterIP   10.96.0.1       &lt;none&gt;        443/TCP        71mnginx-deployment   NodePort    10.109.145.67   &lt;none&gt;        80:32538/TCP   32s\n\n在三个节点内访问nginx\ncurl 10.109.145.67...&lt;body&gt;&lt;h1&gt;Welcome to nginx!&lt;/h1&gt;&lt;p&gt;If you see this page, the nginx web server is successfully installed andworking. Further configuration is required.&lt;/p&gt;...\n\n在集群外访问nginx\ncurl 172.16.64.233:32538curl 172.16.64.232:32538curl 172.16.64.235:32538...&lt;body&gt;&lt;h1&gt;Welcome to nginx!&lt;/h1&gt;&lt;p&gt;If you see this page, the nginx web server is successfully installed andworking. Further configuration is required.&lt;/p&gt;...\n\n至此，一个k8s集群在生产环境的模拟安装，就结束了。\n","categories":["Kubernetes"],"tags":["centos","docker","kubernetes","k8s"]},{"title":"部署Kubernetes Dashboard","url":"/2019/12/%E9%83%A8%E7%BD%B2Kubernetes-Dashboard/","content":"在K8S v1.15.6环境下部署Kubernetes Dashboard。\n查看官网先查看Dashboard的版本，进入官网：https://github.com/kubernetes/dashboard。\nTo deploy Dashboard, execute following command:$ kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v1.10.1/src/deploy/recommended/kubernetes-dashboard.yaml\n\n我们在Releases页面，v1.10.1版本是不支持K8S v1.15的。就算安装上以后页面也是打不开的。\nCompatibility\n\n\n\nKubernetes version\n1.11\n1.12\n1.13\n1.14\n1.15\n\n\n\nCompatibility\n?\n?\n?\n?\n✓\n\n\n所以我们选择v2.0.0-beta4。\n下载yml文件wget https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.0-beta4/aio/deploy/recommended.yaml -O kubernetes-dashboard.yaml\n\n编辑/root/k8s/dashboard/kubernetes-dashboard.yaml文件，在名为kubernetes-dashboard的Service这块，加两行。\n增加type: NodePort和nodePort: 30001，最终内容如下：\nkind: ServiceapiVersion: v1metadata:  labels:    k8s-app: kubernetes-dashboard  name: kubernetes-dashboard  namespace: kubernetes-dashboardspec:  type: NodePort  ports:    - port: 443      targetPort: 8443      noePort: 30001  selector:    k8s-app: kubernetes-dashboard\n\n安装kubernetes dashboardkubectl apply -f kubernetes-dashboard.yamlnamespace/kubernetes-dashboard unchangedserviceaccount/kubernetes-dashboard unchangedservice/kubernetes-dashboard createdsecret/kubernetes-dashboard-certs createdsecret/kubernetes-dashboard-csrf createdsecret/kubernetes-dashboard-key-holder createdconfigmap/kubernetes-dashboard-settings createdrole.rbac.authorization.k8s.io/kubernetes-dashboard createdclusterrole.rbac.authorization.k8s.io/kubernetes-dashboard createdrolebinding.rbac.authorization.k8s.io/kubernetes-dashboard createdclusterrolebinding.rbac.authorization.k8s.io/kubernetes-dashboard createddeployment.apps/kubernetes-dashboard createdservice/dashboard-metrics-scraper createddeployment.apps/dashboard-metrics-scraper created\n\n查看pod状态kubectl get po -n kubernetes-dashboardNAME                                        READY   STATUS    RESTARTS   AGEdashboard-metrics-scraper-fb986f88d-n2fkv   1/1     Running   0          115skubernetes-dashboard-6bb65fcc49-bm676       1/1     Running   0          115s\n\n验证这时，使用Firefox浏览器，通过https://&lt;NodeIP&gt;:30001就可以访问到页面了。\n如果使用的是IE或Chrome浏览器，还是打不开的。解决方案请参考：解决Google Chrome浏览器无法打开Kubernetes(K8S) Dashboard页面\n创建Dashboard管理员创建一个ServiceAccountkubectl create serviceaccount dashboard-admin -n kubernetes-dashboard\n\n查看一下详情\nkubectl describe sa dashboard-admin -n kubernetes-dashboardName:                dashboard-adminNamespace:           kubernetes-dashboardLabels:              &lt;none&gt;Annotations:         &lt;none&gt;Image pull secrets:  &lt;none&gt;Mountable secrets:   dashboard-admin-token-trt79Tokens:              dashboard-admin-token-trt79Events:              &lt;none&gt;\n\ndashboard-admin-token-trt79将成为Secret的名字。\n创建Clusterrolebinding我们可以先看看有哪些clusterrole\nkubectl get clusterroleNAME                                                                   AGEadmin                                                                  18hcluster-admin                                                          18hedit                                                                   18hflannel                                                                17hkubernetes-dashboard                                                   15hsystem:aggregate-to-admin                                              18hsystem:aggregate-to-edit                                               18hsystem:aggregate-to-view                                               18hsystem:auth-delegator                                                  18hsystem:basic-user                                                      18hsystem:certificates.k8s.io:certificatesigningrequests:nodeclient       18h...\n\n绑定\nkubectl create clusterrolebinding dashboard-admin --clusterrole=cluster-admin --serviceaccount=kubernetes-dashboard:dashboard-admin\n\n查看一下绑定关系\nkubectl describe  clusterrolebinding dashboard-adminName:         dashboard-adminLabels:       &lt;none&gt;Annotations:  &lt;none&gt;Role:  Kind:  ClusterRole  Name:  cluster-adminSubjects:  Kind            Name             Namespace  ----            ----             ---------  ServiceAccount  dashboard-admin  kubernetes-dashboard\n\n获取登录Tokenkubectl describe secret -n kubernetes-dashboard $(kubectl get secrets -n kubernetes-dashboard | awk &#x27;/dashboard-admin/&#123;print $1&#125;&#x27; )Name:         dashboard-admin-token-trt79Namespace:    kubernetes-dashboardLabels:       &lt;none&gt;Annotations:  kubernetes.io/service-account.name: dashboard-admin              kubernetes.io/service-account.uid: 084970f9-48cd-44db-8106-887ee76db771Type:  kubernetes.io/service-account-tokenData====ca.crt:     1025 bytesnamespace:  20 bytestoken:      eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlcm5ldGVzLWRhc2hib2FyZCIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJkYXNoYm9hcmQtYWRtaW4tdG9rZW4tdHJ0NzkiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC5uYW1lIjoiZGFzaGJvYXJkLWFkbWluIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQudWlkIjoiMDg0OTcwZjktNDhjZC00NGRiLTgxMDYtODg3ZWU3NmRiNzcxIiwic3ViIjoic3lzdGVtOnNlcnZpY2VhY2NvdW50Omt1YmVybmV0ZXMtZGFzaGJvYXJkOmRhc2hib2FyZC1hZG1pbiJ9.m1twrtEAUCzup9vc0xVJwL3lOKwa4Pizyj4iMNoUctHKBfHD6vKO-NlxJo-jyCwvpDg-Pe8E82haUYQDu5L_HgA_Qa7xyTXSOXAwKVfcifdZyhAjkXJSmZCpklqnAfp91rp7iaCPow8LKTNBkvreSVGEtQO6Fta-fWeQtqdn-4FGCoXX2ICbGTp-j3MTCeE2b2PfkhKcZcaEYu3fho2P6rFvjxH-Xp8pHl6fDDdw01IJHqSGcUmmvE-qkuEMSRkJ9x1P6mAR12w6LbEJH9C5qyq4d-P55zDHYTACsMls0elaHrHYwAURVT2OJjLmkcW38p73uADAIYBLtuTv67phPQ\n\n把这个token复制后，拷贝到登录Dashboard的token输入框中，就能以cluster-admin的角色成功登录了。如图：\n\n","categories":["Kubernetes","Dashboard"],"tags":["kubernetes","k8s"]},{"title":"CentOS7设置固定IP","url":"/2020/02/CentOS7%E8%AE%BE%E7%BD%AE%E5%9B%BA%E5%AE%9AIP/","content":"主机操作系统为macOS，在VMware Fusion上安装了数台CentOS7虚拟机，在其上面安装Kubernetes集群。\n主机待机或休眠后，再次进入虚拟机时，发现虚拟机的时间并不与主机保持同步。这个问题在mac主机待机后和虚拟机时间不统一的问题中解决。\n然后我通过命令date -s把虚拟机延迟的时间统一修改为当前时间。时间是修改好了，却出了一个严重的问题，连到虚拟机的ssh全部断开了，发现虚拟机的IP地址全部变了。随之而来的便是Prometheus的各种报警，监控的Targets全都变红了。\n所以需要把虚拟机从原来的DHCP动态分配IP改为静态固定好的IP。\n修改/etc/sysconfig/network-scripts/ifcfg-ens33文件的内容。\nBOOTPROTO=staticDNS1=114.114.114.114IPADDR=172.16.64.233NETMASK=255.255.255.0GATEWAY=172.16.64.2\n\n把BOOTPROTO由dhcp改为static。\n添加DNS1 Domain Name Server设置为114.114.114.114，这个是全国统一的。添加IPADDR设置为变化之前的IP地址。\n每次安装vmware fusion，自动生成的ip段都不一样。为了保持一致，可以修改vmware fusion的配置文件。\n\nVMnet1: 对应Host-only模式，也就是自定下的“仅供我的Mac专用”\n\n\nVMnet8:对应NAT模式，也就是网络配置中的Internet共享下的“与我的Mac共享”\n\n\nVMnet0:对应桥接模式，也就是桥接模式网络链接下的“自动检测”\n\ncat /Library/Preferences/VMware\\ Fusion/networking...answer VNET_8_HOSTONLY_NETMASK 255.255.255.0answer VNET_8_HOSTONLY_SUBNET 172.16.64.0answer VNET_8_NAT yesanswer VNET_8_VIRTUAL_ADAPTER yes...\n\n添加GATEWAY。\n执行ip route show，获取GATEWAY的地址为：172.16.64.2\nip route showdefault via 172.16.64.2 dev ens33 proto static metric 10010.244.36.64/26 via 172.16.64.232 dev tunl0 proto bird onlink10.244.169.128/26 via 172.16.64.235 dev tunl0 proto bird onlinkblackhole 10.244.235.192/26 proto bird10.244.235.232 dev calia6f6ed2a513 scope link10.244.235.233 dev cali9716a65ec87 scope link10.244.235.234 dev cali26873b73fbd scope link172.16.64.0/24 dev ens33 proto kernel scope link src 172.16.64.233 metric 100172.17.0.0/16 dev docker0 proto kernel scope link src 172.17.0.1172.19.0.0/16 dev br-bebfcba064b2 proto kernel scope link src 172.19.0.1\n\n保存退出后，重启网络systemctl restart network。过一会儿，Kubernetes集群又能正常工作了。\n","categories":["OS","CentOS"],"tags":["centos","vmware","ip","虚拟机"]},{"title":"验证prometheus监控targets的metrics","url":"/2019/12/%E9%AA%8C%E8%AF%81prometheus%E7%9B%91%E6%8E%A7targets%E7%9A%84metrics/","content":"在Prometheus监控Targets页面，显示的都是监控标的列表。我们如何验证访问此&#x2F;metrics的结果呢？\nmonitoring&#x2F;kube-apiserver\n\n这种https://172.16.64.233:6443/metrics URL，是需要带证书才能访问的：\ncurl -k --key /etc/kubernetes/pki/apiserver-kubelet-client.key \\--cert /etc/kubernetes/pki/apiserver-kubelet-client.crt \\https://172.16.64.233:6443/metrics...# HELP go_gc_duration_seconds A summary of the GC invocation durations.# TYPE go_gc_duration_seconds summarygo_gc_duration_seconds&#123;quantile=&quot;0&quot;&#125; 1.2317e-05go_gc_duration_seconds&#123;quantile=&quot;0.25&quot;&#125; 3.1153e-05go_gc_duration_seconds&#123;quantile=&quot;0.5&quot;&#125; 4.6201e-05\n\nmonitoring&#x2F;kube-state-metrics在页面上显示的Endpoint是https://10.244.169.167:9443/metrics\n然后查看kube-state-metrics-serviceMonitor.yaml\n发现有个配置bearerTokenFile: /var/run/secrets/kubernetes.io/serviceaccount/token\n进入到pod prometheus-k8s-0, 通过/var/run/secrets/kubernetes.io/serviceaccount/token文件拿到token。\n然后执行下面的命令就可以拿到指标数据了。\n更多详情请看此文 Prometheus monitoring&#x2F;kube-state-metrics监控指标\ncurl -k https://10.244.169.167:9443/metrics -H &quot;Authorization: Bearer eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJtb25pdG9yaW5nIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZWNyZXQubmFtZSI6InByb21ldGhldXMtazhzLXRva2VuLXNtNmdkIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZSI6InByb21ldGhldXMtazhzIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQudWlkIjoiMTk2OTkyOTItNDY2Yi00NWQ4LWJmYmYtYzkyZjIwNjczOWY3Iiwic3ViIjoic3lzdGVtOnNlcnZpY2VhY2NvdW50Om1vbml0b3Jpbmc6cHJvbWV0aGV1cy1rOHMifQ.iggZ4ZxmD0y04OQfDlo4P6zRgzn0ryVhcdhlgncpnBY5BJ39Xz0a2AA51ePa78R2njFDjPcecgDJRcqPv76X3o-C-G7EZvN_Ru8zSdB51YxqlLNoIW5hy6Jr27aw74lMslg1MYX_31kkRTqD9DxVn6lq6Uqf4Djebj_E-2maiwl863GCeNRfS1X6KM8idsVknLlpdVINbM8U_l1Yuw-auNzelAk1NQlBdbJqsm1CZKIg_YBsT-KbiyTsbjX2v0uL1D6-Q5Xs9NZMLEAa7dfwz_EOYMDnIGbv-eyhD-924H4_pGOIoQ0dCBP01cxFm7pLJPGouwLaEwPs5BRS0B6u-w&quot;\n\nmonitoring&#x2F;kubelethttps://172.16.64.233:10250/metrics\nmonitoring&#x2F;node-exporterhttps://172.16.64.233:9100/metrics\n","categories":["Kubernetes","Prometheus"],"tags":["监控","prometheus","metrics"]},{"title":"Docker网络Network","url":"/2020/02/Docker%E7%BD%91%E7%BB%9CNetwork/","content":"这个实验一定要在Linux环境下做，docker for mac, docker for win是不行的。\ndocker网络类型docker会给我们创建三种网络类型：bridge, host, none\ndocker network lsNETWORK ID          NAME                DRIVER              SCOPEa1681b4a3bc9        bridge              bridge              locald724eb42948a        host                host                localc9381cce7bbb        none                null                local\n\n在没有指定相关网络的情况下，默认情况，会使用bridge网络模式。\n网络验证启动两个busybox容器。\ndocker run -dit --name busybox1 busybox4f3c61775b5e8bcd38a0c97ff97bcd16ed717ab31bea417b198192f83b493846docker run -dit --name busybox2 busyboxc523392d8949b53abfbe736c43d2d47ea60a3420e8931bebc69d05baff93889b\n\n查看一下bridge网络\ndocker network inspect bridge[    &#123;        &quot;Name&quot;: &quot;bridge&quot;,        &quot;Id&quot;: &quot;a1681b4a3bc9bf973e2ff712677e373b663cc65b3a9dd6e868f5635fff295a6a&quot;,        &quot;Created&quot;: &quot;2020-02-05T16:24:09.290786154+08:00&quot;,        &quot;Scope&quot;: &quot;local&quot;,        &quot;Driver&quot;: &quot;bridge&quot;,        &quot;EnableIPv6&quot;: false,        &quot;IPAM&quot;: &#123;            &quot;Driver&quot;: &quot;default&quot;,            &quot;Options&quot;: null,            &quot;Config&quot;: [                &#123;                    &quot;Subnet&quot;: &quot;172.17.0.0/16&quot;,                    &quot;Gateway&quot;: &quot;172.17.0.1&quot;                &#125;            ]        &#125;,        &quot;Internal&quot;: false,        &quot;Attachable&quot;: false,        &quot;Ingress&quot;: false,        &quot;ConfigFrom&quot;: &#123;            &quot;Network&quot;: &quot;&quot;        &#125;,        &quot;ConfigOnly&quot;: false,        &quot;Containers&quot;: &#123;            &quot;4f3c61775b5e8bcd38a0c97ff97bcd16ed717ab31bea417b198192f83b493846&quot;: &#123;                &quot;Name&quot;: &quot;busybox1&quot;,                &quot;EndpointID&quot;: &quot;bb138601e4d4018e8f36b4679cc22823fca3020d53fb7c372e47d1c73345b374&quot;,                &quot;MacAddress&quot;: &quot;02:42:ac:11:00:02&quot;,                &quot;IPv4Address&quot;: &quot;172.17.0.2/16&quot;,                &quot;IPv6Address&quot;: &quot;&quot;            &#125;,            &quot;c523392d8949b53abfbe736c43d2d47ea60a3420e8931bebc69d05baff93889b&quot;: &#123;                &quot;Name&quot;: &quot;busybox2&quot;,                &quot;EndpointID&quot;: &quot;8f3d2661b8bd9fdf4f9bf2d763e36ab87a7e1fa8dca6e1e97b727c7a8ab2d0fe&quot;,                &quot;MacAddress&quot;: &quot;02:42:ac:11:00:03&quot;,                &quot;IPv4Address&quot;: &quot;172.17.0.3/16&quot;,                &quot;IPv6Address&quot;: &quot;&quot;            &#125;        &#125;,        &quot;Options&quot;: &#123;            &quot;com.docker.network.bridge.default_bridge&quot;: &quot;true&quot;,            &quot;com.docker.network.bridge.enable_icc&quot;: &quot;true&quot;,            &quot;com.docker.network.bridge.enable_ip_masquerade&quot;: &quot;true&quot;,            &quot;com.docker.network.bridge.host_binding_ipv4&quot;: &quot;0.0.0.0&quot;,            &quot;com.docker.network.bridge.name&quot;: &quot;docker0&quot;,            &quot;com.docker.network.driver.mtu&quot;: &quot;1500&quot;        &#125;,        &quot;Labels&quot;: &#123;&#125;    &#125;]\n\n可以看到busybox1和busybox2两个容器都连接到了bridge网络。两个容器的ip地址也能看到。\n进入busybox1容器\ndocker exec -it busybox1 sh\n\n通过ip地址和容器名ping容器\n/ # ping 172.17.0.3PING 172.17.0.3 (172.17.0.3): 56 data bytes64 bytes from 172.17.0.3: seq=0 ttl=64 time=0.285 ms64 bytes from 172.17.0.3: seq=1 ttl=64 time=0.130 ms/ # ping busybox2^C\n\n可以得出，通过ip地址是可以ping通的，但通过容器名是不行的。\n查看hosts信息，也找不到任何busybox2的信息\ncat /etc/hosts127.0.0.1\tlocalhost::1\tlocalhost ip6-localhost ip6-loopbackfe00::0\tip6-localnetff00::0\tip6-mcastprefixff02::1\tip6-allnodesff02::2\tip6-allrouters172.17.0.2\t4f3c61775b5e\n\n新建bridge网络docker network create --driver bridge busybox_bridgebebfcba064b2afdfb265d4df19eaccd950a1d957dcb3e2256d05dc544b64369e\n\n查看网络\ndocker network lsNETWORK ID          NAME                DRIVER              SCOPEa1681b4a3bc9        bridge              bridge              localbebfcba064b2        busybox_bridge      bridge              locald724eb42948a        host                host                localc9381cce7bbb        none                null                local\n\n分别创建busybox3和busybox4容器，并加入到busybox_bridge网络中\ndocker run -dit --network busybox_bridge --name busybox3 busybox8e39a57b7543288fb716f66eaa4a54609a571c7d30194ca456d4a1dc443f19e6docker run -dit --network busybox_bridge --name busybox4 busyboxed1715cfaf168f64351d46aaa7f393cae00962c7ef850d8824abe06a4906843c\n\n进入busybox3\n本以为可以PING得通busybox4，但貌似不行，此实验失败。后面再研究。\nhost模式以host模式启动一个nginx容器。\ndocker run --rm -d --net host nginx956b96f5d0c25835a1d4470c12ed5e4abda341e44f9344e72cf42864976d96fa\n\n因为是host模式，容器和主机共享一样的网络。\ncurl localhost&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt;&lt;title&gt;Welcome to nginx!&lt;/title&gt;...&lt;p&gt;&lt;em&gt;Thank you for using nginx.&lt;/em&gt;&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;\n","categories":["Docker","Network"],"tags":["docker","network"]},{"title":"Prometheus Kubelet ServiceMonitor","url":"/2020/02/Prometheus-Kubelet-ServiceMonitor/","content":"kubectl get serviceMonitor -n monitoring kubelet -o yaml\n\napiVersion: monitoring.coreos.com/v1kind: ServiceMonitormetadata:  annotations:    kubectl.kubernetes.io/last-applied-configuration: |      &#123;&quot;apiVersion&quot;:&quot;monitoring.coreos.com/v1&quot;,&quot;kind&quot;:&quot;ServiceMonitor&quot;,&quot;metadata&quot;:&#123;&quot;annotations&quot;:&#123;&#125;,&quot;labels&quot;:&#123;&quot;k8s-app&quot;:&quot;kubelet&quot;&#125;,&quot;name&quot;:&quot;kubelet&quot;,&quot;namespace&quot;:&quot;monitoring&quot;&#125;,&quot;spec&quot;:&#123;&quot;endpoints&quot;:[&#123;&quot;bearerTokenFile&quot;:&quot;/var/run/secrets/kubernetes.io/serviceaccount/token&quot;,&quot;honorLabels&quot;:true,&quot;interval&quot;:&quot;30s&quot;,&quot;port&quot;:&quot;https-metrics&quot;,&quot;scheme&quot;:&quot;https&quot;,&quot;tlsConfig&quot;:&#123;&quot;insecureSkipVerify&quot;:true&#125;&#125;,&#123;&quot;bearerTokenFile&quot;:&quot;/var/run/secrets/kubernetes.io/serviceaccount/token&quot;,&quot;honorLabels&quot;:true,&quot;interval&quot;:&quot;30s&quot;,&quot;metricRelabelings&quot;:[&#123;&quot;action&quot;:&quot;drop&quot;,&quot;regex&quot;:&quot;container_(network_tcp_usage_total|network_udp_usage_total|tasks_state|cpu_load_average_10s)&quot;,&quot;sourceLabels&quot;:[&quot;__name__&quot;]&#125;],&quot;path&quot;:&quot;/metrics/cadvisor&quot;,&quot;port&quot;:&quot;https-metrics&quot;,&quot;scheme&quot;:&quot;https&quot;,&quot;tlsConfig&quot;:&#123;&quot;insecureSkipVerify&quot;:true&#125;&#125;],&quot;jobLabel&quot;:&quot;k8s-app&quot;,&quot;namespaceSelector&quot;:&#123;&quot;matchNames&quot;:[&quot;kube-system&quot;]&#125;,&quot;selector&quot;:&#123;&quot;matchLabels&quot;:&#123;&quot;k8s-app&quot;:&quot;kubelet&quot;&#125;&#125;&#125;&#125;  creationTimestamp: &quot;2019-12-29T06:40:27Z&quot;  generation: 1  labels:    k8s-app: kubelet  name: kubelet  namespace: monitoring  resourceVersion: &quot;2602&quot;  selfLink: /apis/monitoring.coreos.com/v1/namespaces/monitoring/servicemonitors/kubelet  uid: 327bae9c-d09c-4d00-b293-354870b19f91spec:  endpoints:  - bearerTokenFile: /var/run/secrets/kubernetes.io/serviceaccount/token    honorLabels: true    interval: 30s    port: https-metrics    scheme: https    tlsConfig:      insecureSkipVerify: true  - bearerTokenFile: /var/run/secrets/kubernetes.io/serviceaccount/token    honorLabels: true    interval: 30s    metricRelabelings:    - action: drop      regex: container_(network_tcp_usage_total|network_udp_usage_total|tasks_state|cpu_load_average_10s)      sourceLabels:      - __name__    path: /metrics/cadvisor    port: https-metrics    scheme: https    tlsConfig:      insecureSkipVerify: true  jobLabel: k8s-app  namespaceSelector:    matchNames:    - kube-system  selector:    matchLabels:      k8s-app: kubelet\n\n根据上面的namespaceSelector和selector\nkubectl get ep -n kube-system -l k8s-app=kubelet -o yaml\n\napiVersion: v1items:- apiVersion: v1  kind: Endpoints  metadata:    creationTimestamp: &quot;2019-12-29T06:40:06Z&quot;    labels:      k8s-app: kubelet    name: kubelet    namespace: kube-system    resourceVersion: &quot;2371&quot;    selfLink: /api/v1/namespaces/kube-system/endpoints/kubelet    uid: 326583cc-3379-4883-8426-5a856c29c77e  subsets:  - addresses:    - ip: 172.16.64.232      targetRef:        kind: Node        name: k8s-node1        uid: 09ffae8e-e3f1-456b-a398-7891487712da    - ip: 172.16.64.233      targetRef:        kind: Node        name: k8s-master        uid: 774ddb3e-2c3c-4af8-8e1c-15b0c2c3b8c1    - ip: 172.16.64.235      targetRef:        kind: Node        name: k8s-node2        uid: 7c1cc5f0-2b71-462a-907e-191b0d3ee819    ports:    - name: http-metrics      port: 10255      protocol: TCP    - name: cadvisor      port: 4194      protocol: TCP    - name: https-metrics      port: 10250      protocol: TCPkind: Listmetadata:  resourceVersion: &quot;&quot;  selfLink: &quot;&quot;\n","categories":["Kubernetes","Prometheus"],"tags":["kubernetes","k8s","prometheus"]},{"title":"Prometheus monitoring/kube-state-metrics监控指标","url":"/2020/02/Prometheus-monitoring-kube-state-metrics%E7%9B%91%E6%8E%A7%E6%8C%87%E6%A0%87/","content":"通过kube-prometheus安装的k8s监控系统。\n在targets页面，我们来分析一下monitoring/kube-state-metrics的两个Targets。\n\n\nkube-state-metrics-serviceMonitor.yaml内容：\nendpoints:- bearerTokenFile: /var/run/secrets/kubernetes.io/serviceaccount/token  honorLabels: true  interval: 30s  port: https-main  scheme: https  scrapeTimeout: 30s  tlsConfig:    insecureSkipVerify: true- bearerTokenFile: /var/run/secrets/kubernetes.io/serviceaccount/token  interval: 30s  port: https-self  scheme: https  tlsConfig:    insecureSkipVerify: truejobLabel: k8s-appselector:  matchLabels:    k8s-app: kube-state-metrics\n\nEndpoint这里面有两个Targets，是因为在kube-state-metrics-serviceMonitor.yaml文件中的spec下面配置了两个endpoints。\n并且这两个都是用bearerTokenFile: /var/run/secrets/kubernetes.io/serviceaccount/token来获取权限的。\n进入prometheus-k8s-0容器查看这个文件的内容，或者通过secret直接查看，我们选择后者。\nkubectl get secret -n monitoringNAME                              TYPE                                  DATA   AGEprometheus-k8s                    Opaque                                1      3h12mprometheus-k8s-token-sm6gd        kubernetes.io/service-account-token   3      58d...\n\nkubectl describe secret prometheus-k8s-token-sm6gd -n monitoring\n\n取到token\neyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJtb25pdG9yaW5nIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZWNyZXQubmFtZSI6InByb21ldGhldXMtazhzLXRva2VuLXNtNmdkIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZSI6InByb21ldGhldXMtazhzIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQudWlkIjoiMTk2OTkyOTItNDY2Yi00NWQ4LWJmYmYtYzkyZjIwNjczOWY3Iiwic3ViIjoic3lzdGVtOnNlcnZpY2VhY2NvdW50Om1vbml0b3Jpbmc6cHJvbWV0aGV1cy1rOHMifQ.iggZ4ZxmD0y04OQfDlo4P6zRgzn0ryVhcdhlgncpnBY5BJ39Xz0a2AA51ePa78R2njFDjPcecgDJRcqPv76X3o-C-G7EZvN_Ru8zSdB51YxqlLNoIW5hy6Jr27aw74lMslg1MYX_31kkRTqD9DxVn6lq6Uqf4Djebj_E-2maiwl863GCeNRfS1X6KM8idsVknLlpdVINbM8U_l1Yuw-auNzelAk1NQlBdbJqsm1CZKIg_YBsT-KbiyTsbjX2v0uL1D6-Q5Xs9NZMLEAa7dfwz_EOYMDnIGbv-eyhD-924H4_pGOIoQ0dCBP01cxFm7pLJPGouwLaEwPs5BRS0B6u-w\nmonitoring&#x2F;kube-state-metrics&#x2F;0Endpoint: https://10.244.36.69:8443/metrics\n执行如下命令即可获取所有指标了。\ncurl -k https://10.244.36.69:8443/metrics -H &quot;Authorization: Bearer eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJtb25pdG9yaW5nIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZWNyZXQubmFtZSI6InByb21ldGhldXMtazhzLXRva2VuLXNtNmdkIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZSI6InByb21ldGhldXMtazhzIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQudWlkIjoiMTk2OTkyOTItNDY2Yi00NWQ4LWJmYmYtYzkyZjIwNjczOWY3Iiwic3ViIjoic3lzdGVtOnNlcnZpY2VhY2NvdW50Om1vbml0b3Jpbmc6cHJvbWV0aGV1cy1rOHMifQ.iggZ4ZxmD0y04OQfDlo4P6zRgzn0ryVhcdhlgncpnBY5BJ39Xz0a2AA51ePa78R2njFDjPcecgDJRcqPv76X3o-C-G7EZvN_Ru8zSdB51YxqlLNoIW5hy6Jr27aw74lMslg1MYX_31kkRTqD9DxVn6lq6Uqf4Djebj_E-2maiwl863GCeNRfS1X6KM8idsVknLlpdVINbM8U_l1Yuw-auNzelAk1NQlBdbJqsm1CZKIg_YBsT-KbiyTsbjX2v0uL1D6-Q5Xs9NZMLEAa7dfwz_EOYMDnIGbv-eyhD-924H4_pGOIoQ0dCBP01cxFm7pLJPGouwLaEwPs5BRS0B6u-w&quot;\n\n指标以kube_开始。kube_configmap_kube_cronjob_kube_daemonset_kube_deployment_kube_endpoint_kube_namespace_kube_node_kube_persistentvolumeclaim_kube_persistentvolume_kube_pod_kube_pod_container_kube_secret_kube_statefulset_kube_storageclass_\n我将指标存入kube-state-metrics-0.txt ，供下载查看。\nmonitoring&#x2F;kube-state-metrics&#x2F;1Endpoint: https://10.244.36.69:9443/metrics\n指标为go系统级别指标。\n同理可得其指标。kube-state-metrics-1.txt \n","categories":["Kubernetes","Prometheus"],"tags":["kubernetes","k8s","prometheus"]},{"title":"Prometheus monitoring/kubelet监控指标","url":"/2020/02/Prometheus-monitoring-kubelet%E7%9B%91%E6%8E%A7%E6%8C%87%E6%A0%87/","content":"通过kube-prometheus安装的k8s监控系统。\n在targets页面，我们来分析一下monitoring/kubelet/的两个Targets。\n\n\nprometheus-serviceMonitorKubelet.yaml内容：\napiVersion: monitoring.coreos.com/v1kind: ServiceMonitormetadata:  labels:    k8s-app: kubelet  name: kubelet  namespace: monitoringspec:  endpoints:  - bearerTokenFile: /var/run/secrets/kubernetes.io/serviceaccount/token    honorLabels: true    interval: 30s    port: https-metrics    scheme: https    tlsConfig:      insecureSkipVerify: true  - bearerTokenFile: /var/run/secrets/kubernetes.io/serviceaccount/token    honorLabels: true    interval: 30s    metricRelabelings:    - action: drop      regex: container_(network_tcp_usage_total|network_udp_usage_total|tasks_state|cpu_load_average_10s)      sourceLabels:      - __name__    path: /metrics/cadvisor    port: https-metrics    scheme: https    tlsConfig:      insecureSkipVerify: true  jobLabel: k8s-app  namespaceSelector:    matchNames:    - kube-system  selector:    matchLabels:      k8s-app: kubelet\n\nmonitoring&#x2F;kubelet&#x2F;0Endpoint: https://172.16.64.232:10250/metrics\n指标以kubelet_开始apiserver_http_request_http_request_rest_client_request_storage_operation_volume_manager_\n指标kubelet-0-0.txt  \nmonitoring&#x2F;kubelet&#x2F;1Endpoint: https://172.16.64.232:10250/metrics/cadvisor\n使用的自动发现role为endpoints\n- job_name: monitoring/kubelet/1  honor_labels: true  honor_timestamps: true  scrape_interval: 30s  scrape_timeout: 10s  metrics_path: /metrics/cadvisor  scheme: https  kubernetes_sd_configs:  - role: endpoints\n\n包含一个cadvisor_version_info指标可以查看cadvisor的版本信息。其他指标以container_开始\n指标kubelet-1-0.txt \n","categories":["Kubernetes","Prometheus"],"tags":["kubernetes","k8s","prometheus"]},{"title":"Prometheus入门基础","url":"/2020/02/Prometheus%E5%85%A5%E9%97%A8%E5%9F%BA%E7%A1%80/","content":"什么是PrometheusPrometheus是由SoundCloud开发的开源监控报警系统和时序列数据库(TSDB)。Prometheus使用Go语言开发。\n2016年由Google发起Linux基金会旗下的原生云基金会(Cloud Native Computing Foundation), 将Prometheus纳入其下第二大开源项目。\nPrometheus特点多维数据模型：由度量名称和键值对标识的时间序列数据\nnode_cpu_seconds_total&#123;cpu=&quot;0&quot;,endpoint=&quot;https&quot;,instance=&quot;k8s-master&quot;,job=&quot;node-exporter&quot;,mode=&quot;idle&quot;,namespace=&quot;monitoring&quot;,pod=&quot;node-exporter-t9ljw&quot;,service=&quot;node-exporter&quot;&#125;\n\nPromSQL是一种灵活的查询语言，可以利用多维数据完成复杂的查询\n不依赖分布式存储，单个服务器节点可直接工作\n基于HTTP的pull方式采集时间序列数据\n推送时间序列数据通过PushGateway组件支持\n通过服务发现或静态配置发现目标\n多种图形模式及仪表盘支持\nPrometheus架构\n\nPrometheus 由多个组件组成，但是其中许多组件是可选的：\nPrometheus Server用于收集指标和存储时间序列数据，并提供查询接口(http api)\nClient Library客户端库（例如Go，Python，Java等），为需要监控的服务产生相应的&#x2F;metrics并暴露给Prometheus Server。目前已经有很多的软件原生就支持Prometheus，提供&#x2F;metrics，可以直接使用。对于像操作系统已经不提供&#x2F;metrics，可以使用exporter，或者自己开发exporter来提供&#x2F;metrics服务。\nPush Gateway主要用于临时性的jobs。由于这类jobs存在时间较短，可能在Prometheus pull之前就消失了。对此Jobs定时将指标push到pushgateway，再由PrometheusServer从Pushgateway上pull。这种方式主要用于服务层面的metrics\nexporter用于暴露已有的第三方服务的 metrics 给 Prometheus。\nalertmanager从 Prometheus server 端接收到 alerts 后，会进行去除重复数据，分组，并路由到对应的接收方式，发出报警。常见的接收方式有：电子邮件，pagerduty，OpsGenie, webhook 等。\nWeb UIPrometheus内置一个简单的Web控制台，可以查询指标，查看配置信息或者Service Discovery等，实际工作中，查看指标或者创建仪表盘通常使用Grafana，Prometheus作为Grafana的数据源。\n基本原理Prometheus的基本原理是通过HTTP协议周期性抓取被监控组件的状态，任意组件只要提供对应的HTTP接口就可以接入监控。不需要任何SDK或者其他的集成过程。这样做非常适合做虚拟化环境监控系统，比如VM、Docker、Kubernetes等。输出被监控组件信息的HTTP接口被叫做exporter 。目前互联网公司常用的组件大部分都有exporter可以直接使用，比如Varnish、Haproxy、Nginx、MySQL、Linux系统信息(包括磁盘、内存、CPU、网络等等)。\n服务过程Prometheus Daemon负责定时去目标上抓取metrics(指标)数据，每个抓取目标需要暴露一个http服务的接口给它定时抓取。Prometheus支持通过配置文件、文本文件、Zookeeper、Consul、DNS SRV Lookup等方式指定抓取目标。Prometheus采用PULL的方式进行监控，即服务器可以直接通过目标PULL数据或者间接地通过中间网关来Push数据。\nPrometheus在本地存储抓取的所有数据，并通过一定规则进行清理和整理数据，并把得到的结果存储到新的时间序列中。\nPrometheus通过PromQL和其他API可视化地展示收集的数据。Prometheus支持很多方式的图表可视化，例如Grafana、自带的Promdash以及自身提供的模版引擎等等。Prometheus还提供HTTP API的查询方式，自定义所需要的输出。\nPushGateway支持Client主动推送metrics到PushGateway，而Prometheus只是定时去Gateway上抓取数据。\nAlertmanager是独立于Prometheus的一个组件，可以支持Prometheus的查询语句，提供十分灵活的报警方式。\n三大套件Prometheus Server主要负责数据采集和存储，提供PromQL查询语言的支持。\nAlertmanager警告管理器，用来进行报警。\nPush Gateway支持临时性Job主动推送指标的中间网关。\n数据模型Prometheus将所有数据存储为时间序列；具有相同度量名称以及标签属于同一个指标。\n每个时间序列都由度量标准名称和一组键值对（也成为标签）唯一标识。\n时间序列格式：\n&lt;metric name&gt;&#123;&lt;label name&gt;=&lt;label value&gt;, ...&#125;\n指标类型Counter递增的计数器，如CPU使用时间。\nGauge可以任意变化的数值：cup使用率\nHistogram对一段时间范围内数据进行采样，并对所有数值求和与统计数量\nSummary与Histogram类似，带有分位数。\n抓取实例scrape_configs:- job_name: &#x27;prometheus&#x27;  static_configs:  - targets: [&#x27;localhost:9090&#x27;,&#x27;192.168.1.100:9090&#x27;]- job_name: &#x27;node&#x27;  static_configs:  - targets: [&#x27;192.168.1.10:9090&#x27;]\n","categories":["Kubernetes","Prometheus"],"tags":["kubernetes","k8s","prometheus"]},{"title":"Prometheus monitoring/node-exporter监控指标","url":"/2020/02/Prometheus-monitoring-node-exporter%E7%9B%91%E6%8E%A7%E6%8C%87%E6%A0%87/","content":"通过kube-prometheus安装的k8s监控系统。\n在targets页面，我们来分析一下monitoring/node-exporter/的这个Targets。\n\n\nnode-exporter-serviceMonitor.yaml内容：\napiVersion: monitoring.coreos.com/v1kind: ServiceMonitormetadata:  labels:    k8s-app: node-exporter  name: node-exporter  namespace: monitoringspec:  endpoints:  - bearerTokenFile: /var/run/secrets/kubernetes.io/serviceaccount/token    interval: 30s    port: https    relabelings:    - action: replace      regex: (.*)      replacment: $1      sourceLabels:      - __meta_kubernetes_pod_node_name      targetLabel: instance    scheme: https    tlsConfig:      insecureSkipVerify: true  jobLabel: k8s-app  selector:    matchLabels:      k8s-app: node-exporter\n\nmonitoring&#x2F;node-exporter&#x2F;0Endpoint: https://172.16.64.233:9100/metrics\n查看Configuration页面，发现是以role endpoints做为自动服务发现的\n- job_name: monitoring/node-exporter/0  honor_timestamps: true  scrape_interval: 30s  scrape_timeout: 10s  metrics_path: /metrics  scheme: https  kubernetes_sd_configs:  - role: endpoints...\n\n获取token\nkubectl get secret prometheus-k8s-token-sm6gd -n monitoring -o template=&#123;&#123;.data.token&#125;&#125; | base64 -d\n\n通过template获取secret里面的值，要通过base64解密一下的。\n指标大部分以node_开始少许几个以promhttp_metric_开始的指标。\n指标node-exporter-node0.txt \n","categories":["Kubernetes","Prometheus"],"tags":["kubernetes","k8s","prometheus"]},{"title":"docker镜像仓库harbor的使用","url":"/2020/02/docker%E9%95%9C%E5%83%8F%E4%BB%93%E5%BA%93harbor%E7%9A%84%E4%BD%BF%E7%94%A8/","content":"在docker镜像仓库harbor搭建中讲解了如何安装harbor，这一篇文章主要讲如何使用harbor。\n创建用户\n\n输入用户信息，创建用于push image的用户。\n\n\n创建项目\n\n输入项目信息。\n\n\n进入项目，在成员标签下，把刚才创建的user用户以开发人员的角色，添加到此项目下。\n推入镜像我们先下载一个nginx的镜像\ndocker pull nginx:1.16.1\n\n查看镜像\ndocker imagesREPOSITORY                                    TAG                 IMAGE ID            CREATED             SIZEnginx                                         1.16.1              55c440ba1ecb        3 days ago          127MB\n\n修改tag\ndocker tag 55c440ba1ecb 172.16.64.233/my-project/nginx:1.16.1\n\n推镜像docker push 172.16.64.233/my-project/nginx:1.16.1The push refers to repository [172.16.64.233/my-project/nginx]Get https://172.16.64.233/v2/: dial tcp 172.16.64.233:443: connect: connection refused\n\n所以需要加入insecure-registries配置\nvi /etc/docker/daemon.json\n\n添加如下内容：\n&#123;        &quot;insecure-registries&quot;: [&quot;172.16.64.233&quot;]&#125;\n\n重启docker\nsystemctl restart docker\n\n再次推送\ndocker push 172.16.64.233/my-project/nginx:1.16.1The push refers to repository [172.16.64.233/my-project/nginx]37ec257a56ed: Preparing567538016328: Preparing488dfecc21b1: Preparingdenied: requested access to the resource is denied\n\n出现权限不够的错误。\n所以需要登录。之前我们注册了一个user用户。\ndocker login 172.16.64.233Username: userPassword:WARNING! Your password will be stored unencrypted in /root/.docker/config.json.Configure a credential helper to remove this warning. Seehttps://docs.docker.com/engine/reference/commandline/login/#credentials-storeLogin Succeeded\n\n再次push\ndocker push 172.16.64.233/my-project/nginx:1.16.1The push refers to repository [172.16.64.233/my-project/nginx]37ec257a56ed: Pushed567538016328: Pushed488dfecc21b1: Pushed1.16.1: digest: sha256:5f281748501a5ad9f5d657fc6067ac6187d62be2a811c460deee1504cabddc51 size: 948\n\n成功。\n在harbor页面里，也可以看到此镜像了。\n拉取镜像我们可以换一台机器，拉取镜像。\ndocker pull 172.16.64.233/my-project/nginx:1.16.1Error response from daemon: Get https://172.16.64.233/v2/: dial tcp 172.16.64.233:443: connect: connection refused\n\n和前面处理这个问题的方式一样，在/etc/docker/daemon.json加入如下内容。\n&#123;        &quot;insecure-registries&quot;: [&quot;172.16.64.233&quot;]&#125;\n\n并重启docker\nsystemctl restart docker\n\n再次拉取镜像\ndocker pull 172.16.64.233/my-project/nginx:1.16.11.16.1: Pulling from my-project/nginxbc51dd8edc1b: Pull complete60041be5685b: Pull complete5ad6baa9b36b: Pull completeDigest: sha256:5f281748501a5ad9f5d657fc6067ac6187d62be2a811c460deee1504cabddc51Status: Downloaded newer image for 172.16.64.233/my-project/nginx:1.16.1\n\n成功拉取。\n高可用设置如果需要搭几个harbor节点，则可以到仓库管理和同步管理标签中设置同步规则。\n然后再在harbor的前端，通过nginx做一个负载均衡即可。\n","categories":["Docker","Harbor"],"tags":["docker","image","harbor"]},{"title":"hexo博客在百度搜索引擎上的链接提交","url":"/2020/02/hexo%E5%8D%9A%E5%AE%A2%E5%9C%A8%E7%99%BE%E5%BA%A6%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E%E4%B8%8A%E7%9A%84%E9%93%BE%E6%8E%A5%E6%8F%90%E4%BA%A4/","content":"想让自己的博客被百度搜索引擎尽快收录，并在搜索时排在前面一些，那就需要及时提交博客的URL。\n主要有三种提交方法，大家最好三种都用，这也是官方推荐的，不会有任何冲突。\n\n\n主动推送主动推送是实时的，能让百度在第一时间知道你的原创内容。\n首先要安装hexo-baidu-url-submit插件。\n在博客根目录下执行如下命令：\nsudo npm install hexo-baidu-url-submit --save+ hexo-baidu-url-submit@0.0.6added 85 packages from 61 contributors, removed 71 packages and updated 11 packages in 14.783s   ╭────────────────────────────────────────────────────────────────╮   │                                                                │   │      New minor version of npm available! 6.10.0 → 6.13.7       │   │   Changelog: https://github.com/npm/cli/releases/tag/v6.13.7   │   │               Run npm install -g npm to update!                │   │                                                                │   ╰────────────────────────────────────────────────────────────────╯(base) \n\n然后在根目录中的_config.yml文件内容最后，添加如下内容：\ncount为提交Url的数量。\nhost为博客的地址。\ntoken这也可以在百度网站上找到。\n# Baibu url submitbaidu_url_submit:  count: 500  host: https://finolo.gy  token: ## 填下图位置里面token参数的值  path: baidu_urls.txt\n\n\n\n最后在Deploy标签下加入buidu_url_submitter\ndeploy:  - type: baidu_url_submitter\n\n在hexo generate时，会产生.deploy_git&#x2F;baidu_urls.txt文件。\nhexo deploy时，会从上述文件中读取urls，提交到百度。\n成功以后，一般会返回类似这样的信息。remain为今天剩余可推送urls数量，success为成功推送的urls数量。\n&#123;&quot;remain&quot;:99602,&quot;success&quot;:149&#125;INFO  Deploy done: baidu_url_submitter\n\n自动推送在themes/&lt;your_theme&gt;/_config.yaml中，把baidu_push标签的值设置为true。\n这时，系统会调用到themes/&lt;your_theme&gt;/layout/_third-party/seo/baidu-push.swig文件。\nSitemap安装插件\nnpm install hexo-generator-sitemap --save     npm install hexo-generator-baidu-sitemap --save\n\n然后把http://&lt;domain&gt;/baidusitemap.xml和http://&lt;domain&gt;/sitemap.xml提交到百度sitemap页面即可。\n","categories":["Tools","Hexo"],"tags":["hexo","sitemaps"]},{"title":"Prometheus监控配置kubernetes_sd_config","url":"/2020/02/Prometheus%E7%9B%91%E6%8E%A7%E9%85%8D%E7%BD%AEkubernetes-sd-config/","content":"官网对kubernetes_sd_config的详细说明文档\nhttps://prometheus.io/docs/prometheus/latest/configuration/configuration/#kubernetes_sd_config\nkubernetes_sd_config用于配置自动发现的。关于服务自动发现，还可以参考这篇实战：Prometheus kubernetes-cadvisor服务自动发现\nKubernetes SD configurations allow retrieving scrape targets from Kubernetes’ REST API and always staying synchronized with the cluster state.\nrole的类型有以下几种：node, service, pod, endpoints和ingress。\n当role的值为endpoints时，官网说明：\nThe endpoints role discovers targets from listed endpoints of a service. For each endpoint address one target is discovered per port. If the endpoint is backed by a pod, all additional container ports of the pod, not bound to an endpoint port, are discovered as targets as well.\n然后prometheus-additional.yaml的容是：\n- job_name: &#x27;kubernetes-service-endpoints&#x27;  kubernetes_sd_configs:  - role: endpoints  relabel_configs:  - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape]    action: keep    regex: true  - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scheme]    action: replace    target_label: __scheme__    regex: (https?)  - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_path]    action: replace    target_label: __metrics_path__    regex: (.+)  - source_labels: [__address__, __meta_kubernetes_service_annotation_prometheus_io_port]    action: replace    target_label: __address__    regex: ([^:]+)(?::\\d+)?;(\\d+)    replacement: $1:$2  - action: labelmap    regex: __meta_kubernetes_service_label_(.+)  - source_labels: [__meta_kubernetes_namespace]    action: replace    target_label: kubernetes_namespace  - source_labels: [__meta_kubernetes_service_name]    action: replace    target_label: kubernetes_name\n\n我理解的逻辑是：\n\n首先查找ep,svc两个同时存在的\n然后查找svc的annotations标签__meta_kubernetes_service_annotation_prometheus_io_scrape的值为true的。\n最后看到ep里面有两个Addresses，所以显示两个Targets。\n\n\n\n\n\n","categories":["Kubernetes","Prometheus"],"tags":["kubernetes","k8s","prometheus"]},{"title":"k8s master节点nodePort方式不能访问服务","url":"/2020/02/k8s-master%E8%8A%82%E7%82%B9nodePort%E6%96%B9%E5%BC%8F%E4%B8%8D%E8%83%BD%E8%AE%BF%E9%97%AE%E6%9C%8D%E5%8A%A1/","content":"k8s集群三个节点，一个master node, 两个worker node。\n我们可以看到prometheus以NodePort方式把服务暴露出来。\nkubectl get svc -n monitoringNAME                    TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)                      AGEprometheus-k8s          NodePort    10.105.212.163   &lt;none&gt;        9090:30090/TCP               45d\n\n我们通过&lt;worker-node-ip&gt;:30090的方式，可以正确访问到页面，但通过&lt;master-node-ip&gt;:30090，却不能正常访问页面。\n随即，我查看了calico网络插件的运行情况。\nkubectl get po  -n kube-systemNAME                                       READY   STATUS    RESTARTS   AGEcalico-node-jt4fz                          0/1     Running   4          45dcalico-node-mv4ht                          1/1     Running   5          45dcalico-node-nqbkl                          1/1     Running   5          45d\n\n其中一个pod，并未通过健康检查。\n查看一下这个pod的详情信息。\nkubectl describe po calico-node-jt4fz -n kube-systemName:                 calico-node-jt4fzNamespace:            kube-system...Events:  Type     Reason     Age                    From                 Message  ----     ------     ----                   ----                 -------  Warning  Unhealthy  6m22s (x499 over 89m)  kubelet, k8s-master  (combined from similar events): Readiness probe failed: calico/node is not ready: BIRD is not ready: BGP not established with 172.16.64.232,172.16.64.2352020-02-12 12:36:36.591 [INFO][12943] health.go 156: Number of node(s) with BGP peering established = 0\n\n错误的原因是：\nReadiness probe failed: calico&#x2F;node is not ready: BIRD is not ready: BGP not established with 172.16.64.232,172.16.64.2352020-02-12 12:36:36.591 [INFO][12943] health.go 156: Number of node(s) with BGP peering established &#x3D; 0\n解决方案：\n调整calico网络插件的网卡发现机制，修改IP_AUTODETECTION_METHOD对应的value值。\n官方提供的yaml文件中，ip识别策略（IPDETECTMETHOD）没有配置，即默认为first-found，这会导致一个网络异常的ip作为nodeIP被注册，从而影响node-to-node mesh。\n我们可以修改成can-reach或者interface的策略，尝试连接某一个Ready的node的IP，以此选择出正确的IP。\n在calico.yaml文件中添加如下两行内容\n- name: IP_AUTODETECTION_METHOD  value: &quot;interface=ens.*&quot;  # ens 根据实际网卡开头配置\n\n配置如下：\n# Cluster type to identify the deployment type- name: CLUSTER_TYPE  value: &quot;k8s,bgp&quot;# Specify interface- name: IP_AUTODETECTION_METHOD  value: &quot;interface=ens.*&quot;# Auto-detect the BGP IP address.- name: IP  value: &quot;autodetect&quot;# Enable IPIP- name: CALICO_IPV4POOL_IPIP  value: &quot;Always&quot;\n\n修改后重新应用一下calico.yaml文件。\nkubectl apply -f calico.yaml\n\n我们发现calico所有pod都已经成功启动了。\nkubectl get po -n kube-systemNAME                                       READY   STATUS    RESTARTS   AGEcalico-node-jtvh8                          1/1     Running   0          15scalico-node-k6m8t                          1/1     Running   0          45scalico-node-rb9qx                          1/1     Running   0          29s\n\n此时，通过&lt;master-node-ip&gt;:30090，即可以成功访问到页面了。\n","categories":["Kubernetes","Network"],"tags":["kubernetes","k8s","master node","nodePort","calico"]},{"title":"mac主机待机后和虚拟机时间不统一的问题","url":"/2020/02/mac%E4%B8%BB%E6%9C%BA%E5%BE%85%E6%9C%BA%E5%90%8E%E5%92%8C%E8%99%9A%E6%8B%9F%E6%9C%BA%E6%97%B6%E9%97%B4%E4%B8%8D%E7%BB%9F%E4%B8%80%E7%9A%84%E9%97%AE%E9%A2%98/","content":"macOS安装的Vmware Fusion，然后安装了数台CentOS虚拟机。\n在mac主机待机和休眠后，虚拟机的时间老是不能和主机保持统一，虚拟机就是不是最新的时间，集群就会出问题。\n解决方法就是安装Vmware Tools。\n在VMware Fusion中，选中虚拟机，点击菜单虚拟机 -&gt; 安装VMware Tools。相当于在虚拟机里插入了安装软件的光盘。\n\n\n在mac主机环境下，在CentOS7虚拟机里面安装VMware Tools。如果是在Windows主机环境下安装Vmware Tools，请参考：Windows主机环境CentOS7虚拟机安装VMware Tools\n# mount /dev/cdrom /mntmount: /dev/sr0 写保护，将以只读方式挂载\n\n拷贝安装文件并解压。\n# tar -xvf /mnt/VMwareTools-10.3.10-13959562.tar.gz -C /tmp/\n\n安装\n# /tmp/vmware-tools-distrib/vmware-install.pl\n\n如果报如下错误：\n# /tmp/vmware-tools-distrib/vmware-install.pl-bash: /tmp/vmware-tools-distrib/vmware-install.pl: /usr/bin/perl: 坏的解释器: 没有那个文件或目录\n\n安装一下perl就可以了。\n# yum install -y perl\n\n再次安装\n# /tmp/vmware-tools-distrib/vmware-install.plopen-vm-tools packages are available from the OS vendor and VMware recommendsusing open-vm-tools packages. See http://kb.vmware.com/kb/2073803 for moreinformation.Do you still want to proceed with this installation? [no] yes\n\n然后一路回车即可。\n此时，可发现哪怕主机休眠或待机后，虚拟机的时间也和主机保持一致，为最新时间了。\n","categories":["OS","MacOS"],"tags":["centos","vmware","date"]},{"title":"/var/lib/docker/overlay2占用空间太大","url":"/2020/02/var-lib-docker-overlay2%E5%8D%A0%E7%94%A8%E7%A9%BA%E9%97%B4%E5%A4%AA%E5%A4%A7/","content":"执行df -h命令发现/var/lib/docker目录占用空间太大。\n# df -h文件系统                 容量  已用  可用 已用% 挂载点devtmpfs                 898M     0  898M    0% /devtmpfs                    910M     0  910M    0% /dev/shmtmpfs                    910M   60M  851M    7% /runtmpfs                    910M     0  910M    0% /sys/fs/cgroup/dev/mapper/centos-root   17G  5.9G   12G   35% //dev/sda1               1014M  150M  865M   15% /boottmpfs                    910M  4.0K  910M    1% /var/lib/kubelet/pods/0914bac1-2c2f-4294-a036-afe6fb5d2a16/volumes/kubernetes.io~secret/configtmpfs                    910M  4.0K  910M    1% /var/lib/kubelet/pods/7975066f-117d-4523-a55e-53262ed8f131/volumes/kubernetes.io~secret/config-volumetmpfs                    910M   12K  910M    1% /var/lib/kubelet/pods/7454b8be-1bd2-4c17-9784-55bf49d0e6fd/volumes/kubernetes.io~secret/node-exporter-token-hwqlctmpfs                    910M   12K  910M    1% /var/lib/kubelet/pods/7975066f-117d-4523-a55e-53262ed8f131/volumes/kubernetes.io~secret/alertmanager-main-token-z4qrqtmpfs                    910M   12K  910M    1% /var/lib/kubelet/pods/71e0ccae-267d-4025-b4ec-a5ccbd1f0adc/volumes/kubernetes.io~secret/kube-proxy-token-kq9lptmpfs                    910M  4.0K  910M    1% /var/lib/kubelet/pods/e685593b-9684-48cc-a304-040b30707088/volumes/kubernetes.io~secret/configtmpfs                    910M   12K  910M    1% /var/lib/kubelet/pods/53e83a95-4db5-48f7-b166-b37df18b38e0/volumes/kubernetes.io~secret/calico-node-token-f6g2mtmpfs                    910M   12K  910M    1% /var/lib/kubelet/pods/0914bac1-2c2f-4294-a036-afe6fb5d2a16/volumes/kubernetes.io~secret/prometheus-k8s-token-sm6gdtmpfs                    910M   12K  910M    1% /var/lib/kubelet/pods/e685593b-9684-48cc-a304-040b30707088/volumes/kubernetes.io~secret/prometheus-k8s-token-sm6gdoverlay                   17G  5.9G   12G   35% /var/lib/docker/overlay2/70944089660c7647c7a83e1dcef9d6297e3e440d4d2d4742f5d68448bde9ad95/mergedshm                       64M     0   64M    0% /var/lib/docker/containers/e59d5037dc150cf7c2ed10ea94d8549c3b98e0b847980a1a306eda8dcaf7e6de/mounts/shmoverlay                   17G  5.9G   12G   35% /var/lib/docker/overlay2/bbdeb82071f8c4cbb2d59e26721735d5f3fd82f8139c0a448133156a0f12631f/mergedoverlay                   17G  5.9G   12G   35% /var/lib/docker/overlay2/383c5b7d30028f38eca1e4f2b3ac6144d52b5b1e8f296f2314de9ca0fd867f7d/mergedshm                       64M     0   64M    0% /var/lib/docker/containers/df1ee27e97c823d18fe76ca3a7195cd081e466527b3cf9169a9a9f816b4c3391/mounts/shm\n\n执行docker ps -a发现大量已经停止或退出的容器，需要清理空间。\n# docker system pruneWARNING! This will remove:        - all stopped containers        - all networks not used by at least one container        - all dangling images        - all build cacheAre you sure you want to continue? [y/N] yDeleted Containers:64d81b1dc03026478f0114b657d4cd938967868967e5e5ee9b8ea3d72814a736c5f180a1c5c2804d45d13d11f338d241e482ada9673bff0ba8cc18759ae4fcaeaa06316b5fd638c22fc41bd8f8ea4c511566c4c537c81bf5aad5f276ac6c2faece5048ebf25bd48bbe1372aaf38c68078a05c06157ffcb4a8e646470f647acb00c4b37413e3483366fb640b94ad33596aef149a50bc610a2d7206631592808717575ec0a092bc9d3dd1a54ac2e3b92ce0dd8f3f570565ecc918684b0c51087ea1e335ee77f4167c19f6e54a1728d3d49775f651d871009752b1cfb7ec9fd7867393bd6280ecf423804c27f264cfc95e72df7580dc6881bb1a249152b4554b10e63ad5c44a30c80645e84b16cc8e1009e46235bd7001153f8d8d3c4887ed18786ee8c92a30092e7c30338d2a6721d13f250c2a426947a29c2e2c30f9936c0e07acd49e432a75a5f9719e8436af70a40e9869e02db51c4f8c08f02ad26a6bdc3ba35435e8cd10d2dd9b26993da909aba31f4d52df3475b30ba7536d473004c7e916028cf4eac7090a89c05edad7e45096130ee7e3336f28d2c0e2a890111d800436e1d20022ecdd871a684b016c89d4d1122c9c5a00de998f921e2eacbd54da4826417077821f3fc76da1e436401b0ed58a842b8afec6134c3895d5f3c7ae8d069b49f44b5a26aad43d9d7b0f1ff57b659d4aab90d13477063160862018d3ba6f3Deleted Images:untagged: nginx@sha256:b2d89d0a210398b4d1120b3e3a7672c16a4ba09c2c4a0395f18b9f7999b768f2deleted: sha256:f7bb5701a33c0e572ed06ca554edca1bee96cbbc1f76f3b01c985de7e19d0657deleted: sha256:8f1984cf0de0461043fcabd6b2a2040325ac001d9897a242e0d80486fe71575edeleted: sha256:59d91a36ba4b720eadfbf346ddb825c2faa2d66cc7a915238eaf926c4b4b40eedeleted: sha256:556c5fb0d91b726083a8ce42e2faaed99f11bc68d3f70e2c7bbce87e7e0b3e10untagged: nginx@sha256:ee380d57f01eac528f59d031cbeff5eba98b077d25ebdc7d6aef6734f00a41b7deleted: sha256:231d40e811cd970168fb0c4770f2161aa30b9ba6fe8e68527504df69643aa145deleted: sha256:dc8adf8fa0fc82a56c32efac9d0da5f84153888317c88ab55123d9e71777bc62deleted: sha256:77fcff986d3b13762e4777046b9210a109fda20cb261bd3bbe5d7161d4e73c8eTotal reclaimed space: 183.4MB\n\n这样就删除了不用的容器和镜像了。\n","categories":["Docker"],"tags":["docker","kubernetes","k8s"]},{"title":"正则表达式删除行中从某字符串开始的所有内容","url":"/2020/02/%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%E5%88%A0%E9%99%A4%E8%A1%8C%E4%B8%AD%E4%BB%8E%E6%9F%90%E5%AD%97%E7%AC%A6%E4%B8%B2%E5%BC%80%E5%A7%8B%E7%9A%84%E6%89%80%E6%9C%89%E5%86%85%E5%AE%B9/","content":"假如一行的内容如下：\n开始 [Hello] 192.168.0.1\n我们要删除开始后面所有的内容。\n通过Nodepad++的替换功能，查找模式选择正则表达式。\n查找目标： \\[Hello\\].*\n替换为空格即可。\n","categories":["Regex"],"tags":["regex","正则"]},{"title":"Chrome浏览器报错喔唷,崩溃啦","url":"/2020/03/Chrome%E6%B5%8F%E8%A7%88%E5%99%A8%E6%8A%A5%E9%94%99%E5%96%94%E5%94%B7-%E5%B4%A9%E6%BA%83%E5%95%A6/","content":"在Win10环境，Chrome版本为80时，会报错：喔唷，崩溃啦。\n在 mac 环境没有遇到这种情况。\n解决方法：\n找到 Chrome 的快捷键，在执行命令的后面，加入如下参数：\n--disable-features=RendererCodeIntegrity\n\n再次打开，就可以了。\n","categories":["Tools","Chrome"],"tags":["chrome","win10"]},{"title":"docker镜像仓库harbor搭建","url":"/2020/02/docker%E9%95%9C%E5%83%8F%E4%BB%93%E5%BA%93harbor%E6%90%AD%E5%BB%BA/","content":"本篇文章将介绍高可用docker镜像仓库的搭建。\n下载harbor安装文件https://github.com/goharbor/harbor/releases\n找到最新发行版本，v1.9.4的harbor-offline-installer-v1.9.4.tgz\nwget https://github.com/goharbor/harbor/releases/download/v1.9.4/harbor-offline-installer-v1.9.4.tgz\n\n文件有点大，要下载一会儿。\n安装harbor先解压\ntar -xvf harbor-offline-installer-v1.9.4.tgzcd harbor\n\n当直接执行install.sh安装命令时，会报如下错误：\n./install.sh➜ Please set hostname and other necessary attributes in harbor.yml first. DO NOT use localhost or 127.0.0.1 for hostname, because Harbor needs to be accessed by external clients.Please set --with-notary if needs enable Notary in Harbor, and set ui_url_protocol/ssl_cert/ssl_cert_key in harbor.yml bacause notary must run under https.Please set --with-clair if needs enable Clair in HarborPlease set --with-chartmuseum if needs enable Chartmuseum in Harbor\n\n所以先修改harbor.yml\nvi harbor.yml\n\n把hostname改为当前服务器的ip\n#hostname: reg.mydomain.comhostname: 172.16.64.233\n\n同时需要注意一下，harbor的数据默认是存放在这个位置的：\n# The default data volumedata_volume: /data\n\n我们还可以看到管理员登录的账号和密码也是在这里配置的。\n# The initial password of Harbor admin# It only works in first time to install harbor# Remember Change the admin password from UI after launching Harbor.harbor_admin_password: Harbor12345\n\n如果要换到其他地方存放数据，需要修改这个配置。\n再次运行安装命令，又会报如下错误：\n./install.sh[Step 0]: checking installation environment ...Note: docker version: 18.06.3✖ Need to install docker-compose(1.18.0+) by yourself first and run this script again.\n\n所以需要安装docker-compose，具体方法请参考 CentOS环境安装Docker\nyum install docker-compose\n\n再次执行安装\n./install.sh[Step 0]: checking installation environment ...Note: docker version: 18.06.3Note: docker-compose version: 1.18.0[Step 1]: loading Harbor images ...62b223a46a15: Loading layer [==================================================&gt;]  34.29MB/34.29MB40f95e7c4d8c: Loading layer [==================================================&gt;]  12.77MB/12.77MB87bc69f1a650: Loading layer [==================================================&gt;]  55.42MB/55.42MB2d7b6446b66d: Loading layer [==================================================&gt;]  5.632kB/5.632kB4fff34e50f40: Loading layer [==================================================&gt;]  37.38kB/37.38kB5e79cfafc57c: Loading layer [==================================================&gt;]  55.42MB/55.42MBLoaded image: goharbor/harbor-core:v1.9.457c193635092: Loading layer [==================================================&gt;]  115.8MB/115.8MB48c741dd71e6: Loading layer [==================================================&gt;]  12.23MB/12.23MBbca1df60136e: Loading layer [==================================================&gt;]  2.048kB/2.048kB3ded12c0b4d9: Loading layer [==================================================&gt;]  48.13kB/48.13kB1ab30734b178: Loading layer [==================================================&gt;]  3.072kB/3.072kB09dcb0a00864: Loading layer [==================================================&gt;]  12.28MB/12.28MBLoaded image: goharbor/clair-photon:v2.1.0-v1.9.4b3a6b161a0f0: Loading layer [==================================================&gt;]  7.039MB/7.039MB1ed6312f133c: Loading layer [==================================================&gt;]  196.6kB/196.6kBfee283579213: Loading layer [==================================================&gt;]    172kB/172kB1946b2964bfc: Loading layer [==================================================&gt;]  15.36kB/15.36kB026952c4573d: Loading layer [==================================================&gt;]  3.584kB/3.584kB37bd829992ae: Loading layer [==================================================&gt;]  10.84MB/10.84MBLoaded image: goharbor/harbor-portal:v1.9.40fa4e197a1e0: Loading layer [==================================================&gt;]  10.84MB/10.84MBLoaded image: goharbor/nginx-photon:v1.9.49f4e1ee20fe3: Loading layer [==================================================&gt;]  9.009MB/9.009MBeb044190906a: Loading layer [==================================================&gt;]  42.31MB/42.31MB04e55b2b95d5: Loading layer [==================================================&gt;]  2.048kB/2.048kB41efcb18a521: Loading layer [==================================================&gt;]  3.072kB/3.072kB16903b9eaf51: Loading layer [==================================================&gt;]  42.31MB/42.31MBLoaded image: goharbor/chartmuseum-photon:v0.9.0-v1.9.419ccaba72e02: Loading layer [==================================================&gt;]   2.56kB/2.56kBbf35853b2d08: Loading layer [==================================================&gt;]  1.536kB/1.536kBfacfe762a35b: Loading layer [==================================================&gt;]  75.33MB/75.33MBcfcf6d4e7653: Loading layer [==================================================&gt;]  42.65MB/42.65MBc497e06e4e96: Loading layer [==================================================&gt;]  157.2kB/157.2kBf444f52f4af6: Loading layer [==================================================&gt;]   3.01MB/3.01MBLoaded image: goharbor/prepare:v1.9.412f86854bc80: Loading layer [==================================================&gt;]   80.2MB/80.2MB5e76c79bec2e: Loading layer [==================================================&gt;]  3.072kB/3.072kB694b3e7869d5: Loading layer [==================================================&gt;]   59.9kB/59.9kB27609e3dd221: Loading layer [==================================================&gt;]  61.95kB/61.95kBLoaded image: goharbor/redis-photon:v1.9.401c4d294000a: Loading layer [==================================================&gt;]  9.005MB/9.005MBe836fee5658c: Loading layer [==================================================&gt;]  3.072kB/3.072kBac9add5e34a0: Loading layer [==================================================&gt;]   2.56kB/2.56kB1af6a0c8f2bb: Loading layer [==================================================&gt;]  21.76MB/21.76MB692f3a6593bb: Loading layer [==================================================&gt;]  21.76MB/21.76MBLoaded image: goharbor/registry-photon:v2.7.1-patch-2819-2553-v1.9.450d697c7c241: Loading layer [==================================================&gt;]  9.004MB/9.004MBccc72da8223b: Loading layer [==================================================&gt;]  6.239MB/6.239MBd8f9724c0195: Loading layer [==================================================&gt;]  16.04MB/16.04MB813deff8bdee: Loading layer [==================================================&gt;]  28.24MB/28.24MB27eeaef358bd: Loading layer [==================================================&gt;]  22.02kB/22.02kB52a7091247e7: Loading layer [==================================================&gt;]  50.52MB/50.52MBLoaded image: goharbor/notary-server-photon:v0.6.1-v1.9.4391081d598f3: Loading layer [==================================================&gt;]  50.36MB/50.36MB6e82a6bf9097: Loading layer [==================================================&gt;]  3.584kB/3.584kBf44a631c4f72: Loading layer [==================================================&gt;]  3.072kB/3.072kBc841d5236832: Loading layer [==================================================&gt;]   2.56kB/2.56kB253571258f05: Loading layer [==================================================&gt;]  3.072kB/3.072kB88d0f16c60e6: Loading layer [==================================================&gt;]  3.584kB/3.584kBa4499b4f52d1: Loading layer [==================================================&gt;]  12.29kB/12.29kBLoaded image: goharbor/harbor-log:v1.9.4b94d1cd23704: Loading layer [==================================================&gt;]  63.49MB/63.49MB7108c9c351ce: Loading layer [==================================================&gt;]  56.37MB/56.37MB013a04104e87: Loading layer [==================================================&gt;]  5.632kB/5.632kBc91b8f37358e: Loading layer [==================================================&gt;]  2.048kB/2.048kBaa04ed1247cb: Loading layer [==================================================&gt;]   2.56kB/2.56kBea3d1b630734: Loading layer [==================================================&gt;]   2.56kB/2.56kB9dd11aa8e16a: Loading layer [==================================================&gt;]   2.56kB/2.56kB69fab71b0bc5: Loading layer [==================================================&gt;]  10.24kB/10.24kBLoaded image: goharbor/harbor-db:v1.9.4573233339cd8: Loading layer [==================================================&gt;]  12.77MB/12.77MB1c4fa76d32f8: Loading layer [==================================================&gt;]  48.14MB/48.14MBLoaded image: goharbor/harbor-jobservice:v1.9.4401a522fd2d5: Loading layer [==================================================&gt;]  9.005MB/9.005MB93bfd55aee1a: Loading layer [==================================================&gt;]  3.072kB/3.072kB41bc5eeff535: Loading layer [==================================================&gt;]  21.76MB/21.76MB768368529512: Loading layer [==================================================&gt;]  3.072kB/3.072kBd0f0f102247d: Loading layer [==================================================&gt;]  8.661MB/8.661MB32b9c7908fb4: Loading layer [==================================================&gt;]  30.42MB/30.42MBLoaded image: goharbor/harbor-registryctl:v1.9.4f8bf76e63a50: Loading layer [==================================================&gt;]  14.61MB/14.61MB9cc53a4748a9: Loading layer [==================================================&gt;]  28.24MB/28.24MBae2e3edc6219: Loading layer [==================================================&gt;]  22.02kB/22.02kB43599ec252a3: Loading layer [==================================================&gt;]  49.09MB/49.09MBLoaded image: goharbor/notary-signer-photon:v0.6.1-v1.9.4713f7d39cadb: Loading layer [==================================================&gt;]  338.3MB/338.3MBdc092fe63769: Loading layer [==================================================&gt;]  119.8kB/119.8kBLoaded image: goharbor/harbor-migrator:v1.9.4[Step 2]: preparing environment ...prepare base dir is set to /usr/local/software/harborGenerated configuration file: /config/log/logrotate.confGenerated configuration file: /config/log/rsyslog_docker.confGenerated configuration file: /config/nginx/nginx.confGenerated configuration file: /config/core/envGenerated configuration file: /config/core/app.confGenerated configuration file: /config/registry/config.ymlGenerated configuration file: /config/registryctl/envGenerated configuration file: /config/db/envGenerated configuration file: /config/jobservice/envGenerated configuration file: /config/jobservice/config.ymlGenerated and saved secret to file: /secret/keys/secretkeyCreating harbor-log ... doneGenerated configuration file: /compose_location/docker-compose.ymlClean up the input dirCreating registry ... doneCreating harbor-core ... doneCreating network &quot;harbor_harbor&quot; with the default driverCreating nginx ... doneCreating registry ...Creating redis ...Creating harbor-db ...Creating harbor-portal ...Creating registryctl ...Creating harbor-core ...Creating nginx ...Creating harbor-jobservice ...✔ ----Harbor has been installed and started successfully.----Now you should be able to visit the admin portal at http://172.16.64.233 .For more details, please visit https://github.com/goharbor/harbor .\n\n看到这些信息就算是成功安装了。如下所示，访问http://172.16.64.233就可以了。\n\n\n通过用户名admin和默认密码Harbor12345，可以成功登录。\n\n\n关闭harbor进入harbor目录\ndocker-compose downStopping harbor-jobservice ... doneStopping nginx             ... doneStopping harbor-core       ... doneStopping registryctl       ... doneStopping harbor-db         ... doneStopping harbor-portal     ... doneStopping registry          ... doneStopping redis             ... doneStopping harbor-log        ... doneRemoving harbor-jobservice ... doneRemoving nginx             ... doneRemoving harbor-core       ... doneRemoving registryctl       ... doneRemoving harbor-db         ... doneRemoving harbor-portal     ... doneRemoving registry          ... doneRemoving redis             ... doneRemoving harbor-log        ... doneRemoving network harbor_harbor\n","categories":["Docker","Harbor"],"tags":["docker","image","harbor"]},{"title":"CentOS上搭建NFS服务","url":"/2020/03/CentOS%E4%B8%8A%E6%90%AD%E5%BB%BANFS%E6%9C%8D%E5%8A%A1/","content":"为了配合设置k8s的存储，这篇文章讲解一下如何搭建NFS服务。\n为了测试，我们暂时在k8s-master节点(172.16.64.233&#x2F;24)来安装NFS服务，数据共享目录设置为/data/nfs。\n关闭防火墙k8s集群其实都已经关了的，如果在其他机器上，先关掉。\n# systemctl stop firewalld# systemctl disable firewalld\n\n安装NFS通过yum安装nfs-utils，这个组件会依赖rpcbind，也会自动被安装上。\n# yum install nfs-utils\n\n设置共享目录权限# chmod 755 /data/nfs/\n\n配置NFSNFS的默认配置文件在&#x2F;etc&#x2F;exports文件里，在该文件中添加如下内容：\n# vi /etc/exports/data/nfs *(rw,sync,no_root_squash)\n\n\n&#x2F;data&#x2F;nfs：是共享的数据目录\n\n*：表示任何人都有权限连接，当然也可以是一个网段，一个IP，也可以是域名rw：读写的权限sync：表示文件同时写入硬盘和内存no_root_squash：当登录NFS主机使用共享目录的使用者是root时，其权限将被转换成为匿名使用者，通常它的UID与GID，都会变成nobody身份\n\n\n启动NFS注意启动顺序，先启动rpcbind。\n启动服务 nfs 需要向 rpc 注册，rpc 一旦重启了，注册的文件都会丢失，向他注册的服务都需要重启。\n# systemctl enable rpcbind# systemctl start rpcbind# systemctl status rpcbind● rpcbind.service - RPC bind service   Loaded: loaded (/usr/lib/systemd/system/rpcbind.service; enabled; vendor preset: enabled)   Active: active (running) since 日 2020-03-08 13:24:03 CST; 5s ago  Process: 114504 ExecStart=/sbin/rpcbind -w $RPCBIND_ARGS (code=exited, status=0/SUCCESS) Main PID: 114505 (rpcbind)    Tasks: 1   Memory: 748.0K   CGroup: /system.slice/rpcbind.service           └─114505 /sbin/rpcbind -w3月 08 13:24:03 k8s-master systemd[1]: Starting RPC bind service...3月 08 13:24:03 k8s-master systemd[1]: Started RPC bind service.\n\nRPC bind服务Started说明RPC启动成功。\n然后启动NFS服务：\n# systemctl enable nfsCreated symlink from /etc/systemd/system/multi-user.target.wants/nfs-server.service to /usr/lib/systemd/system/nfs-server.service.# systemctl start nfs# systemctl status nfs● nfs-server.service - NFS server and services   Loaded: loaded (/usr/lib/systemd/system/nfs-server.service; enabled; vendor preset: disabled)  Drop-In: /run/systemd/generator/nfs-server.service.d           └─order-with-mounts.conf   Active: active (exited) since 日 2020-03-08 13:25:57 CST; 6s ago  Process: 116906 ExecStartPost=/bin/sh -c if systemctl -q is-active gssproxy; then systemctl reload gssproxy ; fi (code=exited, status=0/SUCCESS)  Process: 116889 ExecStart=/usr/sbin/rpc.nfsd $RPCNFSDARGS (code=exited, status=0/SUCCESS)  Process: 116887 ExecStartPre=/usr/sbin/exportfs -r (code=exited, status=0/SUCCESS) Main PID: 116889 (code=exited, status=0/SUCCESS)    Tasks: 0   Memory: 0B   CGroup: /system.slice/nfs-server.service3月 08 13:25:57 k8s-master systemd[1]: Starting NFS server and services...3月 08 13:25:57 k8s-master systemd[1]: Started NFS server and services.\n\nNFS server成功启动。\n我们还可以通过下面的命令确认NFS服务启动成功。\n# rpcinfo -p | grep nfs    100003    3   tcp   2049  nfs    100003    4   tcp   2049  nfs    100227    3   tcp   2049  nfs_acl    100003    3   udp   2049  nfs    100003    4   udp   2049  nfs    100227    3   udp   2049  nfs_acl\n\n查看具体的挂载权限：\n# cat /var/lib/nfs/etab/data/nfs\t*(rw,sync,wdelay,hide,nocrossmnt,secure,no_root_squash,no_all_squash,no_subtree_check,secure_locks,acl,no_pnfs,anonuid=65534,anongid=65534,sec=sys,rw,secure,no_root_squash,no_all_squash)\n\n查看共享目录在其他机器上，可以检查NFS是否有共享目录\nshowmount -e 172.16.64.233Exports list on 172.16.64.233:/data/nfs *\n","categories":["FileSystem","NFS"],"tags":["centos","kubernetes","k8s","showmount","nfs"]},{"title":"Git远程仓库版本回退","url":"/2020/03/Git%E8%BF%9C%E7%A8%8B%E4%BB%93%E5%BA%93%E7%89%88%E6%9C%AC%E5%9B%9E%E9%80%80/","content":"在 Git版本如何回退 我们知道了，如果 commit 以后，想要回到原来的版本使用 git reset 命令就可以。\n在 SourceTree 工具里面，选中想要回退的版本，右键点击 Reset master to this commit 也可以解决。\n这里想讲一下如何已经提交到远程仓库了怎么办？\n首先，还是和前面的一样，让本地 master 版本先回退回来。\n如图所示：可以看到 master 版本已经落后远程仓库一个版本了。\n\n\n然后，在命令行里面，执行 git push -f 这个命令，我在 SourceTree 里面还没有找到在哪里。force push 到原程仓库里面。就可以了。\n如图 \n版本就在同一条线上了。\n","categories":["Tools","Git"],"tags":["git","github","gitlab","reset","commit"]},{"title":"Grafana安装及持久化","url":"/2020/03/Grafana%E5%AE%89%E8%A3%85%E5%8F%8A%E6%8C%81%E4%B9%85%E5%8C%96/","content":"这篇文章主要讲解在kubernetes集群中如何安装Grafana和持久化Grafana数据。\n使用的镜像是image: grafana/grafana:6.3.3\n安装Grafana使用的yaml文件是安装kube-prometheus里面的。安装 kube-prometheus，请参考 Kubernetes监控方案kube-prometheus(prometheus, node-exporter, grafana)\n不过需要对grafana/grafana-deployment.yaml做一些自定义化的修改。\n设置管理员在containers下级添加GF_SECURITY_ADMIN_USER和GF_SECURITY_ADMIN_PASSWORD，用来配置 grafana 的管理员用户和密码\nspec:  containers:  - image: grafana/grafana:6.3.3    ...    env:    - name: GF_SECURITY_ADMIN_USER      value: admin    - name: GF_SECURITY_ADMIN_PASSWORD      value: &quot;123456&quot;\n\n如果要设置Grafana的匿名登录，请参考Grafana匿名登录Kubernetes设置\n- name: GF_AUTH_ANONYMOUS_ENABLED  value: &quot;true&quot;- name: GF_AUTH_ANONYMOUS_ORG_ROLE  value: Viewer\n\n数据持久化原始的数据是以emptyDir形式存放在pod里面，生命周期与pod相同，出现问题时，容器重启，在Grafana里面设置的数据就全部消失了。\n  volumeMounts:  - mountPath: /var/lib/grafana    name: grafana-storage    readOnly: false...volumes:- emptyDir: &#123;&#125;  name: grafana-storage\n\n可以看出Grafana将dashboard、插件这些数据保存在&#x2F;var&#x2F;lib&#x2F;grafana这个目录下面。做持久化的话，就需要对这个目录进行volume挂载声明。\n把emptyDir修改为pvc方式。\nvolumes:- name: grafana-storage  persistentVolumeClaim:    claimName: grafana\n\n如果要使用一个 pvc 对象来持久化数据，我们就需要添加一个可用的 pv 供 pvc 绑定使用，grafana-volume.yaml内容如下：\napiVersion: v1kind: PersistentVolumemetadata:  name: grafanaspec:  capacity:    storage: 1Gi  accessModes:  - ReadWriteOnce  persistentVolumeReclaimPolicy: Recycle  nfs:    server: 172.16.64.233    path: /data/nfs---apiVersion: v1kind: PersistentVolumeClaimmetadata:  name: grafana  namespace: monitoringspec:  accessModes:  - ReadWriteOnce  resources:    requests:      storage: 1Gi\n\n暴露Service的方法，在安装 kube-prometheus 的时候已经讲过了。Kubernetes监控方案kube-prometheus(prometheus, node-exporter, grafana)\n# kubectl apply -f grafana-volume.yamlpersistentvolume/grafana createdpersistentvolumeclaim/grafana created# kubectl apply -f grafana-deployment.yamldeployment.apps/grafana configured\n\n查看 pod 状态：\n# kubectl get po -n monitoringNAME                                  READY   STATUS              RESTARTS   AGE...grafana-6668cd66cb-ff8kg              0/1     ContainerCreating   0          2m32s\n\n进一步查看描述信息：\n# kubectl describe po grafana-6668cd66cb-ff8kg -n monitoringEvents:  Type     Reason       Age    From                Message  ----     ------       ----   ----                -------  Normal   Scheduled    3m57s  default-scheduler   Successfully assigned monitoring/grafana-6668cd66cb-ff8kg to k8s-node2  Warning  FailedMount  3m54s  kubelet, k8s-node2  MountVolume.SetUp failed for volume &quot;grafana&quot; : mount failed: exit status 32Mounting command: systemd-runMounting arguments: --description=Kubernetes transient mount for /var/lib/kubelet/pods/fc4d98ef-0b80-4570-a487-4ded9133eb58/volumes/kubernetes.io~nfs/grafana --scope -- mount -t nfs 172.16.64.233:/data/nfs /var/lib/kubelet/pods/fc4d98ef-0b80-4570-a487-4ded9133eb58/volumes/kubernetes.io~nfs/grafanaOutput: Running scope as unit run-54024.scope.mount: wrong fs type, bad option, bad superblock on 172.16.64.233:/data/nfs,       missing codepage or helper program, or other error       (for several filesystems (e.g. nfs, cifs) you might       need a /sbin/mount.&lt;type&gt; helper program)       In some cases useful info is found in syslog - try       dmesg | tail or so.  Warning  FailedMount  3m53s  kubelet, k8s-node2  MountVolume.SetUp failed for volume &quot;grafana&quot; : mount failed: exit status 32\n\n说明是 mount 失败了。原因是我们需要在每个节点，都要安装 NFS 客户端。在生产环境下，如何批量操作 terminal 终端？请参考：批量操作Shell终端\n在每个node下面都安装好NFS客户端以后，删除掉出错的 pod。\n查看 pod:\nkubectl get po -n monitoringNAME                                  READY   STATUS             RESTARTS   AGEgrafana-6668cd66cb-jrnjv              0/1     CrashLoopBackOff   4          2m53s\n\n没有正常启动，我们再看一下这个 Pod 的日志：\n# kubectl logs grafana-6668cd66cb-jrnjv -n monitoringGF_PATHS_DATA=&#x27;/var/lib/grafana&#x27; is not writable.You may have issues with file permissions, more information here: http://docs.grafana.org/installation/docker/#migration-from-a-previous-version-of-the-docker-container-to-5-1-or-latermkdir: cannot create directory &#x27;/var/lib/grafana/plugins&#x27;: Permission denied\n\n这个错误是 Grafana 5.1版本以后才会出现的。错误的原因很明显，就是 /var/lib/grafana 目录的权限不够。\n在 &#96;&#96; 中有这样一个属性：\nsecurityContext:  runAsNonRoot: true  runAsUser: 65534\n\n我们查看一下65534是哪个用户：\n# cat /etc/passwd | grep 65534nfsnobody:x:65534:65534:Anonymous NFS User:/var/lib/nfs:/sbin/nologin\n\n所以，你只需要把 /data/nfs 目录的用户改为 nfsnobody 就可以了。当然把属性改为 777 也没问题。\nchown nfsnobody /data/nfs\n\n把刚才出错的那个 Pod 删除，新的  Grafana Pod 就成功启动了。\n\n\n然后可以添加 Dashboard 了，哪怕 Pod 重建也不会丢失数据了。\n","categories":["Kubernetes","Grafana"],"tags":["kubernetes","k8s","grafana","prometheus","monitoring"]},{"title":"Grafana各类指标理解","url":"/2020/03/Grafana%E5%90%84%E7%B1%BB%E6%8C%87%E6%A0%87%E7%90%86%E8%A7%A3/","content":"今天来梳理一下Grafana图表及其后面的公式。\nKubernetes &#x2F; Compute Resources &#x2F; ClusterCPU Utilisation\n\n1 - avg(rate(node_cpu_seconds_total&#123;mode=&quot;idle&quot;, cluster=&quot;&quot;&#125;[1m]))\n\n在prometheus上面查询指标\nnode_cpu_seconds_total&#123;mode=&quot;idle&quot;&#125;\nnode_cpu_seconds_total&#123;cpu=&quot;0&quot;,endpoint=&quot;https&quot;,instance=&quot;k8s-master&quot;,job=&quot;node-exporter&quot;,mode=&quot;idle&quot;,namespace=&quot;monitoring&quot;,pod=&quot;node-exporter-t9ljw&quot;,service=&quot;node-exporter&quot;&#125;\t3102.08node_cpu_seconds_total&#123;cpu=&quot;0&quot;,endpoint=&quot;https&quot;,instance=&quot;k8s-node1&quot;,job=&quot;node-exporter&quot;,mode=&quot;idle&quot;,namespace=&quot;monitoring&quot;,pod=&quot;node-exporter-7vq8n&quot;,service=&quot;node-exporter&quot;&#125;\t3046.73node_cpu_seconds_total&#123;cpu=&quot;0&quot;,endpoint=&quot;https&quot;,instance=&quot;k8s-node2&quot;,job=&quot;node-exporter&quot;,mode=&quot;idle&quot;,namespace=&quot;monitoring&quot;,pod=&quot;node-exporter-vg596&quot;,service=&quot;node-exporter&quot;&#125;\t3069.61node_cpu_seconds_total&#123;cpu=&quot;1&quot;,endpoint=&quot;https&quot;,instance=&quot;k8s-master&quot;,job=&quot;node-exporter&quot;,mode=&quot;idle&quot;,namespace=&quot;monitoring&quot;,pod=&quot;node-exporter-t9ljw&quot;,service=&quot;node-exporter&quot;&#125;\t3096.23\n\n所以CPU Utilisation算的是各节点CPU利用率的平均值。\njob&#x3D;”node-exporter”\nCPU Usage\n\nsum(namespace_pod_container:container_cpu_usage_seconds_total:sum_rate&#123;cluster=&quot;&quot;&#125;) by (namespace)\n\n在prometheus上面查询指标\nnamespace_pod_container:container_cpu_usage_seconds_total:sum_rate&#123;cluster=&quot;&quot;&#125;\nnamespace_pod_container:container_cpu_usage_seconds_total:sum_rate&#123;container=&quot;addon-resizer&quot;,namespace=&quot;monitoring&quot;,pod=&quot;kube-state-metrics-65d5b4b99d-llrjd&quot;&#125;\t0.00022111388787432652namespace_pod_container:container_cpu_usage_seconds_total:sum_rate&#123;container=&quot;alertmanager&quot;,namespace=&quot;monitoring&quot;,pod=&quot;alertmanager-main-0&quot;&#125;\t0.00275714828677409namespace_pod_container:container_cpu_usage_seconds_total:sum_rate&#123;container=&quot;alertmanager&quot;,namespace=&quot;monitoring&quot;,pod=&quot;alertmanager-main-1&quot;&#125;\t0.0029093557196228424namespace_pod_container:container_cpu_usage_seconds_total:sum_rate&#123;container=&quot;alertmanager&quot;,namespace=&quot;monitoring&quot;,pod=&quot;alertmanager-main-2&quot;&#125;\t0.0027905491021107728namespace_pod_container:container_cpu_usage_seconds_total:sum_rate&#123;container=&quot;calico-kube-controllers&quot;,namespace=&quot;kube-system&quot;,pod=&quot;calico-kube-controllers-5598cf8794-8mgdz&quot;&#125;\t0.0009434578301088127namespace_pod_container:container_cpu_usage_seconds_total:sum_rate&#123;container=&quot;calico-node&quot;,namespace=&quot;kube-system&quot;,pod=&quot;calico-node-jtvh8&quot;&#125;\t0.017518785546616542namespace_pod_container:container_cpu_usage_seconds_total:sum_rate&#123;container=&quot;calico-node&quot;,namespace=&quot;kube-system&quot;,pod=&quot;calico-node-k6m8t&quot;&#125;\t0.022689515968190806namespace_pod_container:container_cpu_usage_seconds_total:sum_rate&#123;container=&quot;calico-node&quot;,namespace=&quot;kube-system&quot;,pod=&quot;calico-node-rb9qx&quot;&#125;\t0.01819155156978804namespace_pod_container:container_cpu_usage_seconds_total:sum_rate&#123;container=&quot;config-reloader&quot;,namespace=&quot;monitoring&quot;,pod=&quot;alertmanager-main-0&quot;&#125;\t0.0000029602042055748096namespace_pod_container:container_cpu_usage_seconds_total:sum_rate&#123;container=&quot;config-reloader&quot;,namespace=&quot;monitoring&quot;,pod=&quot;alertmanager-main-1&quot;&#125;\t0.0000024442638833786885\n\nMemorysum(container_memory_rss&#123;cluster=&quot;&quot;, container!=&quot;&quot;&#125;) by (namespace)\n\n在prometheus上面查询指标\ncontainer_memory_rss\ncontainer_memory_rss&#123;container=&quot;POD&quot;,container_name=&quot;POD&quot;,endpoint=&quot;https-metrics&quot;,id=&quot;/kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-pod1a047e8b0c961b34e915140fc2a8711c.slice/docker-20e1377aeb77873fcf4ac5e4380d47f28c0f594773ba047442b00dfc6f116837.scope&quot;,image=&quot;k8s.gcr.io/pause:3.1&quot;,instance=&quot;172.16.64.233:10250&quot;,job=&quot;kubelet&quot;,name=&quot;k8s_POD_etcd-k8s-master_kube-system_1a047e8b0c961b34e915140fc2a8711c_14&quot;,namespace=&quot;kube-system&quot;,node=&quot;k8s-master&quot;,pod=&quot;etcd-k8s-master&quot;,pod_name=&quot;etcd-k8s-master&quot;,service=&quot;kubelet&quot;&#125;\t45056container_memory_rss&#123;container=&quot;POD&quot;,container_name=&quot;POD&quot;,endpoint=&quot;https-metrics&quot;,id=&quot;/kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-pod31622d49_04a8_4a95_8b80_736012e85215.slice/docker-c4e100bf3571c0fa25537cbd0ea7839bf2fd1486462a7b2626a552dfcf7503ec.scope&quot;,image=&quot;k8s.gcr.io/pause:3.1&quot;,instance=&quot;172.16.64.232:10250&quot;,job=&quot;kubelet&quot;,name=&quot;k8s_POD_nginx-deployment-6f89946645-pwpf7_default_31622d49-04a8-4a95-8b80-736012e85215_13&quot;,namespace=&quot;default&quot;,node=&quot;k8s-node1&quot;,pod=&quot;nginx-deployment-6f89946645-pwpf7&quot;,pod_name=&quot;nginx-deployment-6f89946645-pwpf7&quot;,service=&quot;kubelet&quot;&#125;\t45056\n\njob&#x3D;”kubelet”\nKubernetes &#x2F; Compute Resources &#x2F; Namespace (Pods)\n\nsum(namespace_pod_container:container_cpu_usage_seconds_total:sum_rate&#123;cluster=&quot;&quot;, namespace=&quot;monitoring&quot;&#125;) by (pod)\n\nKubernetes &#x2F; Compute Resources &#x2F; Pod\n\n显示各个选中Pod中，各个Container的状态。\nsum(namespace_pod_container:container_cpu_usage_seconds_total:sum_rate{namespace&#x3D;”default”, pod&#x3D;”nginx-deployment-6f89946645-pwpf7”, container!&#x3D;”POD”, cluster&#x3D;””}) by (container)\n现在的逻辑就是要把container的指标打上pod的标签\n- job_name: &#x27;kubernetes-cadvisor&#x27;  scheme: https  tls_config:    ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt  bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token  kubernetes_sd_configs:  - role: node  relabel_configs:  - target_label: __address__    replacement: kubernetes.default.svc:443  - source_labels: [__meta_kubernetes_node_name]    regex: (.+)    replacement: /api/v1/nodes/$&#123;1&#125;/proxy/metrics/cadvisor    target_label: __metrics_path__  - action: labelmap    regex: __meta_kubernetes_node_label_(.+)\n","categories":["Kubernetes","Grafana"],"tags":["kubernetes","k8s","grafana","prometheus"]},{"title":"IntelliJ IDEA新建Maven Springboot项目Cannot find JRE '1.8'","url":"/2020/03/IntelliJ-IDEA%E6%96%B0%E5%BB%BAMaven-Springboot%E9%A1%B9%E7%9B%AECannot-find-JRE-1-8/","content":"通过 IntelliJ IDEA 新建一个 Maven Springboot 项目，建立好之后，发现生成代码的包无法引入，代码已经提示红色标记错误。\n\n\n点击按钮 Reimport All Maven Projects 或执行 maven clean, package 这类命令，都会提示如下错误：\nError running &#39;demo [validate]&#39;: Cannot find JRE &#39;1.8&#39;. You can specify JRE to run maven goals in Settings | Maven | Runner\n\n\n按上述提示，进入 Settings | Maven | Runner，把正常的 JRE 环境选中即可。\n\n\n上面是设置当前项目的，如果要设置以后所有新建的项目，则需要关闭所有项目后，再进入 Preferences\n\n\n\n","categories":["Tools","IntelliJ IDEA"],"tags":["maven","spring boot","intellij idea","jre"]},{"title":"Grafana迁移(导出导入)dashboard","url":"/2020/03/Grafana%E8%BF%81%E7%A7%BB-%E5%AF%BC%E5%87%BA%E5%AF%BC%E5%85%A5-dashboard/","content":"我们在Grafana测试环境，搭建好了一个Dashboard，现在想把它放到生产环境中运行。这时，就需要用到Grafana的导出导入功能。\n导出Dashboard右上角，点击Share Dashboard图标。\n\n\n同时，选中第三个标签Export，最后点击Save to file按钮。这时会把Dashboard保存到一个json文件中，并下载到本地。\n导入点击Import菜单\n\n\n点击Upload .json File按钮，上传刚才保存的json文件。或者在Or paste JSON文本框中，粘贴刚才下载的json的内容后，再点击Load按钮。\n\n","categories":["Kubernetes","Grafana"],"tags":["kubernetes","k8s","grafana"]},{"title":"Intellij IDEA从SVN导入工程","url":"/2020/03/Intellij-IDEA%E4%BB%8ESVN%E5%AF%BC%E5%85%A5%E5%B7%A5%E7%A8%8B/","content":"我们一般都把代码托管到github，公司也会托管自己搭建的gitlab上面，但遇到有些技术稍微陈旧的东西，还在使用SVN。\n如何把SVN中的 Maven 项目导入到 IDEA 中呢？\n有两个入口，但都需要选择 Check out from Version Control\n\n\n\n\n上面两个入口，都会创建项目结构。\n下面这个不行，因为不会创建项目结构。\n\n\n用上述方式，会有创建项目的流程。\n\n\n如果SVN已经提前checkout出来了。\n如果是项目\n\n\n如果是子模块，或者是项目的不同版本。\n\n\n导入Maven项目\n\n\n要选中这个\n\n","categories":["Tools","IntelliJ IDEA"],"tags":["intellij idea","svn"]},{"title":"Java URL参数替换","url":"/2020/03/Java-URL%E5%8F%82%E6%95%B0%E6%9B%BF%E6%8D%A2/","content":"在 Java 程序中，向第三方发起一个 http 请求时，这时往往需要拼接第三方接口的 URL。\n通过字符串+或者 StringBuilder 的 append 方法，都不算是一个最佳实践。因为上述这两种方法都没办法把整个 URL 格式从代码中分离出来。\n我觉得最佳实践是通过 String.format() 为 URL 做字符替换。\n比如接口的 URL http://www.example.com/service?appid=%d&amp;sign=%s\n%d 为数字，%s 为字符串。\n我们可以调用 String.format() 方法做参数替换。\nString url = String.format(url, 1000, &quot;mySign&quot;);\n\n结果为：http://www.example.com/service?appid=1000&amp;sign=mySign\n","categories":["Java"],"tags":["java"]},{"title":"Linux(CentOS)根据进程查找程序位置","url":"/2020/03/Linux-CentOS-%E6%A0%B9%E6%8D%AE%E8%BF%9B%E7%A8%8B%E6%9F%A5%E6%89%BE%E7%A8%8B%E5%BA%8F%E4%BD%8D%E7%BD%AE/","content":"我们可能经常会遇到这种情况，登录到服务器上，想知道某个程序的位置在哪里，或者这个程序加载的配置文件位置在哪里。\n首先，查看进程信息，比如我要查看 redis 程序运行的位置。\n# ps -ef | grep redisroot      31087      1  0 Mar24 ?        00:06:56 ./redis-server *:6379root     114543 112619  0 19:16 pts/2    00:00:00 grep --color=auto redis\n\n获取到了进程号：31087\n# ll /proc/31087total 0dr-xr-xr-x 2 root root 0 Mar 30 19:01 attr-r-------- 1 root root 0 Mar 30 19:01 auxv-r--r--r-- 1 root root 0 Mar 30 19:01 cgroup--w------- 1 root root 0 Mar 30 19:01 clear_refs-r--r--r-- 1 root root 0 Mar 24 10:30 cmdline-rw-r--r-- 1 root root 0 Mar 30 19:01 comm-rw-r--r-- 1 root root 0 Mar 30 19:01 coredump_filter-r--r--r-- 1 root root 0 Mar 30 19:01 cpusetlrwxrwxrwx 1 root root 0 Mar 30 19:01 cwd -&gt; /redis/bin-r-------- 1 root root 0 Mar 30 19:01 environlrwxrwxrwx 1 root root 0 Mar 24 10:30 exe -&gt; /redis/bin/redis-serverdr-x------ 2 root root 0 Mar 30 19:01 fddr-x------ 2 root root 0 Mar 30 19:01 fdinfo-rw-r--r-- 1 root root 0 Mar 30 19:01 gid_map-r-------- 1 root root 0 Mar 30 19:01 io-r-------- 1 root root 0 Mar 30 19:01 kgr_in_progress-r--r--r-- 1 root root 0 Mar 30 19:01 latency-r--r--r-- 1 root root 0 Mar 30 19:01 limits-rw-r--r-- 1 root root 0 Mar 30 19:01 loginuid-rw-r--r-- 1 root root 0 Mar 30 19:01 make-it-faildr-x------ 2 root root 0 Mar 30 19:01 map_files-r--r--r-- 1 root root 0 Mar 30 19:01 maps-rw------- 1 root root 0 Mar 30 19:01 mem-r--r--r-- 1 root root 0 Mar 30 19:01 mountinfo-r--r--r-- 1 root root 0 Mar 30 19:01 mounts-r-------- 1 root root 0 Mar 30 19:01 mountstatsdr-xr-xr-x 5 root root 0 Mar 30 19:01 netdr-x--x--x 2 root root 0 Mar 30 19:01 ns-r--r--r-- 1 root root 0 Mar 30 19:01 numa_maps-rw-r--r-- 1 root root 0 Mar 30 19:01 oom_adj-r--r--r-- 1 root root 0 Mar 30 19:01 oom_score-rw-r--r-- 1 root root 0 Mar 30 19:01 oom_score_adj-r-------- 1 root root 0 Mar 30 19:01 pagemap-r-------- 1 root root 0 Mar 30 19:01 personality-rw-r--r-- 1 root root 0 Mar 30 19:01 projid_maplrwxrwxrwx 1 root root 0 Mar 30 19:01 root -&gt; /-rw-r--r-- 1 root root 0 Mar 30 19:01 sched-r--r--r-- 1 root root 0 Mar 30 19:01 schedstat-r--r--r-- 1 root root 0 Mar 30 19:01 sessionid-rw-r--r-- 1 root root 0 Mar 30 19:01 setgroups-r--r--r-- 1 root root 0 Mar 30 19:01 smaps-r-------- 1 root root 0 Mar 30 19:01 stack-r--r--r-- 1 root root 0 Mar 24 09:47 stat-r--r--r-- 1 root root 0 Mar 24 10:30 statm-r--r--r-- 1 root root 0 Mar 24 16:12 status-r-------- 1 root root 0 Mar 30 19:01 syscalldr-xr-xr-x 6 root root 0 Mar 30 19:01 task-r--r--r-- 1 root root 0 Mar 30 19:01 timers-rw-r--r-- 1 root root 0 Mar 30 19:01 uid_map-r--r--r-- 1 root root 0 Mar 30 19:01 wchan\n\n通过上述信息我们则知道了运行程序的位置 /redis/bin/redis-server。\n","categories":["OS","CentOS"],"tags":["centos","linux"]},{"title":"Kubernetes持久化存储PV,PVC","url":"/2020/03/Kubernetes%E6%8C%81%E4%B9%85%E5%8C%96%E5%AD%98%E5%82%A8PV-PVC/","content":"我们可以通过hostPath或者emptyDir的方式来持久化数据。但这两种存储方式都是很大局限性的。我们希望容器在重建以后，依然能正确使用之前保存的数据。Kubernetes引入了PV和PVC两个重要的资源来实现对存储的管理。\nPVPersistentVolume，持久化卷，是对底层共享存储的一种抽象。底层共享的实现，可以是Ceph, GlusterFS, NFS等。\n我们在CentOS上搭建NFS服务讲了如何搭建NFS服务。PV作为存储资源，主要包括存储能力、访问模式、存储类型、回收策略等关键信息。\n现在新建一个PV资源对象，使用NFS类型的存储，1G空间，访问模式为ReadWriteOnce，回收策略为Recycle，yaml文件pv1-demo.yaml内容如下：\napiVersion: v1kind: PersistentVolumemetadata:  name:  pv1spec:  capacity:     storage: 1Gi  accessModes:  - ReadWriteOnce  persistentVolumeReclaimPolicy: Recycle  nfs:    path: /data/nfs    server: 172.16.64.233\n\n应用一下\n# kubectl apply -f pv1-demo.yamlpersistentvolume/pv1 created\n\n查看刚才创建的结果\n# kubectl get pvNAME   CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM   STORAGECLASS   REASON   AGEpv1    1Gi        RWO            Recycle          Available                                   82s\n\nStatus状态为Available，表示pv1已经就绪，可以被PVC申请了。\n这里讲一下PV的几个属性\nSTATUS一个 PV 的生命周期中，可能会处于4中不同的阶段：\n\nAvailable（可用）：表示可用状态，还未被任何 PVC 绑定\nBound（已绑定）：表示 PVC 已经被 PVC 绑定\nReleased（已释放）：PVC 被删除，但是资源还未被集群重新声明\nFailed（失败）： 表示该 PV 的自动回收失败\n\nCAPACITY目前只有空间大小，pv1的空间为1G，以后可能会加入IOPS，吞吐量等指标。\nACCESS MODESAccessModes是用来对PV进行访问模式的设置。\n\nReadWriteOnce（RWO）：读写权限，但是只能被单个节点挂载\nReadOnlyMany（ROX）：只读权限，可以被多个节点挂载\nReadWriteMany（RWX）：读写权限，可以被多个节点挂载\n\nRECLAIM POLICY回收策略\n\nRetain（保留）- 保留数据，需要管理员手工清理数据\nRecycle（回收）- 清除PV中的数据，效果相当于执行 rm -rf &#x2F;thevoluem&#x2F;*\nDelete（删除）- 与PV相连的后端存储完成volume的删除操作\n\n生产上一般使用Retain策略。\nPVCPersistentVolumeClaim，持久化卷声明。如果说Pod消耗的是Node资源的话，PVC则是消耗的PV资源，Pod可以请求CPU和内存，而PVC可以请求特定的存储空间和访问模式。\n我们新建一个PVC，请求1G的空间，pvc-nfs.yaml内容如下：\nkind: PersistentVolumeClaimapiVersion: v1metadata:  name: pvc-nfsspec:  accessModes:    - ReadWriteOnce  resources:    requests:      storage: 1Gi\n\n这里需要注册的是，PV是没有namespace的，而PVC是有namespace的。\n# kubectl apply -f pvc-nfs.yamlpersistentvolumeclaim/pvc-nfs created# kubectl get pvcNAME      STATUS   VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS   AGEpvc-nfs   Bound    pv1      1Gi        RWO                           3m22s\n\nPVC已经是Bound状态了。我们再查看一个PV的状态，发现也是Bound状态了。对应的声明是default/pvc-nfs，就是default命名空间下面的pvc-nfs。\n# kubectl get pvNAME   CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM             STORAGECLASS   REASON   AGEpv1    1Gi        RWO            Recycle          Bound    default/pvc-nfs                           48m\n\n这里可能觉得奇怪，我们并没有在 yaml 文件里面指定 PVC 和 PV 的关系，他们怎么可能绑定到一起呢？其实这是系统自动帮我们去匹配的，他会根据我们的声明要求去查找处于 Available 状态的 PV，如果没有找到的话那么我们的 PVC 就会一直处于 Pending 状态，找到了的话当然就会把当前的 PVC 和目标 PV 进行绑定，这个时候状态就会变成 Bound 状态了。\n如果使用PVC，可以参考Grafana安装及持久化\n","categories":["Kubernetes","PV"],"tags":["glusterfs","kubernetes","k8s","nfs","persistent volume","pv","persistent volume claim","pvc"]},{"title":"Intellij IDEA自动生成类注释","url":"/2020/03/Intellij-IDEA%E8%87%AA%E5%8A%A8%E7%94%9F%E6%88%90%E7%B1%BB%E6%B3%A8%E9%87%8A/","content":"使用 IDEA 时，我们需要自动生成类的注释。\n\nWindows\n\nFiles -&gt; Settings -&gt; Editor -&gt; File and Code Templates\n\nMac\n\nPreferences -&gt; Editor -&gt; File and Code Templates\n选中 Files 下的 Class，如图所示：\n\n\n然后在类名上方写入注释的内容：\n/**  * @description: $&#123;description&#125;  * @author: Simon  * @date: $&#123;YEAR&#125;-$&#123;MONTH&#125;-$&#123;DAY&#125; $&#123;HOUR&#125;:$&#123;MINUTE&#125;  */\n\n同时在 Interface, Enum 里面也添加上述内容。\n下次当你创建类 Class, Interface, Enum 时，就会提示让你输入 description，然后会自动生成注释。\n\n","categories":["Tools","IntelliJ IDEA"],"tags":["intellij idea"]},{"title":"Linux(CentOS)环境下安装Redis 5","url":"/2020/03/Linux-CentOS-%E7%8E%AF%E5%A2%83%E4%B8%8B%E5%AE%89%E8%A3%85Redis-5/","content":"安装Redis从Redis官网下载最新版本的redis。\n$ sudo wget http://download.redis.io/releases/redis-5.0.8.tar.gz\n\n如需安装 Redis 4 版本，可参考 Linux(CentOS)环境安装Redis 4\n新建redis安装目录\n$ sudo mkdir /usr/local/redis\n\n解压Redis\n$ sudo tar -zxvf redis-5.0.8.tar.gz -C /usr/local/redis\n\n编译Redis\n先安装gcc\n$ sudo yum install gcc-c++\n\n编译\ncd /usr/local/redis/redis-5.0.8/sudo make\n\n安装\n把redis安装在目录/usr/local/redis/redis-5.0.8/中，如果不写 PREFIX 参数，即默认安装在/usr/local/bin下面\n$ cd src/$ sudo make install PREFIX=/usr/local/redis/redis-5.0.8/Hint: It&#x27;s a good idea to run &#x27;make test&#x27; ;)    INSTALL install    INSTALL install    INSTALL install    INSTALL install    INSTALL install\n\n$ make testYou need tcl 8.5 or newer in order to run the Redis testmake: *** [test] Error 1\n\n安装tcl\n$ sudo yum install tcl\n\nsudo make test\\o/ All tests passed without errors!Cleanup: may take some time... OK\n\n安装完成后，在目录 /usr/local/redis/redis-5.0.8 下面会出现一个 bin 目录\n$ ll bin/total 32772-rwxr-xr-x. 1 root root 4366824 Mar 20 13:14 redis-benchmark-rwxr-xr-x. 1 root root 8125024 Mar 20 13:14 redis-check-aof-rwxr-xr-x. 1 root root 8125024 Mar 20 13:14 redis-check-rdb-rwxr-xr-x. 1 root root 4807816 Mar 20 13:14 redis-clilrwxrwxrwx. 1 root root      12 Mar 20 13:14 redis-sentinel -&gt; redis-server-rwxr-xr-x. 1 root root 8125024 Mar 20 13:14 redis-server\n\n启动Redis服务\n$ bin/redis-server 36959:C 20 Mar 2020 13:28:02.467 # oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo36959:C 20 Mar 2020 13:28:02.467 # Redis version=5.0.8, bits=64, commit=00000000, modified=0, pid=36959, just started36959:C 20 Mar 2020 13:28:02.467 # Warning: no config file specified, using the default config. In order to specify a config file use bin/redis-server /path/to/redis.conf36959:M 20 Mar 2020 13:28:02.467 # You requested maxclients of 10000 requiring at least 10032 max file descriptors.36959:M 20 Mar 2020 13:28:02.467 # Server can&#x27;t set maximum open files to 10032 because of OS error: Operation not permitted.36959:M 20 Mar 2020 13:28:02.467 # Current maximum open files is 4096. maxclients has been reduced to 4064 to compensate for low ulimit. If you need higher maxclients increase &#x27;ulimit -n&#x27;.                _._                                                             _.-``__ &#x27;&#x27;-._                                                   _.-``    `.  `_.  &#x27;&#x27;-._           Redis 5.0.8 (00000000/0) 64 bit  .-`` .-```.  ```\\/    _.,_ &#x27;&#x27;-._                                    (    &#x27;      ,       .-`  | `,    )     Running in standalone mode |`-._`-...-` __...-.``-._|&#x27;` _.-&#x27;|     Port: 6379 |    `-._   `._    /     _.-&#x27;    |     PID: 36959  `-._    `-._  `-./  _.-&#x27;    _.-&#x27;                                    |`-._`-._    `-.__.-&#x27;    _.-&#x27;_.-&#x27;|                                   |    `-._`-._        _.-&#x27;_.-&#x27;    |           http://redis.io          `-._    `-._`-.__.-&#x27;_.-&#x27;    _.-&#x27;                                    |`-._`-._    `-.__.-&#x27;    _.-&#x27;_.-&#x27;|                                   |    `-._`-._        _.-&#x27;_.-&#x27;    |                                    `-._    `-._`-.__.-&#x27;_.-&#x27;    _.-&#x27;                                         `-._    `-.__.-&#x27;    _.-&#x27;                                                 `-._        _.-&#x27;                                                         `-.__.-&#x27;                                               36959:M 20 Mar 2020 13:28:02.468 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.36959:M 20 Mar 2020 13:28:02.468 # Server initialized36959:M 20 Mar 2020 13:28:02.468 # WARNING overcommit_memory is set to 0! Background save may fail under low memory condition. To fix this issue add &#x27;vm.overcommit_memory = 1&#x27; to /etc/sysctl.conf and then reboot or run the command &#x27;sysctl vm.overcommit_memory=1&#x27; for this to take effect.36959:M 20 Mar 2020 13:28:02.468 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command &#x27;echo never &gt; /sys/kernel/mm/transparent_hugepage/enabled&#x27; as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.36959:M 20 Mar 2020 13:28:02.468 * Ready to accept connections\n\n通过客户端连接redis服务\n$ bin/redis-cli 127.0.0.1:6379&gt; set name keyOK127.0.0.1:6379&gt; get name&quot;key&quot;127.0.0.1:6379&gt; \n\n如果通过非127.0.0.1连接，会报如下错误：\n$ bin/redis-cli -h 192.168.229.130192.168.229.130:6379&gt; get name\n(error) DENIED Redis is running in protected mode because protected mode is enabled, no bind address was specified, no authentication password is requested to clients. In this mode connections are only accepted from the loopback interface. If you want to connect from external computers to Redis you may adopt one of the following solutions: 1) Just disable protected mode sending the command &#x27;CONFIG SET protected-mode no&#x27; from the loopback interface by connecting to Redis from the same host the server is running, however MAKE SURE Redis is not publicly accessible from internet if you do so. Use CONFIG REWRITE to make this change permanent. 2) Alternatively you can just disable the protected mode by editing the Redis configuration file, and setting the protected mode option to &#x27;no&#x27;, and then restarting the server. 3) If you started the server manually just for testing, restart it with the &#x27;--protected-mode no&#x27; option. 4) Setup a bind address or an authentication password. NOTE: You only need to do one of the above things in order for the server to start accepting connections from the outside.\n\n修改配置文件\n$ sudo vi redis.conf\n\n修改 bind 参数\nbind 0.0.0.0\n\n重启 redis 服务\n通过 -h 指定redis服务的地址\nbin/redis-cli -h 192.168.229.130192.168.229.130:6379&gt; \n\n运行完成以后，如果需要关闭redis服务\n127.0.0.1:6379&gt; SHUTDOWN SAVEnot connected&gt;\n","categories":["Middleware","Redis"],"tags":["redis"]},{"title":"Linux下Shell命令进行日期date和时间戳timestamp的转换 ","url":"/2020/03/Linux%E4%B8%8BShell%E5%91%BD%E4%BB%A4%E8%BF%9B%E8%A1%8C%E6%97%A5%E6%9C%9Fdate%E5%92%8C%E6%97%B6%E9%97%B4%E6%88%B3timestamp%E7%9A%84%E8%BD%AC%E6%8D%A2/","content":"我们常常会遇到如下情况，把日期转换成timestamp，或者把timestamp转换成日期格式。\n初级选手一般都会打开网页，有些站点提供了日期转换的功能。\n高级选手可以直接通过shell命令，就能成功的完成日期和timestamp的转换。\n我们先来看看Linux的date命令。\n\n显示日期时间\n\n# dateWed Mar  4 11:44:05 CST 2020\n\n\n显示带时区的日期时间\n\n# date -RWed, 04 Mar 2020 11:44:41 +0800\n\n\n按指定格式显示日期时间\n\n# date &#x27;+%Y-%m-%d %H:%M:%S %Z&#x27;2020-03-04 11:46:14 CST\n\n\n查看当前时间的timestamp\n\n# date +%s1583293699\n\n\n显示描述类型的日期时间\n\n# date -d &quot;1970-01-01 UTC&quot;Thu Jan  1 08:00:00 CST 1970\n\n-d的参数是指：-d, --date=STRING          display time described by STRING, not &#39;now&#39;\n\n日期时间转timestamp\n\n# date -d &#x27;1970-01-01 UTC&#x27; +%s0\n\ntimestamp转日期时间\n\n# date -d @1583395499.709Thu Mar  5 16:04:59 CST 2020\n","categories":["OS"],"tags":["centos","linux","date","timestamp","shell"]},{"title":"Prometheus中指标,Servicemonitor,Targets的关系","url":"/2020/03/Prometheus%E4%B8%AD%E6%8C%87%E6%A0%87-Servicemonitor-Targets%E7%9A%84%E5%85%B3%E7%B3%BB/","content":"Prometheus下拉列表里面的指标是怎么来的？\n如果安装了alert-manager, kube-state-metrics这样组件，这些组件会提供/metrics接口，然后Prometheus就可以拉取这些接口，从而获取指标数据，便展示于PrometheusDashboard的下拉列表里面了。当然，在指标里面有可能还会加入一些标签，如job, instance之类的。\n网上有人说Servicemonitor是exporter的抽象，我觉得是不对的。最多抽象了一部分而已。\n一个Servicemonitor对应n个Targetes里面的记录。\n当你删除Servicemonitor对象，在Targets页面，这个特定的Target肯定是不显示了，但并不代表Prometheus下拉列表中的指标会消失，指标还是存在的。只是有可能有些标签变化了，这个标签变化的还没有经过验证。\n当你删除Pod时，比如把alert-manager的deployment删除，对应的Pod自动删除，这时，下拉列表对应的指标还存在，但却没有值了。\n这个时候，如果把Prometheus重启一下，这个指标才会消失。\n把kube-state-metrics的pods删除后，kube-开始的指标，如：kube_pod_info，就没有值了。\n把node-exporter的daemonset删除后，node_cpu_seconds_total节点的指标就没值了。\n","categories":["Kubernetes","Prometheus"],"tags":["kubernetes","监控","prometheus","metrics","servicemonitor","targets","指标"]},{"title":"Prometheus kubernetes-cadvisor服务自动发现","url":"/2020/03/Prometheus-kubernetes-cadvisor%E6%9C%8D%E5%8A%A1%E8%87%AA%E5%8A%A8%E5%8F%91%E7%8E%B0/","content":"这一篇文章，通过kubernetes_sd_configs配置服务自动发现。\n还原来一样，是通过kube-prometheus安装的k8s监控系统。\n和之前由kubernetes-operator已经提供的yaml文件不同，这次我们需要新建一个yaml文件配置。\nprometheus-additional.yaml内容如下，注意，这里自动发现的role设置为pod。通过它，将自动发现k8s集群里面所有的pod。\n- job_name: &#x27;kubernetes-cadvisor&#x27;  scheme: https  tls_config:    ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt  bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token  kubernetes_sd_configs:  - role: pod  relabel_configs:  - target_label: __address__    replacement: kubernetes.default.svc:443  - source_labels: [__meta_kubernetes_pod_node_name]    regex: (.+)    replacement: /api/v1/nodes/$&#123;1&#125;/proxy/metrics/cadvisor    target_label: __metrics_path__  - action: labelmap    regex: __meta_kubernetes_pod_label_(.+)\n\n然后还需要修改prometheus-prometheus.yaml的内容，在spec项下级，serviceAccountName标签前面添加additionalScrapeConfigs标签，如下：\nspec:  ...  additionalScrapeConfigs:    name: additional-configs    key: prometheus-additional.yaml  serviceAccountName: prometheus-k8s  ...\n\n删除之前的secret，如果有的话。\n# kubectl delete secret additional-configs -n monitoringsecret &quot;additional-configs&quot; deleted\n\n第一次新建当然是没有的，所以直接创建secret：\n# kubectl create secret generic additional-configs --from-file=prometheus-additional.yaml -n monitoringsecret/additional-configs created\n\n然后应用prometheus的配置，因为做过修改。\n# kubectl apply -f prometheus/prometheus-prometheus.yaml prometheus.monitoring.coreos.com/k8s configured\n\n稍等片刻后，我们可以看到Prometheus Configuration设置已经变化。已经有job_name为kubernetes-cadvisor的记录了。kubernetes-cadvisor部分内容如下：\n- job_name: kubernetes-cadvisor  honor_timestamps: true  scrape_interval: 30s  scrape_timeout: 10s  metrics_path: /metrics  scheme: https  kubernetes_sd_configs:  - role: pod  bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token  tls_config:    ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt    insecure_skip_verify: false  relabel_configs:  - separator: ;    regex: (.*)    target_label: __address__    replacement: kubernetes.default.svc:443    action: replace  - source_labels: [__meta_kubernetes_pod_node_name]    separator: ;    regex: (.+)    target_label: __metrics_path__    replacement: /api/v1/nodes/$&#123;1&#125;/proxy/metrics/cadvisor    action: replace  - separator: ;    regex: __meta_kubernetes_pod_label_(.+)    replacement: $1    action: labelmap\n\n但奇怪的是，我们在Targets页面并没有发现kubernetes-cadvisor这个Target。\n# kubectl get po -n monitoringNAME                                   READY   STATUS    RESTARTS   AGE...prometheus-k8s-0                       3/3     Running   1          77dprometheus-k8s-1                       3/3     Running   1          77d\n\n查看pod，未发现异常。\n查看一下Pod日志。kubectl logs -f prometheus-k8s-0 prometheus -n monitoring\n\nlevel&#x3D;error ts&#x3D;2020-03-05T02:56:36.493Z caller&#x3D;klog.go:94 component&#x3D;k8s_client_runtime func&#x3D;ErrorDepthmsg&#x3D;”&#x2F;app&#x2F;discovery&#x2F;kubernetes&#x2F;kubernetes.go:263: Failed to list *v1.Endpoints: endpoints is forbidden: User &quot;system:serviceaccount:monitoring:prometheus-k8s&quot; cannot list resource &quot;endpoints&quot; in API group &quot;&quot; at the cluster scope”level&#x3D;error ts&#x3D;2020-03-05T02:56:37.491Z caller&#x3D;klog.go:94 component&#x3D;k8s_client_runtime func&#x3D;ErrorDepth msg&#x3D;”&#x2F;app&#x2F;discovery&#x2F;kubernetes&#x2F;kubernetes.go:264: Failed to list *v1.Service: services is forbidden: User &quot;system:serviceaccount:monitoring:prometheus-k8s&quot; cannot list resource &quot;services&quot; in API group &quot;&quot; at the cluster scope”level&#x3D;error ts&#x3D;2020-03-05T02:56:37.494Z caller&#x3D;klog.go:94 component&#x3D;k8s_client_runtime func&#x3D;ErrorDepth msg&#x3D;”&#x2F;app&#x2F;discovery&#x2F;kubernetes&#x2F;kubernetes.go:265: Failed to list *v1.Pod: pods is forbidden: User &quot;system:serviceaccount:monitoring:prometheus-k8s&quot; cannot list resource &quot;pods&quot; in API group &quot;&quot; at the cluster scope”\n\n很多出错日志，主要是上面三条的循环出现。\n\nFailed to list *v1.Endpoints: endpoints is forbidden: User &quot;system:serviceaccount:monitoring:prometheus-k8s&quot; cannot list resource &quot;endpoints&quot; in API group &quot;&quot; at the cluster scope.\n\nFailed to list *v1.Service: services is forbidden: User &quot;system:serviceaccount:monitoring:prometheus-k8s&quot; cannot list resource &quot;services&quot; in API group &quot;&quot; at the cluster scope.\n\nFailed to list *v1.Pod: pods is forbidden: User &quot;system:serviceaccount:monitoring:prometheus-k8s&quot; cannot list resource &quot;pods&quot; in API group &quot;&quot; at the cluster scope.\n\n\nendpoints/services/pods is forbidden，说明是RBAC权限问题，namespace: monitoring下的serviceaccount: prometheus-k8s这个用户没有权限。\n查看prometheus-prometheus.yaml内容，可以看到Prometheus绑定了一个名为prometheus-k8s的serviceAccount对象。\napiVersion: monitoring.coreos.com/v1kind: Prometheusmetadata:  labels:    prometheus: k8s  name: k8s  namespace: monitoringspec:  ...  serviceAccountName: prometheus-k8s  ...\n\n通过查看prometheus-clusterRole.yaml得知，名为prometheus-k8s的serviceAccount对象绑定的是一个名为prometheus-k8s的ClusterRole。\n绑定关系在prometheus-clusterRoleBinding.yaml这个文件。\napiVersion: rbac.authorization.k8s.io/v1kind: ClusterRoleBindingmetadata:  name: prometheus-k8sroleRef:  apiGroup: rbac.authorization.k8s.io  kind: ClusterRole  name: prometheus-k8ssubjects:- kind: ServiceAccount  name: prometheus-k8s  namespace: monitoring\n\n查看这个Role：\napiVersion: rbac.authorization.k8s.io/v1kind: ClusterRolemetadata:  name: prometheus-k8srules:- apiGroups:  - &quot;&quot;  resources:  - nodes/metrics  verbs:  - get- nonResourceURLs:  - /metrics  verbs:  - get\n\n在上面的权限规则中，并未发现对endpoints&#x2F;services&#x2F;pods的list权限。所以做如下修改：\napiVersion: rbac.authorization.k8s.io/v1kind: ClusterRolemetadata:  name: prometheus-k8srules:- apiGroups:  - &quot;&quot;  resources:  - nodes/metrics  - configmaps  verbs:  - get- nonResourceURLs:  - /metrics  verbs:  - get# new apiGroups- apiGroups:  - &quot;&quot;  resources:  - nodes  - services  - endpoints  - pods  - nodes/proxy  verbs:  - get  - list  - watch\n\n应用一下\n# kubectl apply -f prometheus-clusterRole.yaml clusterrole.rbac.authorization.k8s.io/prometheus-k8s configured\n\n这时就可以看到Targets了。\n\n\n我们分析一下第二个Endpoint为：https://kubernetes.default.svc:443/api/v1/nodes/k8s-node1/proxy/metrics/cadvisor\n在node节点上是无法解析kubernetes.default.svc的，所以我们先获取这个域名的真实地址。\nkubectl get svcNAME               TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)        AGEkubernetes         ClusterIP   10.96.0.1      &lt;none&gt;        443/TCP        62d\n\n再通过secret获取到token，就可以访问url以获取指标了。\n# curl -k https://10.96.0.1:443/api/v1/nodes/k8s-node1/proxy/metrics/cadvisor -H &quot;Authorization: Bearer eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJtb25pdG9yaW5nIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZWNyZXQubmFtZSI6InByb21ldGhldXMtazhzLXRva2VuLXNtNmdkIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZSI6InByb21ldGhldXMtazhzIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQudWlkIjoiMTk2OTkyOTItNDY2Yi00NWQ4LWJmYmYtYzkyZjIwNjczOWY3Iiwic3ViIjoic3lzdGVtOnNlcnZpY2VhY2NvdW50Om1vbml0b3Jpbmc6cHJvbWV0aGV1cy1rOHMifQ.iggZ4ZxmD0y04OQfDlo4P6zRgzn0ryVhcdhlgncpnBY5BJ39Xz0a2AA51ePa78R2njFDjPcecgDJRcqPv76X3o-C-G7EZvN_Ru8zSdB51YxqlLNoIW5hy6Jr27aw74lMslg1MYX_31kkRTqD9DxVn6lq6Uqf4Djebj_E-2maiwl863GCeNRfS1X6KM8idsVknLlpdVINbM8U_l1Yuw-auNzelAk1NQlBdbJqsm1CZKIg_YBsT-KbiyTsbjX2v0uL1D6-Q5Xs9NZMLEAa7dfwz_EOYMDnIGbv-eyhD-924H4_pGOIoQ0dCBP01cxFm7pLJPGouwLaEwPs5BRS0B6u-w&quot;\n\n指标以container_开始。和我们之前讨论的kubelet/1的指标完全一样。Prometheus monitoring&#x2F;kubelet监控指标\n所以其实是可以通过修改kubelet/1的配置来达到新增这个Target的目的。但kubelet/1的目的主要是监控kubelet的情况，其他的Pod它不监控，Target是没有被keep的。如下图：\nrelabel_configs:- source_labels: [__meta_kubernetes_service_label_k8s_app]  separator: ;  regex: kubelet  replacement: $1  action: keep\n\n我将指标存入kubernetes-cadvisor-1.txt ，供下载查看。\n","categories":["Kubernetes","Prometheus"],"tags":["kubernetes","k8s","prometheus"]},{"title":"Spring Boot dev-tools实现项目热部署自动重启","url":"/2020/03/Spring-Boot-dev-tools%E5%AE%9E%E7%8E%B0%E9%A1%B9%E7%9B%AE%E7%83%AD%E9%83%A8%E7%BD%B2%E8%87%AA%E5%8A%A8%E9%87%8D%E5%90%AF/","content":"我们有时候需要这样的功能，代码修改后，希望项目能自动重启，不然每次都要手动 Rerun 一样，很麻烦。\n开启IDEA的自动编译在 Compiler 里面勾选 Build project automatically 自动构建项目。\n\n\n开启IDEA的热部署策略点击 Edit Configurations…\n\n\n在 pom.xml 加入依赖&lt;dependency&gt;    &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;    &lt;artifactId&gt;spring-boot-devtools&lt;/artifactId&gt;    &lt;scope&gt;runtime&lt;/scope&gt;    &lt;optional&gt;true&lt;/optional&gt;&lt;/dependency&gt;\n\n之后，修改代码以后，项目会自动重新编译并重启了，开发也更有效率了。\n","categories":["Java","Spring Boot"],"tags":["java","spring boot","dev-tools"]},{"title":"Spring Boot使用@JsonProperty,@JsonIgnore,@JsonFormat注解","url":"/2020/03/Spring-Boot%E4%BD%BF%E7%94%A8-JsonProperty-JsonIgnore-JsonFormat%E6%B3%A8%E8%A7%A3/","content":"@JsonProperty, @JsonIgnore 和 @JsonFormat 注解都是 fasterxml jackson 里面的注解，现在也被 Spring Boot 集成了。\n我们在使用上面的注解时，不需要在 pom.xml 显示的引入 fasterxml jackson 的依赖包。只需要加入如下依赖即可。\n&lt;dependency&gt;    &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;    &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;&lt;/dependency&gt;\n\n@JsonProperty用于属性、setter &#x2F; getter 方法上，属性序列化后可重命名。\n@JsonProperty(&quot;image_width&quot;)private Double imageWidth;@JsonProperty(&quot;image_height&quot;)private Double imageHeight;\n\n生成的 json 字符串就是image_width和image_height。\n@JsonIgnore属性使用此注解后，将不被序列化。\n@JsonFormat用于格式化日期\n@JsonFormat(pattern = &quot;yyyy-MM-dd&quot;)private Date birthday;\n","categories":["Java","Spring Boot"],"tags":["java","json","spring boot"]},{"title":"Redis (error) NOAUTH Authentication required解决方法","url":"/2020/03/Redis-error-NOAUTH-Authentication-required%E8%A7%A3%E5%86%B3%E6%96%B9%E6%B3%95/","content":"当 Redis 设置了密码信息，我们通过客户端访问时，就会报错：(error) NOAUTH Authentication required。\n使用auth &lt;password&gt;命令，输入密码就可以解决。&lt;password&gt;就是你填入密码的位置。\n# ./redis-cli 127.0.0.1:6379&gt; get name(error) NOAUTH Authentication required.127.0.0.1:6379&gt; auth &lt;password&gt;OK127.0.0.1:6379&gt; get name&quot;finology&quot;127.0.0.1:6379&gt; \n\n","categories":["Middleware","Redis"],"tags":["redis"]},{"title":"Spring Boot序列化对象时json忽略null值","url":"/2020/03/Spring-Boot%E5%BA%8F%E5%88%97%E5%8C%96%E5%AF%B9%E8%B1%A1%E6%97%B6json%E5%BF%BD%E7%95%A5null%E5%80%BC/","content":"在 Spring Boot 项目中，我们以 json 作为返回结果时，往往不需要输出值为 null 的属性，有点多余。\n有两种方法可以用。\n第一种，就是在类上添加注解：@JsonInclude(JsonInclude.Include.NON_NULL)\n这种方法的好处，就是可以单独处理每个类。不好的地方就是，一个输出类，其属性，以及属性的属性都是类，这样每个类都设置这个注解，有些繁琐。\n第二种，在 application.yaml 里面添加如下配置：\nspring:  jackson:    default-property-inclusion: non_null\n\n所有的类，在序列化为 json，只要属性值为 null 的，都不会输出出来。\n","categories":["Java","Spring Boot"],"tags":["json","spring boot"]},{"title":"Spring Boot通过DigestUtils计算MD5的值","url":"/2020/03/Spring-Boot%E9%80%9A%E8%BF%87DigestUtils%E8%AE%A1%E7%AE%97MD5%E7%9A%84%E5%80%BC/","content":"我们大部分的项目都是 Spring Boot 的项目，它集成了很多工具，所以我不需要再加入其他的类。\n比如求 MD5 的值，我就可以用 org.springframework.util.DigestUtils\npublic abstract class DigestUtilsextends Object\n\nmd5DigestAsHexpublic static String md5DigestAsHex(InputStream inputStream)                             throws IOException\n","categories":["Java","Spring Boot"],"tags":["java","spring boot","digestUtils","md5"]},{"title":"Springboot使用profiles区分环境","url":"/2020/03/Springboot%E4%BD%BF%E7%94%A8profiles%E5%8C%BA%E5%88%86%E7%8E%AF%E5%A2%83/","content":"配置文件，格式一般为application-&#123;profile&#125;.yaml。\n\napplication.yaml\napplication-dev.yaml\napplication-prod.yaml\n\n未指定任何profile时，会默认加载application.yaml配置文件。\n所以，与环境无关的属性配置，都应该放到application.yaml文件中，与环境有关的，则放到相应的其他配置文件中。\n激活profile在application.yaml文件里面指定。spring:  profiles:    active: dev\n\n在IDEA里面修改编辑配置\n方法1在Environment下VM options这一栏里面填入：-Dspring.profiles.active=dev。\n方法2在Environment下Program arguments这一栏里面填入： --spring-profiles-active=dev。\n方法3在Spring boot下Active profiles这一栏里面填入：dev\n\n\n运行jar时指定参数java -jar demo.jar --spring.profiles.active=dev\n\n运行后，console里面会提示本次运行，是用到了哪个配置文件覆盖或追加了最基本的application.yaml文件。\n[18:25:42.127] INFO  org.springframework.boot.SpringApplication 679 logStartupProfileInfo - The following profiles are active: dev\n","categories":["Java","Spring Boot"],"tags":["java","spring boot"]},{"title":"TCP三次握手四次挥手的过程","url":"/2020/03/TCP%E4%B8%89%E6%AC%A1%E6%8F%A1%E6%89%8B%E5%9B%9B%E6%AC%A1%E6%8C%A5%E6%89%8B%E7%9A%84%E8%BF%87%E7%A8%8B/","content":"Http请求百度\nTCP连接\n# exec 5&lt;&gt; /dev/tcp/www.baidu.com/80# echo -e &#x27;GET / HTTP/1.0\\n&#x27; &gt;&amp; 5# cat &lt;&amp; 5HTTP/1.0 200 OKAccept-Ranges: bytesCache-Control: no-cacheContent-Length: 14615Content-Type: text/htmlDate: Sat, 21 Mar 2020 08:14:27 GMTP3p: CP=&quot; OTI DSP COR IVA OUR IND COM &quot;P3p: CP=&quot; OTI DSP COR IVA OUR IND COM &quot;Pragma: no-cacheServer: BWS/1.1Set-Cookie: BAIDUID=42C9E3F8B0C101BDEA241D5174987905:FG=1; expires=Thu, 31-Dec-37 23:55:55 GMT; max-age=2147483647; path=/; domain=.baidu.comSet-Cookie: BIDUPSID=42C9E3F8B0C101BDEA241D5174987905; expires=Thu, 31-Dec-37 23:55:55 GMT; max-age=2147483647; path=/; domain=.baidu.comSet-Cookie: PSTM=1584778467; expires=Thu, 31-Dec-37 23:55:55 GMT; max-age=2147483647; path=/; domain=.baidu.comSet-Cookie: BAIDUID=42C9E3F8B0C101BDD0E9FB6623D5C445:FG=1; max-age=31536000; expires=Sun, 21-Mar-21 08:14:27 GMT; domain=.baidu.com; path=/; version=1; comment=bdTraceid: 1584778467046850433011449469410765311951Vary: Accept-EncodingX-Ua-Compatible: IE=Edge,chrome=1&lt;!DOCTYPE html&gt;&lt;!--STATUS OK--&gt;&lt;html&gt;&lt;head&gt;...\n\n5 叫文件描述符\nSocket 连接\n# ss -antpState       Recv-Q Send-Q                                                   Local Address:Port                                                                  Peer Address:PortLISTEN      0      128                                                                  *:22                                                                               *:*                   users:((&quot;sshd&quot;,pid=1007,fd=3))LISTEN      0      100                                                          127.0.0.1:25                                                                               *:*                   users:((&quot;master&quot;,pid=1166,fd=13))ESTAB       0      0                                                        172.16.64.220:22                                                                     172.16.64.1:62779               users:((&quot;sshd&quot;,pid=6702,fd=3))ESTAB       0      0                                                        172.16.64.220:22                                                                     172.16.64.1:62925               users:((&quot;sshd&quot;,pid=6820,fd=3))CLOSE-WAIT  0      0                                                        172.16.64.220:57720                                                                 36.152.44.95:80                  users:((&quot;ss&quot;,pid=6930,fd=5),(&quot;bash&quot;,pid=6706,fd=5))LISTEN      0      128                                                                 :::22                                                                              :::*                   users:((&quot;sshd&quot;,pid=1007,fd=4))LISTEN      0      100                                                                ::1:25                                                                              :::*                   users:((&quot;master&quot;,pid=1166,fd=14))\n\n或者\n# netstat -antpActive Internet connections (servers and established)Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program nametcp        0      0 0.0.0.0:22              0.0.0.0:*               LISTEN      1007/sshdtcp        0      0 127.0.0.1:25            0.0.0.0:*               LISTEN      1166/mastertcp        0      0 172.16.64.220:22        172.16.64.1:62779       ESTABLISHED 6702/sshd: root@ptstcp        0      0 172.16.64.220:22        172.16.64.1:62925       ESTABLISHED 6820/sshd: root@ptstcp        0      0 172.16.64.220:57720     36.152.44.95:80         CLOSE_WAIT  6706/-bashtcp6       0      0 :::22                   :::*                    LISTEN      1007/sshdtcp6       0      0 ::1:25                  :::*                    LISTEN      1166/master\n\n\n查看内核一些参数\n# ulimit -acore file size          (blocks, -c) 0data seg size           (kbytes, -d) unlimitedscheduling priority             (-e) 0file size               (blocks, -f) unlimitedpending signals                 (-i) 3802max locked memory       (kbytes, -l) 64max memory size         (kbytes, -m) unlimitedopen files                      (-n) 1024pipe size            (512 bytes, -p) 8POSIX message queues     (bytes, -q) 819200real-time priority              (-r) 0stack size              (kbytes, -s) 8192cpu time               (seconds, -t) unlimitedmax user processes              (-u) 3802virtual memory          (kbytes, -v) unlimitedfile locks                      (-x) unlimited\n\nopen files 1024 个\n# cd /proc/$$/fd# pwd/proc/6706/fd# ll总用量 0lrwx------. 1 root root 64 11月 22 18:54 0 -&gt; /dev/pts/0lrwx------. 1 root root 64 11月 22 18:54 1 -&gt; /dev/pts/0lrwx------. 1 root root 64 11月 22 18:54 2 -&gt; /dev/pts/0lrwx------. 1 root root 64 11月 22 19:01 255 -&gt; /dev/pts/0lrwx------. 1 root root 64 11月 22 18:54 5 -&gt; socket:[49378]\n\n修改\n# ulimit -SHn 65535# ulimit -n65535\n\n查看路由表\n网络层 IP协议。携带的是目标地址的IP地址。\n# route -nKernel IP routing tableDestination     Gateway         Genmask         Flags Metric Ref    Use Iface0.0.0.0         172.16.64.2     0.0.0.0         UG    100    0        0 ens33172.16.64.0     0.0.0.0         255.255.255.0   U     100    0        0 ens33\n\n查看链路\n如何找到下一跳地址，是通过 mac 地址来确定的。\n# arp -nAddress                  HWtype  HWaddress           Flags Mask            Iface172.16.64.254            ether   00:50:56:e7:e8:58   C                     ens33172.16.64.2              ether   00:50:56:eb:6f:44   C                     ens33172.16.64.1              ether   00:50:56:c0:00:08   C                     ens33\n\n请求百度抓包\n再打开另一个终端\n使用 tcpdump 命令监控\n# tcpdump -nnn -i ens33 port 80 or arptcpdump: verbose output suppressed, use -v or -vv for full protocol decodelistening on ens33, link-type EN10MB (Ethernet), capture size 262144 bytes\n\n如果没有找到命令就安装一下。\n# yum install tcpdump\n\n在之前的终端访问百度，在访问之前，我们先删除链路层的记录，让数据包被发送时找不到网关下一跳的 mac 地址。\n# arp -d 172.16.64.2\n\n删除以后，如果有访问网络，这个 arp 的记录很快又会生成。所以我们在监控时，要在删除记录后的瞬间请求百度。\n# arp -d 172.16.64.2 &amp;&amp; curl www.baidu.com\n\n通过 tcpdump 我们可以看到抓包后的内容。172.16.64.220 是本机。\n# tcpdump -nnn -i ens33 port 80 or arptcpdump: verbose output suppressed, use -v or -vv for full protocol decodelistening on ens33, link-type EN10MB (Ethernet), capture size 262144 bytes21:54:21.286505 ARP, Request who-has 172.16.64.2 tell 172.16.64.220, length 2821:54:21.286832 ARP, Reply 172.16.64.2 is-at 00:50:56:eb:6f:44, length 4621:54:21.316879 IP 172.16.64.220.57734 &gt; 36.152.44.95.80: Flags [S], seq 2491677366, win 29200, options [mss 1460,sackOK,TS val 81477028 ecr 0,nop,wscale 7], length 021:54:21.330823 IP 36.152.44.95.80 &gt; 172.16.64.220.57734: Flags [S.], seq 1247240682, ack 2491677367, win 64240, options [mss 1460], length 021:54:21.330911 IP 172.16.64.220.57734 &gt; 36.152.44.95.80: Flags [.], ack 1, win 29200, length 021:54:21.331466 IP 172.16.64.220.57734 &gt; 36.152.44.95.80: Flags [P.], seq 1:78, ack 1, win 29200, length 77: HTTP: GET / HTTP/1.121:54:21.331749 IP 36.152.44.95.80 &gt; 172.16.64.220.57734: Flags [.], ack 78, win 64240, length 021:54:21.347623 IP 36.152.44.95.80 &gt; 172.16.64.220.57734: Flags [P.], seq 1:2782, ack 78, win 64240, length 2781: HTTP: HTTP/1.1 200 OK21:54:21.347688 IP 172.16.64.220.57734 &gt; 36.152.44.95.80: Flags [.], ack 2782, win 33580, length 021:54:21.348071 IP 172.16.64.220.57734 &gt; 36.152.44.95.80: Flags [F.], seq 78, ack 2782, win 33580, length 021:54:21.348981 IP 36.152.44.95.80 &gt; 172.16.64.220.57734: Flags [.], ack 79, win 64239, length 021:54:21.362444 IP 36.152.44.95.80 &gt; 172.16.64.220.57734: Flags [FP.], seq 2782, ack 79, win 64239, length 021:54:21.362479 IP 172.16.64.220.57734 &gt; 36.152.44.95.80: Flags [.], ack 2783, win 33580, length 0^C13 packets captured13 packets received by filter0 packets dropped by kernel\n\nmss 1460\n","categories":["Network"],"tags":["network","tcp","http"]},{"title":"会计基础","url":"/2020/03/%E4%BC%9A%E8%AE%A1%E5%9F%BA%E7%A1%80/","content":"\n\n会计是干嘛的——目标：提供信息（核算监督是过程）\n\n\n财务状况——“富”（有钱）\n经营成果——“高”（利润高不高，能赚钱）\n现金流量——“帅”（舍不舍得花钱）\n\n\n财务状况——资产负债表\n经营成果——利润表\n现金流量——现金流量表\n1.资产负债表——基于 资产&#x3D;负债+所有者权益 编制\n反映静态（时点）要素\n如果一个人有2000万的别墅，买别墅的钱，其中70%是借来的，另外30%是爹妈给的。2000万中的1400万是借来的，要还的，是负债，另外600万是爹妈给的，是权益，不用还。\n等式左边资产是资金占用，右边是资金的来源。资产负债表也是这样。\n资产负债表需要区分资产和负债，除了这些就是权益。\n名称中有“应收”“投资”“资产”大多数是资产，资产是一种资源。资产是将资源进行分类。\n名称中有“应付”“应交”“负债”基本是负债。\n\n\n资产&#x3D;负债+所有者权益+（收入-费用），收入和费用影响权益，从而影响资产\n2.利润表——收入-费用&#x3D;利润（动态、过程）\n利润表中有三种利润：营业利润、利润总额、净利润\n\n\n营业外收入（例捐赠、捡钱、中彩票）、营业外支出（例自然灾害）不影响营业利润。\n\n\n影响营业利润的项目：1（营业收入）、2（营业成本、税金及附加）、4（4个费用）、2（2个减值损失）、4（4个收益），共13个。（去掉“净敞口套期收益”，考试不考）\n营业收入&#x3D;主营业务收入（主营）+其他业务收入（辅营）——都是日常活动\n销售费用：广告营销费\n管理费用：如办公费\n研发费用：如研发人员工资\n财务费用：利息费用\n投资收益：股票分红、债券利息收益\n公允价值变动收益：\n资产处置收益：\n\n\n不影响营业利润，说明是营业外，就是自然灾害。\n\n上图6个红色章节是重难点。\n18、19、20现在复习可以先跳过，2020年可能会发生变化。\n\n\n\n","categories":["财务"],"tags":["会计","财务报表","利润表","资产负债表","现金流量表"]},{"title":"pandas.apply()的axis参数","url":"/2020/03/pandas-apply-%E7%9A%84axis%E5%8F%82%E6%95%B0/","content":"DataFrame.apply(func, axis=0, broadcast=None, raw=False, reduce=None, result_type=None, args=(), **kwds)\n\nfunc : Function to be applied to each column or row. This function accepts a series and returns a series.\n我们往往对 axis 搞不太清楚。\naxis : Axis along which the function is applied in dataframe. Default value 0.\n\nIf value is 0 then it applies function to each column.\nIf value is 1 then it applies function to each row.\n\nargs : tuple &#x2F; list of arguments to passed to function.\n","categories":["Python","Pandas"],"tags":["python","pandas"]},{"title":"批量操作Shell终端","url":"/2020/03/%E6%89%B9%E9%87%8F%E6%93%8D%E4%BD%9CShell%E7%BB%88%E7%AB%AF/","content":"在生产环境，往往有几十上百台服务器要同时操作，如何同时控制这么多服务器呢？\nWindows在Windows环境下，可以使用XShell。\n点击工具(T)-&gt;发送键到所有会话(K)\n\n\nMacOS可以使用 iTerm2 终端工具，快捷键是Command + Shift + I。然后 iTerm2 窗口会出现背景斜条纹。\n","categories":["Tools","SSH"],"tags":["linux","shell","terminal","xshell","iterm2"]},{"title":"解决IDEA卡住resolving maven dependencies, Failed to resolve org.junit.platform:junit-platform-launcher:1.5.2","url":"/2020/03/%E8%A7%A3%E5%86%B3IDEA%E5%8D%A1%E4%BD%8Fresolving-maven-dependencies-Failed-to-resolve-org-junit-platform-junit-platform-launcher-1-5-2/","content":"IntelliJ IDEA 版本：Ultimate 2019.1\nspring-boot-starter-parent: 2.2.0.RELEASE\n我们在运行 test case 时，会一直卡住，resolving maven dependencies… org.junit.platform:junit-platform-launcher:1.5.2，超时以后就 Failed 了。\n\n\n这是 IDEA 的一个bug，解析 artifact 时，没有用到 maven 的 user settings file\n解决办法：\n在 pom.xml 文件中添加如下依赖：\n&lt;dependency&gt;    &lt;groupId&gt;org.junit.platform&lt;/groupId&gt;    &lt;artifactId&gt;junit-platform-launcher&lt;/artifactId&gt;    &lt;scope&gt;test&lt;/scope&gt;&lt;/dependency&gt;\n","categories":["Tools","IntelliJ IDEA"],"tags":["intellij idea","junit"]},{"title":"IntelliJ IDEA一键生成实体类所有Setter方法(GenerateAllSetter插件)","url":"/2020/06/IntelliJ-IDEA%E4%B8%80%E9%94%AE%E7%94%9F%E6%88%90%E5%AE%9E%E4%BD%93%E7%B1%BB%E6%89%80%E6%9C%89Setter%E6%96%B9%E6%B3%95-GenerateAllSetter%E6%8F%92%E4%BB%B6/","content":"查如下代码，User 实体类有5个属性，如果属性再多一些，每次赋值时就非常麻烦。\nUser user = new User();user.setId(0L);user.setName(&quot;&quot;);user.setAge(0);user.setCreatedAt(LocalDateTime.now());user.setUpdatedAt(LocalDateTime.now());\n\n如果使用 IntelliJ IDEA 的话，则可以安装 GenerateAllSetter 插件，一键生成所有 Setter 方法。\n\n\n把光标放在 user 上面，然后使用快捷键 alt + enter\n\n\n选择 Generate all setter with default value 就可以了，非常方便。\n","categories":["Tools"],"tags":["intellij idea"]},{"title":"腾迅云云直播混流开发环境搭建","url":"/2020/03/%E8%85%BE%E8%BF%85%E4%BA%91%E4%BA%91%E7%9B%B4%E6%92%AD%E6%B7%B7%E6%B5%81%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/","content":"记录一下通过腾迅云云直播平台开发混流操作的环境搭建。主要是在腾迅平台做一些设置，和下载一个推流软件并做相关配置。\n进入腾迅云去直播控制台。\n添加自有域名添加推流域名关于推流域名：直播已提供系统推流域名，亦可添加自有已备案域名进行推流。\n\n\n由于是测试使用，我们就先不绑定自有域名了。\n添加播放域名播放域名必须要有自有域名，并且已经备案，这里假设我们的备案域名是：finolo.gy。\n\n\n添加以后，会有红色警告提示。\n\n\n去域名解析那里添加一条 CNAME 记录，主机记录为 play.live，记录值为上图的 play.live.finolo.gy.livecdn.liveplay.myqcloud.com。\n绑定后很快就能生效，可以通过 nslookup 命令查看。然后刚才的红色图标报警也变成绿色了。\n获取推流地址进入域名管理页面，点击推流域名或管理按钮，进入推流配置标签页，填写好 StreamName，就可以生成推流地址了。\n\n\n做为后端开发，一般不需要通过程序来获取推流地址，但APP端，需要推流，就需要通过程序来获取这个推流地址了。\n也可以在辅助工具下面的地址生成器下面生成推流地址，这些信息也是下面我们使用 OBS 设备需要填入的。\n\n\n直播推流我下载了一个 OBS 设备来推流，目前觉得还是挺好用的。\n可以去 OBS 官网 https://obsproject.com/download 下载。\n然后设置推流。如图：\n\n\n同时需要设置一个推流的来源，我选择的是窗口捕获，这样从某个窗体捕获的视频流就采集到，并推送到前面设置的那个推流地址了。\n流直播进入 流管理 页面，我们就可以看到有一流记录在上面了。点击测试按钮，就可以看到流的内容了。非常方便后端测试了。\n\n","categories":["Java","Spring Boot"],"tags":["java","混流","云直播","腾迅云"]},{"title":"IntelliJ IDEA中如何查看方法的每个出口","url":"/2020/06/IntelliJ-IDEA%E4%B8%AD%E5%A6%82%E4%BD%95%E6%9F%A5%E7%9C%8B%E6%96%B9%E6%B3%95%E7%9A%84%E6%AF%8F%E4%B8%AA%E5%87%BA%E5%8F%A3/","content":"在 IntelliJ IDEA 中，如果方法体很长，这时想查看方法有哪些出口，可以这样操作：\nWindows:\n把光标放在 return 上面，再按 ctrl + shift + F7\n","categories":["Tools","IntelliJ IDEA"],"tags":["intellij idea"]},{"title":"Spring Boot Actuator","url":"/2020/06/Spring-Boot-Actuator/","content":"Spring Boot Actuator 是 Spring Boot 的一套监控系统。\n依赖\n&lt;dependency&gt;    &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;    &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt;&lt;/dependency&gt;\n\n顺便提一句，spring boot 的官方依赖，一般是 spring-boot-starter-xxxx 这种格式的。非官方依赖则是 xxxx-spring-boot-starter 这种格式的。\n启动项目。\n访问 http://localhost:8080/actuator\n可以看到有两个endpoints，一个是 health, 一个是 info。\n访问 health：\nhttp://localhost:8080/actuator/health\n&#123;  status: &quot;UP&quot;&#125;\n\n访问 info:\nhttp://localhost:8080/actuator/info\n&#123; &#125;\n\n是一个空对象。\n我们可以在 application.yml 配置文件中做如下配置：\nmanagement:  endpoint:    health:      show-details: always  endpoints:    web:      exposure:        include: metrics, health, info\n\nmanagement.endpoint.health.show-details=always 表示在访问 health 时，除了显示 UP 这个状态以外，还要显示更多的信息。\n&#123;  status: &quot;UP&quot;,  components: &#123;    db: &#123;      status: &quot;UP&quot;,      details: &#123;        database: &quot;MySQL&quot;,        result: 1,        validationQuery: &quot;/* ping */ SELECT 1&quot;      &#125;    &#125;,    diskSpace: &#123;      status: &quot;UP&quot;,      details: &#123;        total: 250790436864,        free: 18968088576,        threshold: 10485760      &#125;    &#125;,    ping: &#123;      status: &quot;UP&quot;    &#125;  &#125;&#125;\n\nendpoints.web.exposure.include=metrics, health, info 这个表示可以显示的endpoint。\n这个值可以填写 *，但是，如果是yaml文件，必须是&#39;*&#39;&#39;，而 application.properties 可以直接写 *。这是 yaml 文件和 properties 文件的区别。\n我们可以设置 info:\ninfo:  app-name: springboot-mybatis-plus  author: simon\n\n这样，在访问 info endpoint 时，就可以显示上面配置的信息了。\n&#123;  app-name: &quot;springboot-mybatis-plus&quot;,  author: &quot;simon&quot;&#125;\n\n如果 yaml 里面带了环境变量，在开发时，可以在 IDEA 里面设置。在控制台运行时，命令如下：\njava -jar xxx.jar --SOME_ENV=certain_env\n","categories":["Java","Spring Boot"],"tags":["spring boot","actuator"]},{"title":"python处理.net 18位时间戳","url":"/2020/06/python%E5%A4%84%E7%90%86-net-18%E4%BD%8D%E6%97%B6%E9%97%B4%E6%88%B3/","content":"平时我们使用的时间时间戳一般都是13位带毫秒数和10位不带毫秒数的。.net有个 ticks 的概念，是18位的时间戳。\n意义是从公元1月1日零点开始的，到现在有多少个100纳秒。\n这个 ticks * 100 为纳秒数，然后除以 1000 变为微秒数。\nfrom datetime import *ticks = 637170897393060000t = datetime(1, 1, 1) + timedelta(microseconds = ticks // 10)print(datetime.strftime(t, &#x27;%Y-%m-%d %H:%M:%S&#x27;))2020-02-12 07:35:39\n","categories":["Python"],"tags":["python","datetime"]},{"title":"JPA, JMS, JTA与XA","url":"/2020/04/JPA-JMS-JTA%E4%B8%8EXA/","content":"JPA, JMS, JTA 都是 Java 企业级规范。\nJPA（java persistence API）JPA 通过JDK5.0的注解或XML来描述 对象-关系表的映射关系，并将运行期的实体对象持久化存储到数据库中。 \nJMS（java message service）JMS是java平台上面向中间件的技术规范，便于消息系统中java应用程序进行信心交换，并且通过提供标准的产生、发送、接收消息的接口简化企业应用的开发，翻译为Java消息服务。\nXA一个事务规范，JTA 就是 XA 规范的 Java 实现。\nJTA（java transaction API）JTA允许应用程序执行分布式事务管理，在两个或多个网络计算机资源上访问并且更新数据。JDBC驱动程序的JTA支持极大地增强了数据访问能力。\nJTA 接口JTA 定义了三个接口，分别是：\nTransactionManager\nXAResource\nXID\nJT\nJTA 弊端两阶段提交\n事务时间太长，锁数据的时间太长\n性能低，吞吐量低\n","categories":["Java","Spring"],"tags":["spring","transaction"]},{"title":"使用pdfbox把pdf转图片时遇到的问题","url":"/2020/06/%E4%BD%BF%E7%94%A8pdfbox%E6%8A%8Apdf%E8%BD%AC%E5%9B%BE%E7%89%87%E6%97%B6%E9%81%87%E5%88%B0%E7%9A%84%E9%97%AE%E9%A2%98/","content":"我们使用 apache 的 pdfbox 工具，来处理 pdf。在把 pdf 转图片时，会遇到如下一些问题。\nWARN 8776 --- [           main] o.a.pdfbox.pdmodel.font.PDType0Font      : No Unicode mapping for CID+3585 (3585) in font DJXXII+PingFangSC-Regular\n\n这个应该是找不到字体，不过是 WARN 级别，在这里可以不用太在意。我找到很多方法，也没能解决掉这个警告。\n处理字体时，需要加入以下依赖。\n&lt;dependency&gt;    &lt;groupId&gt;org.apache.pdfbox&lt;/groupId&gt;    &lt;artifactId&gt;fontbox&lt;/artifactId&gt;    &lt;version&gt;2.0.19&lt;/version&gt;&lt;/dependency&gt;\n\nERROR 8776 --- [           main] o.a.p.contentstream.PDFStreamEngine      : Cannot read JBIG2 image: jbig2-imageio is not installed\n\n需要添加如下依赖：\n&lt;dependency&gt;    &lt;groupId&gt;org.apache.pdfbox&lt;/groupId&gt;    &lt;artifactId&gt;jbig2-imageio&lt;/artifactId&gt;    &lt;version&gt;3.0.3&lt;/version&gt;&lt;/dependency&gt;\n\nERROR 9074 --- [           main] o.a.p.contentstream.PDFStreamEngine      : Cannot read JPEG2000 image: Java Advanced Imaging (JAI) Image I/O Tools are not installed\n\n需要添加如下依赖：\n&lt;dependency&gt;    &lt;groupId&gt;com.github.jai-imageio&lt;/groupId&gt;    &lt;artifactId&gt;jai-imageio-jpeg2000&lt;/artifactId&gt;    &lt;version&gt;1.3.0&lt;/version&gt;&lt;/dependency&gt;\n\njai-imageio-jpeg2000 会依赖相同版本的 jai-imageio-core，但我们可以使用最新版本的 jai-imageio-core。\n&lt;dependency&gt;    &lt;groupId&gt;com.github.jai-imageio&lt;/groupId&gt;    &lt;artifactId&gt;jai-imageio-core&lt;/artifactId&gt;    &lt;version&gt;1.4.0&lt;/version&gt;&lt;/dependency&gt;\n","categories":["Tools"],"tags":["pdfbox"]},{"title":"Java Integer, String, Object互相转换","url":"/2020/04/Java-Integer-String-Object%E4%BA%92%E7%9B%B8%E8%BD%AC%E6%8D%A2/","content":"在类型转换的时候要非常小心，有些是编译时错误，简单些，有些是运行时错误，要特别注意。\npublic class ConvTest &#123;    public static void main(String[] args) &#123;        // Integer -&gt; String        Integer i1 = 1;        String s1 = String.valueOf(i1);        System.out.println(s1);        // String i1 = (String) i1; 不相容类型, 不能强制转换        // Integer &lt;- String        String s2 = &quot;1&quot;;        Integer i2 = Integer.valueOf(s2);        System.out.println(i2);        // Integer i2 = (Integer) s2; 不相容类型, 不能强制转换        int i22 = Integer.parseInt(s2);        System.out.println(i22);        // Integer - Object        Object o3 = 1;        Integer i3 = (Integer) o3; // 如果o3为字符串, 会有运行时异常, 所以建议用下面 instanceof 做类型判断        System.out.println(i3);        // Integer.valueOf(o3) // 不能用 Object 作为参数        // String - Object        Object o6 = &quot;1&quot;;        String s6 = (String) o6;        System.out.println(s6);        Object o4 = &quot;1&quot;;        if (o4 instanceof Integer) &#123;            System.out.println(&quot;Integer&quot;);        &#125; else if (o4 instanceof String) &#123;            System.out.println(&quot;String&quot;);        &#125;        Object o7 = 1;        String s7 = String.valueOf(o7);  // 这里却可以用 Object 作为参数        System.out.println(s7);    &#125;&#125;\n","categories":["Java"],"tags":["java"]},{"title":"Java, MySQL开发关于时间时区的最佳实践","url":"/2020/04/Java-MySQL%E5%BC%80%E5%8F%91%E5%85%B3%E4%BA%8E%E6%97%B6%E9%97%B4%E6%97%B6%E5%8C%BA%E7%9A%84%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/","content":"Java 开发实践中，我们在处理时间、时区上总是搞得很乱。这里总结一下认为比较好的一个最佳实践，欢迎大家提出自己的意见。\nJava 中不带时区的类 java.time.LocalDateTime，这个类对应 MySQL 中 datetime 类型。MySQL 中的 datetime 类型也是不带时区概念的。\n不带时区概念的意思就是说，当我看到这个值的时候，并不能唯一确定时间，因为没有时区信息。\n但 Java 中的类 java.util.Date 或 java.time.ZonedDateTime 却是带了时区的类，这个值，是能唯一确定时间的。\n我们在中国开发应用程序时，最佳解决方案：\n\nJava, MySQL 所在服务器时间设置为东八区时间。\n\nJava, MySQL 默认使用服务器时区。\n\njava connector 6 版本以后，MySQL 连接字符串指定东八区，不指定是默认西五区的美国东部时间。\n\n前端拿到东八区时间后，根据浏览器时区做相应的调整。前端提交表单时，拿到当地时间后，转换成东八区时间提交。\n\n\n","categories":["Java"],"tags":["java","mysql"]},{"title":"Lombok @Data, @Builder, @NoArgsConstructor注解实体类运行MyBatis报错","url":"/2020/04/Lombok-Data-Builder-NoArgsConstructor%E6%B3%A8%E8%A7%A3%E5%AE%9E%E4%BD%93%E7%B1%BB%E8%BF%90%E8%A1%8CMyBatis%E6%8A%A5%E9%94%99/","content":"Java 程序中，我们使用 MyBatis ORM 框架和 MySQL 交互。MySQL 有个 user 表，所以在 Java 程序我们需要有个 User 类。\n我给 User 类添加了 @Data 和 @Builder 注解，结果在做 select 操作时报错了，MyBatis 应该是说需要一个无参构造器。\n@Data 注解会默认生成 getter/setter，和无参构造方法，但是当我们添加了 @Builder 注解时，无参构造器就被删除掉了，所以报错了。\n这时我们会想到那我再加一个 @NoArgsConstructor 注解行不行呢，实验后发现是不行的。\n@NoArgsConstructor 和 @Builder 会有冲突，导致编译出错。\n可以添加一个无参构造方法并加上@Tolerate注解，如下：\n@Data@Builderpublic class User &#123;        @Tolerate    public User() &#123;    &#125;&#125;\n\n或者，类似这种和MySQL表结构对应的Entity类，就不使用 @Builder 注解，需要User实例时，就 new 一个吧。\n","categories":["Java"],"tags":["java","mybatis"]},{"title":"Linux(CentOS)安装RabbitMQ","url":"/2020/04/Linux-CentOS-%E5%AE%89%E8%A3%85RabbitMQ/","content":"通过 yum 在 CentOS 7 环境下安装 RabbitMQ Server 3.8。\n配置yum源这次安装使用官方推荐的，基于 PackageCloud 的 Yum 仓库安装。\n进入官网 https://packagecloud.io/rabbitmq/rabbitmq-server/install#bash-rpm 。\n我们可以看到 quick install 的快速安装命令。\n$ curl -s https://packagecloud.io/install/repositories/rabbitmq/rabbitmq-server/script.rpm.sh | sudo bash...Complete!Generating yum cache for rabbitmq_rabbitmq-server...Importing GPG key 0x4D206F89: Userid     : &quot;https://packagecloud.io/rabbitmq/rabbitmq-server (https://packagecloud.io/docs#gpg_signing) &lt;support@packagecloud.io&gt;&quot; Fingerprint: 8c69 5b02 19af deb0 4a05 8ed8 f4e7 8920 4d20 6f89 From       : https://packagecloud.io/rabbitmq/rabbitmq-server/gpgkeyGenerating yum cache for rabbitmq_rabbitmq-server-source...The repository is setup! You can now install packages.\n\n脚本执行后，会安装一些最基本的底层依赖，比如 yum-utils.noarch。同时还会生成 /etc/yum.repos.d/rabbitmq_rabbitmq-server.repo 文件。\n安装 Erlang 依赖。进入页面https://packagecloud.io/rabbitmq/erlang/install#bash-rpm。\n$ curl -s https://packagecloud.io/install/repositories/rabbitmq/erlang/script.rpm.sh | sudo bash...Generating yum cache for rabbitmq_erlang-source...The repository is setup! You can now install packages.\n\n安装安装Erlang首先安装Erlang语言依赖。\n$ sudo yum install erlang\n\n安装RabbitMQ$ sudo yum install rabbitmq-server\n\n设置开机自动启动\nchkconfig rabbitmq-server on\n\n启动&#x2F;关闭 RabbitMQ 服务\nsystemctl start rabbitmq-serversystemctl stop rabbitmq-server\n\n安装插件查看插件列表\n$ sudo rabbitmq-plugins listListing plugins with pattern &quot;.*&quot; ... Configured: E = explicitly enabled; e = implicitly enabled | Status: * = running on rabbit@localhost |/[  ] rabbitmq_amqp1_0                  3.8.3[  ] rabbitmq_auth_backend_cache       3.8.3[  ] rabbitmq_auth_backend_http        3.8.3[  ] rabbitmq_auth_backend_ldap        3.8.3[  ] rabbitmq_auth_backend_oauth2      3.8.3[  ] rabbitmq_auth_mechanism_ssl       3.8.3[  ] rabbitmq_consistent_hash_exchange 3.8.3[  ] rabbitmq_event_exchange           3.8.3[  ] rabbitmq_federation               3.8.3[  ] rabbitmq_federation_management    3.8.3[  ] rabbitmq_jms_topic_exchange       3.8.3[  ] rabbitmq_management               3.8.3[  ] rabbitmq_management_agent         3.8.3[  ] rabbitmq_mqtt                     3.8.3[  ] rabbitmq_peer_discovery_aws       3.8.3[  ] rabbitmq_peer_discovery_common    3.8.3[  ] rabbitmq_peer_discovery_consul    3.8.3[  ] rabbitmq_peer_discovery_etcd      3.8.3[  ] rabbitmq_peer_discovery_k8s       3.8.3[  ] rabbitmq_prometheus               3.8.3[  ] rabbitmq_random_exchange          3.8.3[  ] rabbitmq_recent_history_exchange  3.8.3[  ] rabbitmq_sharding                 3.8.3[  ] rabbitmq_shovel                   3.8.3[  ] rabbitmq_shovel_management        3.8.3[  ] rabbitmq_stomp                    3.8.3[  ] rabbitmq_top                      3.8.3[  ] rabbitmq_tracing                  3.8.3[  ] rabbitmq_trust_store              3.8.3[  ] rabbitmq_web_dispatch             3.8.3[  ] rabbitmq_web_mqtt                 3.8.3[  ] rabbitmq_web_mqtt_examples        3.8.3[  ] rabbitmq_web_stomp                3.8.3[  ] rabbitmq_web_stomp_examples       3.8.3\n\n启动管理平台插件。\n$ sudo rabbitmq-plugins enable rabbitmq_managementEnabling plugins on node rabbit@localhost:rabbitmq_managementThe following plugins have been configured:  rabbitmq_management  rabbitmq_management_agent  rabbitmq_web_dispatchApplying plugin configuration to rabbit@localhost...The following plugins have been enabled:  rabbitmq_management  rabbitmq_management_agent  rabbitmq_web_dispatchstarted 3 plugins.\n\n查看端口占用情况。\n$ ss -tunlp | grep 5672tcp    LISTEN     0      128       *:15672                 *:*                  tcp    LISTEN     0      128       *:25672                 *:*                  tcp    LISTEN     0      128    [::]:5672               [::]:*\n\n5672 是 RabbitMQ Server 端口，25672 是集群端口，15672 是web管理平台端口。\n配置设置登录用户，编译文件：\n$ sudo vi /usr/lib/rabbitmq/lib/rabbitmq_server-3.8.3/ebin/rabbit.app\n\n把如下内容：\n&#123;loopback_users, [&lt;&lt;&quot;guest&quot;&gt;&gt;]&#125; 改为 &#123;loopback_users, [&quot;guest&quot;]&#125;。\n重启。\n访问页面：http://&lt;server-ip&gt;:15672/ ，通过用户名 guest 和密码 guest 就可以成功登录了。\n\n","categories":["Middleware","RabbitMQ"],"tags":["rabbitmq"]},{"title":"Mac Docker安装MySQL","url":"/2020/04/Mac-Docker%E5%AE%89%E8%A3%85MySQL/","content":"需要快速安装一个MySQL数据库，还是使用Docker方便快捷。\n首先创建本地目录/usr/local/mysql-docker，并设置权限。\n$ sudo chmod 777 mysql-docker\n\n这样存放Docker上面MySQL的数据文件，哪怕重启以后，数据不会丢失。\n运行下面命令，就可以在Docker下面成功安装 MySQL 5.7。\ndocker run --name mysql --env MYSQL_ROOT_HOST=172.17.%.% --env MYSQL_ROOT_PASSWORD=123456 -v /usr/local/mysql-docker:/var/lib/mysql -p 3306:3306 -d mysql/mysql-server:5.7\n\n如果没有把本地路径 添加到Docker的文件共享目录里，可能会报如下错误：\nThe path /usr/local/mysql-dockeris not shared from OS X and is not known to Docker.You can configure shared paths from Docker -&gt; Preferences... -&gt; File Sharing.See https://docs.docker.com/docker-for-mac/osxfs/#namespaces for more info.\n\n进入Docker -&gt; Preferences... -&gt; File Sharing，把路径添加进去即可。\n\n\n进入Docker容器\n$ docker exec -it 5fd4bb2b6c67 bash\n\n进入数据库\nbash-4.2# mysql -uroot -p123456mysql: [Warning] Using a password on the command line interface can be insecure.Welcome to the MySQL monitor.  Commands end with ; or \\g.Your MySQL connection id is 8Server version: 5.7.29 MySQL Community Server (GPL)Copyright (c) 2000, 2020, Oracle and/or its affiliates. All rights reserved.Oracle is a registered trademark of Oracle Corporation and/or itsaffiliates. Other names may be trademarks of their respectiveowners.Type &#x27;help;&#x27; or &#x27;\\h&#x27; for help. Type &#x27;\\c&#x27; to clear the current input statement.mysql&gt;\n","categories":["Database","MySQL"],"tags":["mysql","docker"]},{"title":"Linux清除系统缓存命令","url":"/2020/04/Linux%E6%B8%85%E9%99%A4%E7%B3%BB%E7%BB%9F%E7%BC%93%E5%AD%98%E5%91%BD%E4%BB%A4/","content":"drop_cachesecho 3 &gt; /proc/sys/vm/drop_caches\n\n上面的echo 3 是清理所有缓存\necho 0 是不释放缓存\necho 1 是释放页缓存\nehco 2 是释放dentries和inodes缓存\necho 3 是释放 1 和 2 中说道的的所有缓存\n","categories":["Tools"],"tags":["linux"]},{"title":"Mac环境下编译安装Redis 4","url":"/2020/04/Mac%E7%8E%AF%E5%A2%83%E4%B8%8B%E7%BC%96%E8%AF%91%E5%AE%89%E8%A3%85Redis-4/","content":"下载源码进入官网下载页面，https://redis.io/download，我们下载 Redis 4。\nhttp://download.redis.io/releases/redis-4.0.14.tar.gz\n$ cd /usr/local/redis$ sudo wget http://download.redis.io/releases/redis-4.0.14.tar.gz$ sudo tar -zxvf redis-4.0.14.tar.gz\n\n编译安装$ cd redis-4.0.14$ sudo make test\n\n可能会遇到如下错误：\n[exception]: Executing test client: couldn&#x27;t execute &quot;src/redis-benchmark&quot;: no such file or directory.couldn&#x27;t execute &quot;src/redis-benchmark&quot;: no such file or directory\n\n然后执行：\n$ sudo make distclean$ sudo make$ sudo make test\\o/ All tests passed without errors!Cleanup: may take some time... OK(base)\n\n这时可执行文件都默认安装到了 /usr/local/redis/redis-4.0.14/src。\n如果要指定安装位置，我们可以在前面执行这样的命令：\n$ cd src$ sudo make install PREFIX=/usr/local/redis/redis-4.0.14\n\n这样就会在 /usr/local/redis/redis-4.0.14/bin 生成可执行文件了。\n启动redis服务$ sudo bin/redis-server redis.conf\n\n启动客户端：\n$ bin/redis-cli127.0.0.1:6379&gt; pingPONG\n\n测试成功。\n如果要查询 Redis Server 的版本，在客户执行：\n127.0.0.1:6379&gt; info# Serverredis_version:4.0.14...\n\n就可以看到包括版本号的各种信息了。\n","categories":["Middleware","Redis"],"tags":["redis"]},{"title":"Mac环境修改Jupyter Notebook启动文件夹位置","url":"/2020/04/Mac%E7%8E%AF%E5%A2%83%E4%BF%AE%E6%94%B9Jupyter-Notebook%E5%90%AF%E5%8A%A8%E6%96%87%E4%BB%B6%E5%A4%B9%E4%BD%8D%E7%BD%AE/","content":"Windows 10 环境上修改 Jupyter Notebook 启动文件夹地址，请参考 Win10环境修改Jupyter Notebook默认文件夹位置。\n如何修改 mac 系统 Jupyter Notebook 启动文件夹的位置呢？\n生成配置文件$ jupyter notebook --generate-configWriting default config to: /Users/simon/.jupyter/jupyter_notebook_config.py(base) \n\n编译文件$ vi /Users/simon/.jupyter/jupyter_notebook_config.py\n\n修改 c.NotebookApp.notebook_dir 参数，取值设为你打算启动的位置。\n## The directory to use for notebooks and kernels.#c.NotebookApp.notebook_dir = &#x27;&#x27;c.NotebookApp.notebook_dir = &#x27;/Users/simon/Development/workspace/python&#x27;\n\n保存退出，重启 Jupyter Notebook，就能指定显示该位置的内容了。\n","categories":["Python","Jupyter Notebook"],"tags":["mac","jupyter notebook"]},{"title":"MySQL事务隔离","url":"/2020/04/MySQL%E4%BA%8B%E5%8A%A1%E9%9A%94%E7%A6%BB/","content":"MySQL 事务隔离相关命令如下：\n-- 查看当前会话隔离级别SELECT @@tx_isolation;-- 查看系统隔离级别SELECT @@global.tx_isolation;-- 0. SERIALIZABLE-- 1. REPEATABLE READ-- 2. READ COMMITTED-- 3. READ UNCOMMITTEDSET SESSION TRANSACTION ISOLATION LEVEL READ UNCOMMITTED;\n\nMySQL 默认的隔离级别是 RR, Repeatable Read。\n","categories":["Database","MySQL"],"tags":["mysql","transaction"]},{"title":"Python Pandas groupby with count, sum, and avg etc","url":"/2020/04/Python-Pandas-groupby-with-count-sum-and-avg-etc/","content":"我们要对一个 DataFrame 做分组统计时，其实跟 SQL 语句的逻辑是差不多的。\ngroupby.sum(), groupby.count() 等等。\n如果要把一个 DataFrame 里的不同列各做 sum(), count() 该如何操作呢？在这里给大家做一个演示。\n# 原始数据keyword_list = [&#x27;dog&#x27;, &#x27;cat&#x27;, &#x27;horse&#x27;, &#x27;dog&#x27;, &#x27;dog&#x27;, &#x27;horse&#x27;]weight_list = [0.12, 0.5, 0.07, 0.1, 0.2, 0.3]other_keywords_list = [[&#x27;cat&#x27;, &#x27;horse&#x27;, &#x27;pig&#x27;], [&#x27;dog&#x27;, &#x27;pig&#x27;, &#x27;camel&#x27;], [&#x27;dog&#x27;, &#x27;camel&#x27;, &#x27;cat&#x27;], [&#x27;cat&#x27;, &#x27;horse&#x27;], [&#x27;cat&#x27;, &#x27;horse&#x27;, &#x27;pig&#x27;], [&#x27;camel&#x27;]]# 生成DataFramedict = &#123;&#x27;keyword&#x27;: keyword_list, &#x27;weight&#x27;: weight_list, &#x27;other keywords&#x27;: other_keywords_list&#125;animaldf = pd.DataFrame(data=dict)animaldf\n\n\n\n\n&nbsp;\nkeyword\nweight\nother keywords\n\n\n\n0\ndog\n0.12\n[cat, horse, pig]\n\n\n1\ncat\n0.50\n[dog, pig, camel]\n\n\n2\nhorse\n0.07\n[dog, camel, cat]\n\n\n3\ndog\n0.10\n[cat, horse]\n\n\n4\ndog\n0.20\n[cat, horse, pig]\n\n\n5\nhorse\n0.30\n[camel]\n\n\ndf = animaldf.groupby(&#x27;keyword&#x27;).agg(&#123;&#x27;keyword&#x27;: &#x27;count&#x27;, &#x27;weight&#x27;: &#x27;sum&#x27;, &#x27;other keywords&#x27;: &#x27;sum&#x27;&#125;)df\n\n\n\n\n&nbsp;\nkeyword\nweight\nother keywords\n\n\n\nkeyword\n\n\n\n\n\ncat\n1\n0.50\n[dog, pig, camel]\n\n\ndog\n3\n0.42\n[cat, horse, pig, cat, horse, cat, horse, pig]\n\n\nhorse\n2\n0.37\n[dog, camel, cat, camel]\n\n\n这样就已经达到效果了。\n如果接下来要继续对数据进行处理，可以先把 index 的名字 keyword 去掉，不去掉的话，转成列时就和现有列 keyword 名字冲突了。\n然后把索引转换成列。\ndf.index.name = Nonedf.reset_index()\n\n最后你想把列改成什么名字就随你了。\n","categories":["Python","Pandas"],"tags":["python","pandas"]},{"title":"Python Pandas对DataFrame float数据进行四舍五入","url":"/2020/04/Python-Pandas%E5%AF%B9DataFrame-float%E6%95%B0%E6%8D%AE%E8%BF%9B%E8%A1%8C%E5%9B%9B%E8%88%8D%E4%BA%94%E5%85%A5/","content":"原始数据如图：\n\n\n我们要对float数据四舍五入，并把单位改为亿元。\n方法一\n使用 lambda 表达式。\ncap_list = industry_df[&#x27;MKT_CAP_ARD&#x27;].apply(lambda x: round(x / 100000000, 0)).astype(int)cap_list\n\n方法二\ncap_list = round(industry_df[&#x27;MKT_CAP_ARD&#x27;] / 100000000, 0).astype(int)cap_list\n\n最后需要使用 astype 函数把 float 转为 int。不然数据还是会带一位小数的，哪怕是0。\n","categories":["Python"],"tags":["python","pandas"]},{"title":"Python函数的位置参数，默认参数，可变参数，关键字参数和命名关键字参数","url":"/2020/04/Python%E5%87%BD%E6%95%B0%E7%9A%84%E4%BD%8D%E7%BD%AE%E5%8F%82%E6%95%B0-%E9%BB%98%E8%AE%A4%E5%8F%82%E6%95%B0-%E5%8F%AF%E5%8F%98%E5%8F%82%E6%95%B0-%E5%85%B3%E9%94%AE%E5%AD%97%E5%8F%82%E6%95%B0%E5%92%8C%E5%91%BD%E5%90%8D%E5%85%B3%E9%94%AE%E5%AD%97%E5%8F%82%E6%95%B0/","content":"Python 函数具有非常多的参数形态，刚开始接触时很多人不一定了解所有参数的含义。\n位置参数这个很简单吧，跟其他语言都是一样的。\n# 可以设置多个位置参数def stock_information(code, name):    print(&#x27;code:&#x27;, code)    print(&#x27;name:&#x27;, name)    stock_information(&#x27;000001.SZ&#x27;, &#x27;平安银行&#x27;)code: 000001.SZname: 平安银行\n\n默认参数# 若不知道股票名称时，可以将股票名默认为“股票”def stock_information(code, name=&#x27;股票&#x27;):    print(&#x27;code:&#x27;, code)    print(&#x27;name:&#x27;, name)    stock_information(&#x27;000001.SZ&#x27;)code: 000001.SZname: 股票\n\n可变参数可变参数可以是0个到任意个，调用函数时，会自动将可变参数组装成元组。可变参数的形式为 *args。\n# 若不知道要存几天的收盘价def stock_information(code, name=&#x27;股票&#x27;, *args):    print(&#x27;code:&#x27;, code)    print(&#x27;name:&#x27;, name)    print(&#x27;others:&#x27;, args)    stock_information(&#x27;000001.SZ&#x27;, &#x27;平安银行&#x27;, &#x27;1987-12-22&#x27;, &#x27;银行&#x27;)code: 000001.SZname: 平安银行others: (&#x27;1987-12-22&#x27;, &#x27;银行&#x27;)\n\n这里需要注意的是，如果是赋值语句：\ncode, *list = &#x27;000001.SZ&#x27;, &#x27;平安银行&#x27;, &#x27;1987-12-22&#x27;, &#x27;银行&#x27;print(type(list))&lt;class &#x27;list&#x27;&gt;\n\n这时，list就是一个list类型的变量了。\n关键字参数和上面不同的是，调用函数时，会自动将关键字参数组装成字典。可变参数的形式为 **kws。\n# 想要传入其他别的信息，比如公司成立日期、所属行业def stock_information(code, name =&#x27;股票&#x27;, **kws):    print(&#x27;code:&#x27;, code)    print(&#x27;name:&#x27;, name)    print(&#x27;others:&#x27;, kws)    stock_information(&#x27;000001.SZ&#x27;,&#x27;平安银行&#x27;, date=&#x27;1987-12-22&#x27;, industry=&#x27;银行&#x27; )code: 000001.SZname: 平安银行others: &#123;&#x27;date&#x27;: &#x27;1987-12-22&#x27;, &#x27;industry&#x27;: &#x27;银行&#x27;&#125;\n\n命名关键字参数用户想要输入的关键字参数，在关键字参数前加分隔符 * ，即*, nkw，命名关键字不能缺少参数名。Python2貌似是不支持的。\ndef stock_information(code, name=&#x27;股票&#x27;, *, industry):    print(&#x27;code:&#x27;, code)    print(&#x27;name:&#x27;, name)    print(&#x27;industry:&#x27;, industry)    stock_information(&#x27;000001.SZ&#x27;, &#x27;平安银行&#x27;, industry=&#x27;银行&#x27; )code: 000001.SZname: 平安银行industry: 银行\n\n参数组合5种参数中的4种可以组合在一起使用，但组合有顺序。\n位置参数、默认参数、可变参数、关键字参数\n位置参数、默认参数、命名关键字参数、关键字参数\n","categories":["Python"],"tags":["python"]},{"title":"Python的数据结构","url":"/2020/04/Python%E7%9A%84%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/","content":"Python的基础数据结构有以下几种\n数组list[]list([1, 2, 3])list(range(5))\n\n元组tuple()tuple((1, 2))tuple([1, 2, 3])\n\n字典dict&#123;&#125;\n\n&#123;&#125; 是空dict，{‘key’: ‘value’}\ndict(key = &#x27;k&#x27;, val = &#x27;v&#x27;)dict([(&#x27;key&#x27;, &#x27;k&#x27;), (&#x27;val&#x27;, &#x27;v&#x27;)])&#123;&#x27;key&#x27;: &#x27;k&#x27;, &#x27;val&#x27;: &#x27;v&#x27;&#125;\n\n集合set&#123;&#x27;a&#x27;&#125;set((1, 2))set(&#123;1, 2&#125;)\n\n如果是空集合，是不能写 &#123;&#125; 的，&#123;&#125; 代表空dict。\n空集合是 set()\nz = zip([&#x27;a&#x27;, &#x27;b&#x27;], [1, 2], [3, 4])list(z)[(&#x27;a&#x27;, 1, 3), (&#x27;b&#x27;, 2, 4)]z = zip([&#x27;a&#x27;, &#x27;b&#x27;], [1, 2])l = list(z)dict(l)&#123;&#x27;a&#x27;: 1, &#x27;b&#x27;: 2&#125;\n","categories":["Python"],"tags":["python"]},{"title":"Spring Boot yaml文件配置多维数组","url":"/2020/04/Spring-Boot-yaml%E6%96%87%E4%BB%B6%E9%85%8D%E7%BD%AE%E5%A4%9A%E7%BB%B4%E6%95%B0%E7%BB%84/","content":"有时候我们需要在配置文件里面配置一些数组。以下示例是配置的二维数组，如果把 kv 键值分开来看，可以算作三维数组。\napplication.yaml\nconfig:  arrs:    -      - k: a1k        v: a1v      - k: a2k        v: a2v    -      - k: b1k        v: b1v      - k: b2k        v: b2v\n\nArrsConfig.java\npackage gy.finolo;import lombok.Data;import org.springframework.boot.context.properties.ConfigurationProperties;import org.springframework.context.annotation.Configuration;import java.util.List;@Configuration@ConfigurationProperties(&quot;config&quot;)@Datapublic class ArrsConfig &#123;    private List&lt;Kv&gt;[] arrs;    @Data    public static class Kv &#123;        public String k;        private String v;    &#125;&#125;\n\n需要特别注意，这里定义了一个 Kv 静态内部类，必须为 public static，属性如果使用 private，那就必须得到 getter&#x2F;setter 方法，这里我使用了 @Data 注解，自动生成了getter&#x2F;setter 方法\n需要使用配置变量的文件。\n@Autowiredprivate ArrsConfig config;config.getArrs()[0].get(0).kconfig.getArrs()[1].get(1).getV()\n","categories":["Java","Spring Boot"],"tags":["spring boot"]},{"title":"Python创建DataFrame","url":"/2020/04/Python%E5%88%9B%E5%BB%BADataFrame/","content":"创建 DataFrame 的几种方法。\nclass pandas.DataFrame(data=None, index: Optional[Collection] = None, columns: Optional[Collection] = None,    dtype: Union[str, numpy.dtype, ExtensionDtype, None] = None, copy: bool = False)\n\ndata 参数可以是：ndarray (structured or homogeneous), Iterable, dict, or DataFrame.Dict can contain Series, arrays, constants, or list-like objects.\n由数组&#x2F;list组成的字典创建 DataFrameimport pandas as pdd = &#123;&#x27;col1&#x27;: [1, 2], &#x27;col2&#x27;: [3, 4]&#125;df = pd.DataFrame(d)df\tcol1\tcol20\t1\t31\t2\t4\n\n看看创建的 DataFrame 元素的类型。\ndf.dtypescol1    int64col2    int64dtype: object\n\n如果要修改类型\nimport numpy as npdf2 = pd.DataFrame(d, dtype=np.int8)df2.dtypescol1    int8col2    int8dtype: object\n\n由 Series 组成的字典创建 DataFramedf3 = pd.DataFrame(&#123;&#x27;col1&#x27;: pd.Series([1, 2]), &#x27;col2&#x27;: pd.Series([3, 4])&#125;)df3    col1\tcol20\t1\t31\t2\t4\n\n由字典组成的列表创建 DataFramedf4 = pd.DataFrame([&#123;&#x27;col1&#x27;: 1, &#x27;col2&#x27;: 3&#125;, &#123;&#x27;col1&#x27;: 2, &#x27;col2&#x27;: 4&#125;])df4    col1\tcol20\t1\t31\t2\t4\n\n由字典组成的字典创建 DataFramecolumn 为父字典的 key，index 为子字典的 key。\ndf4 = pd.DataFrame(&#123;&#x27;col1&#x27;: &#123;&#x27;idx1&#x27;: 1, &#x27;idx2&#x27;: 2&#125;, &#x27;col2&#x27;: &#123;&#x27;idx1&#x27;: 3, &#x27;idx2&#x27;: 4&#125;&#125;)df4        col1\tcol2idx1\t1\t3idx2\t2\t4\n\n由二维数组创建 DataFramedf5 = pd.DataFrame([[1, 3], [2, 4]])df5    0\t10\t1\t31\t2\t4\n\n当然，也可以自定义 index 和 column\ndf6 = pd.DataFrame([[1, 3], [2, 4]], index=[&#x27;a&#x27;, &#x27;b&#x27;], columns=[&#x27;c1&#x27;, &#x27;c2&#x27;])df6\tc1\tc2a\t1\t3b\t2\t4","categories":["Python","Pandas"],"tags":["python","pandas"]},{"title":"Spring事务的传播机制","url":"/2020/04/Spring%E4%BA%8B%E5%8A%A1%E7%9A%84%E4%BC%A0%E6%92%AD%E6%9C%BA%E5%88%B6/","content":"两个 Service 下面的方法，互相调用，这时就会涉及到事务的传播。\n传播机制类型\n这些类型都是针对被调用的方法来说的。\nPROPAGATION_REQUIRED (默认)支持当前事务，如果当前没有事务，则新建事务\n如果当前存在事务，则加入当前事务，合并成一个事务\nREQUIRES_NEW新建事务，如果当前存在事务，则把当前事务挂起\n这个方法会独立提交事务，不受调用者的事务影响，父级异常，它也是正常提交\nNESTED如果当前存在事务，它将会成为父级事务的一个子事务，方法结束后并没有提交，只有等父事务结束才提交\n如果当前没有事务，则新建事务\n如果它异常，父级可以捕获它的异常而不进行回滚，正常提交\n但如果父级异常，它必然回滚，这就是和 REQUIRES_NEW 的区别\nSUPPORTS如果当前存在事务，则加入事务\n如果当前不存在事务，则以非事务方式运行，这个和不写注解是一样\nNOT_SUPPORTED以非事务方式运行\n如果当前存在事务，则把当前事务挂起\nMANDATORY如果当前存在事务，则运行在当前事务中\n如果当前无事务，则抛出异常，也即父级方法必须有事务\nNEVER以非事务方式运行，如果当前存在事务，则抛出异常，即父级方法必须无事务。\n","categories":["Java","Spring"],"tags":["spring","transaction"]},{"title":"Win10环境修改Jupyter Notebook默认文件夹位置","url":"/2020/04/Win10%E7%8E%AF%E5%A2%83%E4%BF%AE%E6%94%B9Jupyter-Notebook%E5%90%AF%E5%8A%A8%E6%96%87%E4%BB%B6%E5%A4%B9%E4%BD%8D%E7%BD%AE/","content":"Mac 环境上修改 Jupyter Notebook 启动文件夹地址，请参考 Mac环境修改Jupyter Notebook启动文件夹位置。\n在 Windows10 环境下安装了 Anaconda 或者 Miniconda3。\n命令行方式在 Anaconda 或者 Miniconda3 的 Prompt 命令行窗口，输入命令时传入地址参数，如下：\njupyter notebook D:\\development\\python\n\n桌面快捷方式打开 Jupyter Notebook 时，文件夹的默认位置不是想要的位置，这时应该按如下方法进行修改。\n找到 Jupyter Notebook 的快捷方式，查看属性，修改目标栏目的内容。\n\n\n把%USERPROFILE%/改为你想要保存文件的路径，如图所示：\n\n\n然后重新启动 Jupyter Notebook，页面就会自动加载你设置文件夹里面的内容了。\n","categories":["Python","Jupyter Notebook"],"tags":["python","jupyter notebook"]},{"title":"Spring Boot集成Alibaba Druid数据库连接池","url":"/2020/04/Spring-Boot%E9%9B%86%E6%88%90Alibaba-Druid%E6%95%B0%E6%8D%AE%E5%BA%93%E8%BF%9E%E6%8E%A5%E6%B1%A0/","content":"我们在 Spring Boot 项目中通过 Druid Spring Boot Starter 来集成 Druid 数据库连接池和监控。\n加入依赖添加 druid-spring-boot-starter maven 依赖。\n截止 Nov, 2019, 最新版本是 1.1.21。\n&lt;dependency&gt;   &lt;groupId&gt;com.alibaba&lt;/groupId&gt;   &lt;artifactId&gt;druid-spring-boot-starter&lt;/artifactId&gt;   &lt;version&gt;1.1.21&lt;/version&gt;&lt;/dependency&gt;\n\n添加配置在 application.yml 配置文件里面添加如下必填信息。注：H2 内存数据库啥也不填都可以。\nspring.datasource.druid.url= # 或spring.datasource.url= spring.datasource.druid.username= # 或spring.datasource.username=spring.datasource.druid.password= # 或spring.datasource.password=spring.datasource.druid.driver-class-name= #或 spring.datasource.driver-class-name=\n\nspring:  # jdbc配置  datasource:    url: jdbc:mysql://localhost:3306/springboot-mybatis-plus?useUnicode=true&amp;characterEncoding=utf8&amp;useSSL=false    username: root    password: password    driver-class-name: com.mysql.cj.jdbc.Driver    druid:      initial-size: 5      max-active: 5      min-idle: 5\n\n如果使用 MySQL，还需要添加 MySQL 驱动依赖。\n&lt;dependency&gt;    &lt;groupId&gt;mysql&lt;/groupId&gt;    &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt;&lt;/dependency&gt;\n\n不然就会报错 java.lang.ClassNotFoundException: com.mysql.jdbc.Driver。\n还有添加 jdbc starter。\n&lt;dependency&gt;    &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;    &lt;artifactId&gt;spring-boot-starter-jdbc&lt;/artifactId&gt;    &lt;exclusions&gt;        &lt;exclusion&gt;            &lt;artifactId&gt;HikariCP&lt;/artifactId&gt;            &lt;groupId&gt;com.zaxxer&lt;/groupId&gt;        &lt;/exclusion&gt;    &lt;/exclusions&gt;&lt;/dependency&gt;\n\n不然就会报错：java.lang.ClassNotFoundException: org.springframework.dao.DataAccessException。\n当然，如果使用 mybatis plus 这类框架，可以直接添加 mybatis-plus-boot-starter，它也会依赖 jdbc 的。\n\n\n&lt;dependency&gt;    &lt;groupId&gt;com.baomidou&lt;/groupId&gt;    &lt;artifactId&gt;mybatis-plus-boot-starter&lt;/artifactId&gt;    &lt;version&gt;3.3.1.tmp&lt;/version&gt;    &lt;exclusions&gt;        &lt;exclusion&gt;            &lt;artifactId&gt;HikariCP&lt;/artifactId&gt;            &lt;groupId&gt;com.zaxxer&lt;/groupId&gt;        &lt;/exclusion&gt;    &lt;/exclusions&gt;&lt;/dependency&gt;\n\n值得注意的是，默认会添加 HikariCP 的依赖，我们使用了 Druid 后，需要把 HikariCP 排除在外。\n最后，我们可以打印看看 dataSource:\n&#123;\tCreateTime:&quot;2020-04-06 15:04:34&quot;,\tActiveCount:0,\tPoolingCount:5,\tCreateCount:5,\tDestroyCount:0,\tCloseCount:0,\tConnectCount:0,\tConnections:[\t\t&#123;ID:1011276990, ConnectTime:&quot;2020-04-06 15:04:34&quot;, UseCount:0, LastActiveTime:&quot;2020-04-06 15:04:34&quot;&#125;,\t\t&#123;ID:1750563752, ConnectTime:&quot;2020-04-06 15:04:34&quot;, UseCount:0, LastActiveTime:&quot;2020-04-06 15:04:34&quot;&#125;,\t\t&#123;ID:285781448, ConnectTime:&quot;2020-04-06 15:04:34&quot;, UseCount:0, LastActiveTime:&quot;2020-04-06 15:04:34&quot;&#125;,\t\t&#123;ID:145329976, ConnectTime:&quot;2020-04-06 15:04:34&quot;, UseCount:0, LastActiveTime:&quot;2020-04-06 15:04:34&quot;&#125;,\t\t&#123;ID:33558975, ConnectTime:&quot;2020-04-06 15:04:34&quot;, UseCount:0, LastActiveTime:&quot;2020-04-06 15:04:34&quot;&#125;\t]&#125;\n","categories":["Java","Spring Boot"],"tags":["spring boot","druid"]},{"title":"万矿WindQuant API函数简介","url":"/2020/04/%E4%B8%87%E7%9F%BFWindQuant-API%E5%87%BD%E6%95%B0%E7%AE%80%E4%BB%8B/","content":"万矿这个API，取的名字很不好记，完全猜不出是什么单词的简写。\nwsd 日期序列用于提取多品种单指标或者单品种多指标的时间序列数据。就是说要么是一只股票和多个指标或者是多个股票的一个指标。\n一只股票和多个指标，index 为日期，指标名为 columns 的名字。\n多个股票和一个指标，index 为日期，股票名为 columns 的名字。\n\n每次限8000单元格；\n技术指标、技术形态、部分融资融券指标单次限取2000个单元格（大于2000个单元格，可分多次获取）\n\nwss 多维数据用于提取多品种多指标在某个时间点的截面数据。\n\n每次限8000单元格；\n技术指标、技术形态、部分融资融券指标单次限取2000个单元格（大于2000个单元格，可分多次获取）\n\nwset 数据集用来获取数据集信息，主要用于获取板块成分、指数成分以及各证券品种的专题统计报表数据。\n\n停牌、复牌股票：一次最长获取一个月时间段内的数据；\n分红送转：一次最长获取10年时间段内的数据；\n期货合约持仓排名：一次最长获取一个月时间段内的数据\n\nwses 板块时序获取选定股票板块一段时间内的历史序列数据。\n\n支持多板块，但仅支持单指标；\n提取多板块、单指标时，时间跨度＜250个交易日、板块数＜20；\n提取单板块、单指标时，时间跨度约800个周期。\n\nwsee 板块多维获取选定股票板块的历史截面数据。\n\n支持多板块、多指标且板块数*指标数≤1000\n\nwsi 分钟数据主要包括证券的基本行情和部分技术指标的分钟数据。\n\n目前只支持上交、深交、中金、上期、大商、郑商的行情；\n每次取一个code的时候，可以取最近3年数据(推荐使用此方法)；\n每次取多个code的时候，有如下限制：code数*天数&lt;&#x3D;100\n\nwst 日内跳价主要包括证券的日内盘口买卖五档快照数据和分时成交(tick数据)。\n\n目前只支持上交、深交、中金、上期、大商、郑商的行情；\n可取最近7个交易日的数据；\n一次只能取1个Windcode。\n\nwsq 实时行情主要包括各证券当天指标实时数据，可以一次性请求实时快照数据，也可以通过订阅的方式获取实时数据。\n","categories":["Quant"],"tags":["quant"]},{"title":"Windows 10环境生成Github SSH公钥","url":"/2020/04/Windows-10%E7%8E%AF%E5%A2%83%E7%94%9F%E6%88%90Github-SSH%E5%85%AC%E9%92%A5/","content":"不管是 Windows 还是 Linux，想要不输入用户名和密码操作 Git，只需要把 id_rsa.pub 公钥复制粘贴到 Github 里面就行了。\n安装git打开 Git Bash$ ssh-keygen -t rsa -C &quot;simon@finolo.gy&quot;Generating public/private rsa key pair.Enter file in which to save the key (/c/Users/simon/.ssh/id_rsa):Enter passphrase (empty for no passphrase):Enter same passphrase again:Your identification has been saved in /c/Users/simon/.ssh/id_rsa.Your public key has been saved in /c/Users/simon/.ssh/id_rsa.pub.The key fingerprint is:SHA256:epRQbzd91E6aLp3aM+ym2FEZMgLNCkA108nb5lsk7CM simon@finolo.gyThe key&#x27;s randomart image is:+---[RSA 3072]----+|   .oo+oo+      o||      .++oo  . .o||      .. =+ = o=.||       .oo=o.+o+.||        S+ o oo. ||       oE + o.+  ||      . .. +.=   ||       .  .o..*  ||          . o+.o |+----[SHA256]-----+\n\n这个邮箱可以随便输入的，并不需要是 Github.com 的登录帐户。\n拷贝 id_rsa.pub 内容到 GithubSettings -&gt; SSH and GPG keys -&gt; New SSH key\n把前面生成的 &#x2F;c&#x2F;Users&#x2F;simon&#x2F;.ssh&#x2F;id_rsa.pub 文件内容粘贴上去就可以了。\n","categories":["Tools","Git"],"tags":["github"]},{"title":"如何下载网页视频","url":"/2020/04/%E5%A6%82%E4%BD%95%E4%B8%8B%E8%BD%BD%E7%BD%91%E9%A1%B5%E8%A7%86%E9%A2%91/","content":"有时候看到喜欢的视频，想把它下载下来，如何下载呢？\n有些网页上的视频，是分成多个 ts 文件片段的，我们可以通过 Chrome 的开发者工具发现。\n通过 m3u8 过滤，我们可以发现一个独立的 m3u8 文件，这个文件就是播放列表，如图：\n\n\n我们可以到这个网站 www.ffmpeg.org 去下载这个 ffmpeg 工具。然后执行命令，把视频片段拼接起来。\n命令如下：\nsudo ./ffmpeg  -i &lt;m3u8 url&gt; -c copy output.mp4\n\n然后就开始下载了，非常好用。\n","categories":["Tools"]},{"title":"关于股票市值的几个指标差异","url":"/2020/04/%E5%85%B3%E4%BA%8E%E8%82%A1%E7%A5%A8%E5%B8%82%E5%80%BC%E7%9A%84%E5%87%A0%E4%B8%AA%E6%8C%87%E6%A0%87%E5%B7%AE%E5%BC%82/","content":"使用万得数据时，获取市值信息时，有如下几个关于市值的指标。\nev 总市值1上市公司的股权公平市场价值。对于一家多地上市公司，区分不同类型的股份价格和股份数量分别计算类别市值，然后加总。 注：优先股不适用此类估值方法，指定证券的类型为优先股时，该指标返回为空。\n这个指标，一般在行情软件里面是看不到的。各地股数 * 各地价格，最后再相加。\nmkt_cap_ard 总市值2按指定证券价格乘指定日总股本计算上市公司在该市场的估值。该总市值为计算PE、PB等估值指标的基础指标。暂停上市期间或退市后该指标不计算。 注：优先股不适用此类估值方法：指定证券的类型为优先股时，该指标返回为空。\n总股本 * 股价\n我们在行情软件里面一般就是看到的这个值。\nmkt_cap_csrc 证监会市值所有A股+B股的市值，不包括H股。\nrt_mkt_cap 总市值这个应该是实行行情接口才能使用。\nmkt_cap不可回测，目前还不知道有什么作用，先留在这里。\n","categories":["Quant"],"tags":["quant"]},{"title":"快速从Github Clone下载项目","url":"/2020/04/%E5%BF%AB%E9%80%9F%E4%BB%8EGithub-Clone%E4%B8%8B%E8%BD%BD%E9%A1%B9%E7%9B%AE/","content":"在大陆从 github 克隆项目，速度非常慢。\n\n\n以这种速度，稍微大一点的项目，git clone 一个晚上也下载不完，而且还会经常因为网络问题而失败。按网上说的设置代理，或者配置 hosts 记录，也行不通。\n后来发现了码云(https://gitee.com/)这个网站，我们可以通过它来下载，相当做一个中转站。\n注册、登录，然后选择点击下拉菜单 从 GitHub / GitLab 导入仓库。\n\n\n然后开始导入仓库，从 URL 导入。\n\n\n输入 github 项目的 URL，很快就可以导入成功了。\n项目导入成功后，在安全设置里面添加SSH公钥，接着，就可以非常快的执行 git clone 命令了。\n最后，我们只需要把远程仓库的地址(remote repository)改回 github 上面的地址就可以了。\n\n\n这时，就跟你从 github 仓库上 git clone 下来的一样了。非常方便。\n","categories":["Tools","Git"],"tags":["github"]},{"title":"使用Python matplotlib画图","url":"/2020/04/%E4%BD%BF%E7%94%A8Python-matplotlib%E7%94%BB%E5%9B%BE/","content":"import numpy as npimport pandas as pd\n\n一、matplotlib 中一幅图的构成\n层级结构：figure 图 ～ axes 坐标系（绘图区域） ～ axis 坐标轴（y轴 Yaxis 和 x轴 Xaxis） ～ ticks 刻度（主刻度 MajorTicks 和次刻度 MinorTicks） \n\n\n其他元素：标题（suptitle 图的中心标题 和 title 坐标系的子标题）、轴标签（xlabel 和 ylabel）、刻度标签（xticklabels 和 yticklabels）、轴脊线（spines）、图例（legend）、网格（grid）、各类图形（lines 线图、bar 柱状图、scatter散点图、hist 直方图、pie 饼图 ……）\n\n\n\n二、如何用 matplotlib 画图 ?matplotlib 共有如下 2 种绘图方式： \n2.1 方式 1：基于 pyplot 接口的绘图方式pyplot 接口是使 matplotlib 像 MATLAB 一样工作的**命令样式函数的集合**，每个 pyplot 函数都会对图形进行一些更改：例如，创建图形，在图形中创建绘图区域，在绘图区域中绘制曲线，用标签装饰绘图等。\n\nmatplotlib.pyplot 官方文档：https://matplotlib.org/tutorials/introductory/pyplot.html#sphx-glr-tutorials-introductory-pyplot-py\n\nimport matplotlib.pyplot as plt   # 首先导入 pyplot 模块\n\n\nimport numpy as npnp.linspace(0, 2, 100)\n\n\n\n\narray([0.        , 0.02020202, 0.04040404, 0.06060606, 0.08080808,\n       0.1010101 , 0.12121212, 0.14141414, 0.16161616, 0.18181818,\n       0.2020202 , 0.22222222, 0.24242424, 0.26262626, 0.28282828,\n       0.3030303 , 0.32323232, 0.34343434, 0.36363636, 0.38383838,\n       0.4040404 , 0.42424242, 0.44444444, 0.46464646, 0.48484848,\n       0.50505051, 0.52525253, 0.54545455, 0.56565657, 0.58585859,\n       0.60606061, 0.62626263, 0.64646465, 0.66666667, 0.68686869,\n       0.70707071, 0.72727273, 0.74747475, 0.76767677, 0.78787879,\n       0.80808081, 0.82828283, 0.84848485, 0.86868687, 0.88888889,\n       0.90909091, 0.92929293, 0.94949495, 0.96969697, 0.98989899,\n       1.01010101, 1.03030303, 1.05050505, 1.07070707, 1.09090909,\n       1.11111111, 1.13131313, 1.15151515, 1.17171717, 1.19191919,\n       1.21212121, 1.23232323, 1.25252525, 1.27272727, 1.29292929,\n       1.31313131, 1.33333333, 1.35353535, 1.37373737, 1.39393939,\n       1.41414141, 1.43434343, 1.45454545, 1.47474747, 1.49494949,\n       1.51515152, 1.53535354, 1.55555556, 1.57575758, 1.5959596 ,\n       1.61616162, 1.63636364, 1.65656566, 1.67676768, 1.6969697 ,\n       1.71717172, 1.73737374, 1.75757576, 1.77777778, 1.7979798 ,\n       1.81818182, 1.83838384, 1.85858586, 1.87878788, 1.8989899 ,\n       1.91919192, 1.93939394, 1.95959596, 1.97979798, 2.        ])\n\n# 基于 pyplot 接口 绘图的 简单例子x = np.linspace(0, 2, 100)  plt.plot(x, x, label=&#x27;linear&#x27;)  # 调用 plot() 时，底层会默默的创建好图、坐标系、坐标轴，直接画就行plt.plot(x, x**2, label=&#x27;quadratic&#x27;)  plt.plot(x, x**3, label=&#x27;cubic&#x27;)plt.xlabel(&#x27;x label&#x27;)  # 添加坐标轴的标签plt.ylabel(&#x27;y label&#x27;)plt.title(&quot;Simple Plot&quot;)  # 添加图标题plt.legend() # 添加图例\n\n\n\n\n&lt;matplotlib.legend.Legend at 0x123cff950&gt;\n\n\n\n\n\nnames = [&#x27;group_a&#x27;, &#x27;group_b&#x27;, &#x27;group_c&#x27;]values = [1, 10, 100]plt.subplot(131)  # 往图中添加子图，返回的是子图的坐标系 axesplt.bar(names, values)\n\n\n\n\n&lt;BarContainer object of 3 artists&gt;\n\n\n\n\n\nnames = [&#x27;group_a&#x27;, &#x27;group_b&#x27;, &#x27;group_c&#x27;]values = [1, 10, 100]plt.subplot(131)  # 往图中添加子图，返回的是子图的坐标系 axesplt.bar(names, values)plt.subplot(132)plt.scatter(names, values)plt.subplot(133)plt.plot(names, values)plt.suptitle(&#x27;Categorical Plotting&#x27;)plt.show()\n\n\n\n\n\n\npyplot-style的绘图方式：易学易上手（直接调用函数，无需了解底层结构，黑箱）、更适合在 jupyter notebook 中进行交互式画图（边画边出结果）；\n\n2.2 方式 2：面向对象的绘图方式（object-oriented (OO) style）面向对象的绘图方式首先要创建好图 figure 和坐标系 axes，然后以坐标系为基础，直接调用坐标系上的各种绘图方法绘制图形。\n虽然 OO-style 的绘图方式有别与 pyplot-style 绘图方式，但图 figure 和坐标系 axes 还是需要通过调用 pyplot 接口来创建。\n\nmatplotlib.figure.Figure 官方文档： https://matplotlib.org/api/_as_gen&#x2F;matplotlib.figure.Figure.html#matplotlib.figure.Figure \n\nmatplotlib.axes.Axes 官方文档： https://matplotlib.org/api/axes_api.html#matplotlib.axes.Axes\n\n\nimport matplotlib.pyplot as plt   # 首先导入 pyplot 模块fig, ax = plt.subplots()  # 创建图和坐标系，然后在坐标系中添加图形和相关元素\n\n\n\n\n\n\nfig, ax = plt.subplots(figsize=(10, 8))  # 同时创建了 图 figure 和 坐标系 axesx = np.linspace(0, 2, 100)ax.plot(x, x, label=&#x27;linear&#x27;)   ax.plot(x, x**2, label=&#x27;quadratic&#x27;)   ax.plot(x, x**3, label=&#x27;cubic&#x27;)   ax.set_xlabel(&#x27;x label&#x27;)  ax.set_ylabel(&#x27;y label&#x27;)  ax.set_title(&quot;Simple Plot&quot;)  ax.legend()  plt.show()# 如何放大图形 figsize=(10,8)\n\n\n\n\n\n\nOO-style 的绘图方式：需要了解绘图的底层结构和绘图逻辑、比 pyplot-style 更为灵活和复杂、更适用于非交互式绘图（如在较大项目中作为一部分重复使用的函数或在脚本中编写绘图函数）。\n\n三、一些常用图元素的实现3.1 如何创建图和坐标系 ？主要包括：\n\n3.1.1 创建 figure；\n\n3.1.2 创建坐标系；\n\n3.1.3 创建子图；\n\n3.1.4 创建分布不规则的子图；\n\n3.1.5 总结 。\n\n\n3.1.1 创建 figure方式1（pyplot-style）：plt.figure()\n\nmatplotlib.pyplot.figure(num=None, figsize=None, dpi=None, facecolor=None, edgecolor=None, frameon=True, FigureClass=&lt;class &#x27;matplotlib.figure.Figure&#x27;&gt;, clear=False, **kwargs)返回的是：Figure 对象\n# 方式1：plt.figure()fig = plt.figure(num=2,figsize=(10,8), facecolor=&#x27;#f5f5f5&#x27;)  # 创建了一个图，没有坐标系的图\n\n\n&lt;Figure size 720x576 with 0 Axes&gt;\n\nprint(fig)\n\nFigure(720x576)\n\n方式2：plt.subplots()，同时创建 figure 和 多幅子图 subplot \n\nmatplotlib.pyplot.subplots(nrows=1, ncols=1, sharex=False, sharey=False, squeeze=True, subplot_kw=None, gridspec_kw=None, **fig_kw)返回的是：Figure 对象和 axes.Axes 对象\n# 方式2：plt.subplots()fig,ax = plt.subplots(facecolor=&#x27;#f5f5f5&#x27;)  # 同时生成了坐标系，所以能在眼前展示出来\n\n\n\n\n\n\nprint(fig)\n\nFigure(432x288)\n\n3.1.2 创建坐标系方式1（pyplot-style）：plt.axes() 在创建完 figure 后接着创建 坐标系\n\n\nplt.axes()plt.axes(rect=[left, bottom, width, height], projection=None, polar=False, **kwargs)  plt.axes(ax)其中：left 代表坐标系左边到 Figure 左边的水平距离；bottom 代表坐标系底边到 Figure 底边的垂直距离；width 代表坐标系的宽度；height 代表坐标系的高度\n\n\n# 方式1plt.figure( facecolor=&#x27;#f5f5f5&#x27;)plt.axes([0,0,0.5,0.5])plt.axes([0.2,0.2,0.5,0.5])plt.axes([0.5,0.5,0.5,0.5])\n\n\n\n\n\n\n\n\n方式2（OO-style）：Figure.add_axes() 基于 figure 对象，在 figure 上添加坐标系\n\n\nax = fig.add_axes(rect=[left, bottom, width, height], projection=None, polar=False, **kwargs)  # 添加坐标系ax = fig.add_axes(ax) ax = fig.delaxes(ax)  # 删除坐标系返回：axes.Axes 对象\n\n\n# 简单的例子fig = plt.figure()fig.add_axes([0,0,0.5,0.5])fig.add_axes([0.2,0.2,0.5,0.5])fig.add_axes([0.5,0.5,0.5,0.5], frame_on=False)  # 不显示坐标系的矩形补丁ax = fig.add_axes([0.8,0.8,0.5,0.5], polar=True)#ax = fig.add_axes([0.8,0.8,0.5,0.5], projection=&#x27;polar&#x27;)fig.delaxes(ax)fig.add_axes(ax)\n\n\n\n\n&lt;matplotlib.projections.polar.PolarAxes at 0x1240876d0&gt;\n\n\n\n\n3.1.3 创建子图方式 1（pyplot-style）：plt.subplots()，同时创建 figure 和 多幅子图（子图对应的还是坐标系 axes 这个对象） \n常用形式：\n\n\nfig, ax = plt.subplots()  #默认下只创建一幅子图fig, axs = plt.subplots(2, 2)  #创建了4幅子图，2*2 的子图矩阵fig, (ax1, ax2) = plt.subplots(1, 2)  #可通过元组拆分的形式获取相应的子图fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2)\n\n\nx = np.linspace(0, 2*np.pi, 400)y = np.sin(x**2)fig, (ax1, ax2) = plt.subplots(1, 2, sharey=True)ax1.plot(x, y)ax1.set_title(&#x27;Sharing Y axis&#x27;)ax2.scatter(x, y)\n\n\n\n\n&lt;matplotlib.collections.PathCollection at 0x1242f4a50&gt;\n\n\n\n\n\nfig, axs = plt.subplots(1, 2, sharey=True)print(axs)  # 多个子图返回的是制图 的list，可通过索引方式获取相应的子图axs[0].plot(x,y)\n\n[&lt;matplotlib.axes._subplots.AxesSubplot object at 0x124347990&gt;\n &lt;matplotlib.axes._subplots.AxesSubplot object at 0x124322610&gt;]\n\n\n\n\n\n[&lt;matplotlib.lines.Line2D at 0x124407250&gt;]\n\n\n\n\n\n\n\n方式 2（pyplot-style）： plt.subplot() 往创建的 figure 中添加一副子图\n\n\nplt.subplot(nrows, ncols, index, **kwargs)  # n*n 子图矩阵中的第 index 幅子图plt.subplot(pos, **kwargs)plt.subplot(ax)\n\n\nplt.subplot(221)ax1=plt.subplot(2, 2, 1)  # 与上面是等价的ax2=plt.subplot(222, frame_on=False)  # 没有框架plt.subplot(223, projection=&#x27;polar&#x27;)plt.subplot(224, sharex=ax2, facecolor=&#x27;red&#x27;)  # 与第一幅图的坐标共享X轴# plt.delaxes(ax2)# plt.subplot(ax2)\n\n\n\n\n\n\n\n方式 3（OO-style）： Figure.add_subplot() 往 figure 中添加子图的坐标系\n\n\nfig.add_subplot(nrows, ncols, index, **kwargs)fig.add_subplot(pos, **kwargs)fig.add_subplot(ax)fig.add_subplot()\n\n\nfig = plt.figure()fig.add_subplot(221)ax1 = fig.add_subplot(2, 2, 1)ax2 = fig.add_subplot(222, frameon=False)fig.add_subplot(223, projection=&#x27;polar&#x27;)fig.add_subplot(224, sharex=ax1, facecolor=&#x27;red&#x27;)#fig.delaxes(ax2)#fig.add_subplot(ax2)\n\n\n\n\n\n\n\n3.1.4 创建分布不规则的子图方式 1：使用 gridspec.GridSpec()函数 \n首先：需要借助 gridspec 模块中的 GridSpec 函数将 figure 分割成几个基本单元（一个矩阵）\n\n\nimport matplotlib.gridspec as gridspecmatplotlib.gridspec.GridSpec(nrows, ncols, figure=None, left=None, bottom=None, right=None, top=None, wspace=None, hspace=None, width_ratios=None, height_ratios=None)\n\n然后，结合 plt.subplot() 函数或fig.add_subplot() 函数，通过组合不同位置上的基本单元来分布子图的大小和位置。\n\n绘制不规则子图各类方法的官方文档： Customizing Figure Layouts Using GridSpec and Other Functions\n\n# 结合 plt.subplot()的方式import matplotlib.gridspec as gridspecgs =gridspec.GridSpec(3,3)ax1 = plt.subplot(gs[0, :]) # identical to ax1 = plt.subplot(gs.new_subplotspec((0, 0), colspan=3))ax1.text(0.5, 0.5, &#x27;ax1&#x27;, va=&quot;center&quot;, ha=&quot;center&quot;)ax2 = plt.subplot(gs[1, :-1])ax2.text(0.5, 0.5, &#x27;ax2&#x27;, va=&quot;center&quot;, ha=&quot;center&quot;)ax3 = plt.subplot(gs[1:, -1])ax3.text(0.5, 0.5, &#x27;ax3&#x27;, va=&quot;center&quot;, ha=&quot;center&quot;)ax4 = plt.subplot(gs[-1, 0])ax4.text(0.5, 0.5, &#x27;ax4&#x27;, va=&quot;center&quot;, ha=&quot;center&quot;)ax5 =plt.subplot(gs[-1, -2])ax5.text(0.5, 0.5, &#x27;ax5&#x27;, va=&quot;center&quot;, ha=&quot;center&quot;)plt.show()\n\n\n\n\n\n\n# 结合 fig.add_subplot() 的方式import matplotlib.gridspec as gridspecdef format_axes(fig):    for i, ax in enumerate(fig.axes):        ax.text(0.5, 0.5, &quot;ax%d&quot; % (i+1), va=&quot;center&quot;, ha=&quot;center&quot;)        ax.tick_params(labelbottom=False, labelleft=False)  # 隐藏坐标刻度标签fig = plt.figure(constrained_layout=True)gs =gridspec. GridSpec(3, 3, figure=fig)ax1 = fig.add_subplot(gs[0, :]) # identical to ax1 = plt.subplot(gs.new_subplotspec((0, 0), colspan=3))ax2 = fig.add_subplot(gs[1, :-1])ax3 = fig.add_subplot(gs[1:, -1])ax4 = fig.add_subplot(gs[-1, 0])ax5 = fig.add_subplot(gs[-1, -2])format_axes(fig)\n\n\n\n\n\n方式 2：在 figure 上直接调用fig.add_gridspec()函数进行图形分割，然后使用fig.add_subplot()获取绘图的组合区域 \n\n\nfig.add_gridspec(self, nrows, ncols, **kwargs)\n\n\nfig = plt.figure()gs = fig.add_gridspec(2, 2)ax1 = fig.add_subplot(gs[0, 0])ax2 = fig.add_subplot(gs[1, 0])ax3 = fig.add_subplot(gs[:, 1])  # 合并两行\n\n\n\n\n\n3.1.5 总结\n**无论是 pyplot-style 绘图方式还是 OO-style 绘图方式，都需要调用 pyplot 接口。**前者直接调用  pyplot 接口中的函数进行画图，后者需要借助  pyplot 接口来显示的构建最顶层的绘图元素 Figure （Figure is the top level container for all the plot elements.）或是显示的构建 坐标系 Axes （The Axes contains most of the figure elements: Axis, Tick, Line2D, Text, Polygon, etc., and sets the coordinate system.），若不显示的构建 Figure 对象或 Axes 对象，就无法调用这两个对象的各类绘图方法进行  OO-style 绘图。\n\n推荐使用一次性生成图和子图坐标系的方式 fig, ax = plt.subplots() 来画图，操作方便且高效。\n\n\n3.2 如何设置坐标轴和刻度？（基于 OO-style）\n先弄清楚 5 个概念：\n major ticks :  主刻度\n minor ticks ：次刻度\n tick locator : 刻度定位器，用于设置刻度的位置\n tick formatting：刻度格式，用于设置刻度标签的格式\n axis label : 轴标签，用于设置 轴 的名字\n\nmatplotlib.axis 模块中包含 2 个对象：\n对象1：轴（matplotlib.axis.Axis），包括 x 轴（matplotlib.axis.XAxis）和 y 轴（matplotlib.axis.YAxis），对应一系列方法（get_xxx()、set_xxx()）；\n对象2：刻度（matplotlib.axis.Tick），包括  x 轴 上的刻度（matplotlib.axis.XTick） 和 y 轴上的刻度（matplotlib.axis.YTick），同样对应一系列方法（get_xxx()、set_xxx()）。\n\nmatplotlib.axis 官方文档： https://matplotlib.org/api/axis_api.html\n\nmatplotlib.ticker 官方文档： https://matplotlib.org/api/ticker_api.html\n\n\n3.2.1 设置轴刻度位置（主刻度位置和次刻度位置）\n\nax.xaxis.set_major_locator(xmajor_locator)  # 设置 x 轴的主次刻度位置ax.xaxis.set_minor_locator(xminor_locator) ax.yaxis.set_major_locator(ymajor_locator)  # 设置 y 轴的主次刻度位置ax.yaxis.set_minor_locator(yminor_locator)\n\n当然也有对应的 get_major_locator() 和 get_minor_locator() 。\n\n matplotlib.ticker 模块中可调用的定位器\n\n\n\n\n定位器\n含义\n\n\n\nAutoLocator\n自动定位器，大多数绘图的默认刻度线定位器。\n\n\nMaxNLocator\n在最合适的位置找到带有刻度的最大间隔数。\n\n\nLinearLocator\n线性定位器，基于刻度线的个数，从最小到最大均匀分布刻度间隔。\n\n\nLogLocator\n对数定位器，刻度间隔从最小到最大取对数。\n\n\nMultipleLocator\n多重定位器，根据刻度间隔进行定位，刻度和范围是间隔的倍数，既适用于整数，也适用于浮点数。\n\n\nFixedLocator\n固定定位器，刻度线位置是固定的。\n\n\nIndexLocator\n索引定位器，适用于带有索引数据的刻度进行定位，例如 where x &#x3D; range(len(y))\n\n\nNullLocator\n空定位器，不对刻度进行定位，不会显示刻度。\n\n\nSymmetricalLogLocator\n与符号规范一起使用的定位器。\n\n\nLogitLocator\n用于 logit 缩放的定位器。\n\n\nOldAutoLocator\n旧的自动定位器，选择一个MultipleLocator并动态重新分配它，以便在导航期间智能显示刻度。\n\n\nAutoMinorLocator\n轴为线性且主刻度线等距分布时，副刻度线的定位器，将主刻度间隔细分为指定数量的次间隔，根据主间隔默认为4或5。\n\n\nimport numpy as npimport matplotlib.pyplot as pltimport matplotlib.ticker as ticker   # matplotlib.ticker 模块中def setup(ax, title):    &quot;&quot;&quot;Set up common parameters for the Axes in the example.&quot;&quot;&quot;    # only show the bottom spine    ax.yaxis.set_major_locator(ticker.NullLocator())    ax.spines[&#x27;right&#x27;].set_color(&#x27;none&#x27;)    ax.spines[&#x27;left&#x27;].set_color(&#x27;none&#x27;)    ax.spines[&#x27;top&#x27;].set_color(&#x27;none&#x27;)    ax.xaxis.set_ticks_position(&#x27;bottom&#x27;)    ax.tick_params(which=&#x27;major&#x27;, width=1.00, length=5)    ax.tick_params(which=&#x27;minor&#x27;, width=0.75, length=2.5)     ax.set_xlim(0, 5)  # 默认整个X轴的取值为 0-5    ax.set_ylim(0, 1)    ax.text(0.0, 0.2, title, transform=ax.transAxes,            fontsize=14, fontname=&#x27;Monospace&#x27;, color=&#x27;tab:blue&#x27;)fig, axs = plt.subplots(8, 1, figsize=(8, 6))# Null Locatorsetup(axs[0], title=&quot;NullLocator()&quot;)axs[0].xaxis.set_major_locator(ticker.NullLocator())axs[0].xaxis.set_minor_locator(ticker.NullLocator())# Multiple Locatorsetup(axs[1], title=&quot;MultipleLocator(0.5)&quot;)axs[1].xaxis.set_major_locator(ticker.MultipleLocator(0.5))  # 基于间隔进行定位axs[1].xaxis.set_minor_locator(ticker.MultipleLocator(0.1))# Fixed Locatorsetup(axs[2], title=&quot;FixedLocator([0, 1, 5])&quot;)axs[2].xaxis.set_major_locator(ticker.FixedLocator([0, 1, 5]))  # 基于输入的刻度位置固定刻度axs[2].xaxis.set_minor_locator(ticker.FixedLocator(np.linspace(0.2, 0.8, 4)))# Linear Locatorsetup(axs[3], title=&quot;LinearLocator(numticks=3)&quot;)  # 基于刻度个数分配刻度位置axs[3].xaxis.set_major_locator(ticker.LinearLocator(3))axs[3].xaxis.set_minor_locator(ticker.LinearLocator(31))# Index Locatorsetup(axs[4], title=&quot;IndexLocator(base=0.5, offset=0.25)&quot;)axs[4].plot(range(0, 5), [0]*5, color=&#x27;white&#x27;)axs[4].xaxis.set_major_locator(ticker.IndexLocator(base=0.5, offset=0.25))    # 从 0.25 开始定位，刻度间隔为 0.5 # Auto Locatorsetup(axs[5], title=&quot;AutoLocator()&quot;)axs[5].xaxis.set_major_locator(ticker.AutoLocator())axs[5].xaxis.set_minor_locator(ticker.AutoMinorLocator())# MaxN Locatorsetup(axs[6], title=&quot;MaxNLocator(n=4)&quot;)axs[6].xaxis.set_major_locator(ticker.MaxNLocator(4))axs[6].xaxis.set_minor_locator(ticker.MaxNLocator(40))# Log Locatorsetup(axs[7], title=&quot;LogLocator(base=10, numticks=15)&quot;)axs[7].set_xlim(10**3, 10**10)axs[7].set_xscale(&#x27;log&#x27;)axs[7].xaxis.set_major_locator(ticker.LogLocator(base=10, numticks=15))plt.tight_layout()plt.show()\n\n\n\n\n\n3.2.2 设置轴刻度样式（主刻度样式和次刻度样式）\n\nax.xaxis.set_major_formatter(xmajor_formatter)  # 设置 x 轴的主次刻度样式ax.xaxis.set_minor_formatter(xminor_formatter)  ax.yaxis.set_major_formatter(ymajor_formatter)  # 设置 y 轴的主次刻度样式ax.yaxis.set_minor_formatter(yminor_formatter)  \n\n当然也有对应的 get_major_formatter() 和 get_minor_formatter() 。\n\n matplotlib.ticker 模块中可调用的格式化函数\n\n\n\n\n刻度格式\n含义\n\n\n\nNullFormatter\n刻度线上没有标签。\n\n\nIndexFormatter\n从标签列表中设置字符串。\n\n\nFixedFormatter\n固定格式器，手动设置标签的字符串。\n\n\nFuncFormatter\n用户定义的功能设置标签。\n\n\nStrMethodFormatter\n使用字符串格式方法设置刻度样式。\n\n\nFormatStrFormatter\n使用旧式的 sprintf 格式字符串。\n\n\nScalarFormatter\n标量的默认格式化程序：自动选择格式字符串。\n\n\nLogFormatter\n日志轴的格式化程序。\n\n\nLogFormatterExponent\n使用指数&#x3D; log_base（值）格式化对数轴的值。\n\n\nLogFormatterMathtext\n使用Math文本使用exponent &#x3D; log_base（value）格式化对数轴的值。\n\n\nLogFormatterSciNotation\n使用科学计数法设置对数轴的值格式。\n\n\nLogitFormatter\n概率格式器。\n\n\nEngFormatter\n以工程符号格式格式化标签。\n\n\nPercentFormatter\n将标签格式化为百分比。\n\n\nimport matplotlib.pyplot as pltimport matplotlib.ticker as tickerdef setup(ax, title):    &quot;&quot;&quot;Set up common parameters for the Axes in the example.&quot;&quot;&quot;    # only show the bottom spine    ax.yaxis.set_major_locator(ticker.NullLocator())    ax.spines[&#x27;right&#x27;].set_color(&#x27;none&#x27;)    ax.spines[&#x27;left&#x27;].set_color(&#x27;none&#x27;)    ax.spines[&#x27;top&#x27;].set_color(&#x27;none&#x27;)    # define tick positions    ax.xaxis.set_major_locator(ticker.MultipleLocator(1.00))    ax.xaxis.set_minor_locator(ticker.MultipleLocator(0.25))    ax.xaxis.set_ticks_position(&#x27;bottom&#x27;)    ax.tick_params(which=&#x27;major&#x27;, width=1.00, length=5)    ax.tick_params(which=&#x27;minor&#x27;, width=0.75, length=2.5, labelsize=10)    ax.set_xlim(0, 5)    ax.set_ylim(0, 1)    ax.text(0.0, 0.2, title, transform=ax.transAxes,            fontsize=14, fontname=&#x27;Monospace&#x27;, color=&#x27;tab:blue&#x27;)fig, axs = plt.subplots(7, 1, figsize=(8, 6))# Null formattersetup(axs[0], title=&quot;NullFormatter()&quot;)axs[0].xaxis.set_major_formatter(ticker.NullFormatter())# Fixed formattersetup(axs[1], title=&quot;FixedFormatter([&#x27;A&#x27;, &#x27;B&#x27;, &#x27;C&#x27;, ...])&quot;)# FixedFormatter should only be used together with FixedLocator.# Otherwise, one cannot be sure where the labels will end up.positions = [0, 1, 2, 3, 4, 5]labels = [&#x27;A&#x27;, &#x27;B&#x27;, &#x27;C&#x27;, &#x27;D&#x27;, &#x27;E&#x27;, &#x27;F&#x27;]axs[1].xaxis.set_major_locator(ticker.FixedLocator(positions))axs[1].xaxis.set_major_formatter(ticker.FixedFormatter(labels))  # 对固定的刻度一对一设置刻度标签样式# FuncFormatter can be used as a decorator@ticker.FuncFormatterdef major_formatter(x, pos):    return &quot;[%.2f]&quot; % xsetup(axs[2], title=&#x27;FuncFormatter(lambda x, pos: &quot;[%.2f]&quot; % x)&#x27;)axs[2].xaxis.set_major_formatter(major_formatter)# FormatStr formattersetup(axs[3], title=&quot;FormatStrFormatter(&#x27;#%d&#x27;)&quot;)axs[3].xaxis.set_major_formatter(ticker.FormatStrFormatter(&quot;#%d&quot;))  # 基于字符串格式化函数来设置# Scalar formattersetup(axs[4], title=&quot;ScalarFormatter()&quot;)axs[4].xaxis.set_major_formatter(ticker.ScalarFormatter(useMathText=True))# StrMethod formattersetup(axs[5], title=&quot;StrMethodFormatter(&#x27;&#123;x:.3f&#125;&#x27;)&quot;)axs[5].xaxis.set_major_formatter(ticker.StrMethodFormatter(&quot;&#123;x:.3f&#125;&quot;))# Percent formattersetup(axs[6], title=&quot;PercentFormatter(xmax=5)&quot;)axs[6].xaxis.set_major_formatter(ticker.PercentFormatter(xmax=5))plt.tight_layout()plt.show()\n\n\n\n\n\n3.2.3 关于轴脊线（spine）的设置\n什么是轴脊线：轴脊线-记录数据区域边界的线（An axis spine – the line noting the data area boundaries）\n\n常用形式：\n\n\n\n\nax.spines[direction].set_visible(bool)  # 是否可见ax.spines[direction].set_bounds(low=None, high=None)  # 设置边界ax.spines[direction].set_color(c)  # 设置颜色ax.spines[direction].set_position(position=(position type, amount))  # 设置位置\n\n\nx = np.linspace(0, 2 * np.pi, 100)y = 2 * np.sin(x)fig, (ax0, ax1, ax2) = plt.subplots(nrows=3, constrained_layout=True)ax0.plot(x, y)ax0.set_title(&#x27;normal spines&#x27;)ax1.plot(x, y)ax1.set_title(&#x27;bottom-left spines&#x27;)ax1.spines[&#x27;right&#x27;].set_visible(False) # 是否可见ax1.spines[&#x27;top&#x27;].set_visible(False)ax1.yaxis.set_ticks_position(&#x27;left&#x27;)  # 只显示y轴左侧的刻度ax1.xaxis.set_ticks_position(&#x27;bottom&#x27;)  # 只显示x轴底部的刻度ax2.plot(x, y)ax2.spines[&#x27;left&#x27;].set_bounds(-1, 1)  # 设置 轴脊线 的范围ax2.spines[&#x27;right&#x27;].set_visible(False)ax2.spines[&#x27;top&#x27;].set_visible(False)ax2.yaxis.set_ticks_position(&#x27;left&#x27;)ax2.xaxis.set_ticks_position(&#x27;bottom&#x27;)plt.show()\n\n\n\n\n\n3.2.4 关于轴或刻度的其他常用设置\n\nAxes.tick_params(self, axis=&#x27;both&#x27;, **kwargs)  # 更改刻度线，刻度线标签和网格线的样式Axes.set_xticks(self, ticks, *, minor=False)  # 设置 x 刻度的位置Axes.set_yticks(self, ticks, *, minor=False)  # 设置 y 刻度的位置Axes.set_xticklabels(self, labels, fontdict=None, minor=False, **kwargs)  # 设置 x 刻度标签Axes.set_yticklabels(self, labels, fontdict=None, minor=False, **kwargs)  # 设置 y 刻度标签Axes.set_xlim((left, right))  # 设置 x 轴的可视化边界 Axes.set_ylim((left, right))  # 设置 y 轴的可视化边界 Axes.set_xlabel(self, xlabel, fontdict=None, labelpad=None, **kwargs)  # 设置 x 轴的标签Axes.set_ylabel(self, yabel, fontdict=None, labelpad=None, **kwargs)  # 设置 y轴的标签\n\n\nfig, (ax0, ax1, ax2,ax3) = plt.subplots(nrows=4, constrained_layout=True)ax0.tick_params(axis=&#x27;y&#x27;, direction=&#x27;out&#x27;,  length=6, width=6, colors=&#x27;r&#x27;, grid_color=&#x27;r&#x27;, grid_alpha=0.5)ax0.tick_params(axis=&#x27;x&#x27;, direction=&#x27;out&#x27;,  labelsize=&#x27;x-large&#x27;, labelcolor=&#x27;g&#x27;, labelrotation=70)ax1.set_xticks(range(10))ax1.set_xticklabels(list(&#x27;ABCDEFGHIJ&#x27;))ax2.plot(range(8))  ax2.set_xticks(range(10))ax2.set_xlim((3,5))  # 只绘制了 3-5 这个范围的内容ax3.set_xlabel(&#x27; x 轴&#x27;)ax3.set_ylabel(&#x27; y 轴&#x27;)plt.show()\n\n\n\n\n\n3.3 如何绘制各类图形（基于 OO-style）建议大家采用面向对象的编程方式，以坐标系为基础绘制图形（前提：先显示的创建好 Figure 和 Axes）：ax.plot()、ax.bar()、ax.scatter()、ax.pie() ……\n\nmatplotlib.axes.Axes 官方文档： https://matplotlib.org/api/axes_api.html#matplotlib.axes.Axes\n\n3.3.1 常用的绘图函数对于绘图函数中各参数的含义直接参考官方文档：\n\n\nAxes.plot(self, *args, scalex=True, scaley=True, data=None, **kwargs)  # 绘制曲线\nAxes.scatter(self, x, y, s=None, c=None, marker=None, cmap=None, norm=None, vmin=None, vmax=None, alpha=None, linewidths=None, verts=&lt;deprecated parameter&gt;, edgecolors=None, *, plotnonfinite=False, data=None, **kwargs)  # 绘制散点图\nAxes.bar(self, x, height, width=0.8, bottom=None, *, align=&#x27;center&#x27;, data=None, **kwargs)  # 绘制柱状图（垂直）\nAxes.barh(self, y, width, height=0.8, left=None, *, align=&#x27;center&#x27;, **kwargs)  # 绘制柱状图（水平）\nAxes.pie(self, x, explode=None, labels=None, colors=None, autopct=None, pctdistance=0.6, shadow=False, labeldistance=1.1, startangle=None, radius=None, counterclock=True, wedgeprops=None, textprops=None, center=(0, 0), frame=False, rotatelabels=False, *, data=None)  # 绘制饼图\nAxes.step(self, x, y, *args, where=&#x27;pre&#x27;, data=None, **kwargs)   # 绘制阶梯图\nAxes.stem(self, *args, linefmt=None, markerfmt=None, basefmt=None, bottom=0, label=None, use_line_collection=False, data=None)  # 绘制茎图（棒棒糖）\nAxes.stackplot(axes, x, *args, labels=(), colors=None, baseline=&#x27;zero&#x27;, data=None, **kwargs)  # 绘制堆积图\nAxes.hist(self, x, bins=None, range=None, density=False, weights=None, cumulative=False, bottom=None, histtype=&#x27;bar&#x27;, align=&#x27;mid&#x27;, orientation=&#x27;vertical&#x27;, rwidth=None, log=False, color=None, label=None, stacked=False, *, data=None, **kwargs)  # 绘制直方图\nAxes.boxplot(self, x, notch=None, sym=None, vert=None, whis=None, positions=None, widths=None, patch_artist=None, bootstrap=None, usermedians=None, conf_intervals=None, meanline=None, showmeans=None, showcaps=None, showbox=None, showfliers=None, boxprops=None, labels=None, flierprops=None, medianprops=None, meanprops=None, capprops=None, whiskerprops=None, manage_ticks=True, autorange=False, zorder=None, *, data=None)  # 绘制箱线图\nAxes.violinplot(self, dataset, positions=None, vert=True, widths=0.5, showmeans=False, showextrema=True, showmedians=False, quantiles=None, points=100, bw_method=None, *, data=None)  # 绘制小提琴图\nAxes.table(ax, cellText=None, cellColours=None, cellLoc=&#x27;right&#x27;, colWidths=None, rowLabels=None, rowColours=None, rowLoc=&#x27;left&#x27;, colLabels=None, colColours=None, colLoc=&#x27;center&#x27;, loc=&#x27;bottom&#x27;, bbox=None, edges=&#x27;closed&#x27;, **kwargs)  # 绘制表格\nAxes.annotate(self, s, xy, *args, **kwargs)  # 绘制标签Axes.text(self, x, y, s, fontdict=None, withdash=&lt;deprecated parameter&gt;, **kwargs)\n\n3.3.2 如何叠加绘图？绘制图中常常遇到在一个坐标系中绘制多条曲线或多种图形的情况，只需：画完一条接着画另一条。\n# 同一类型，多条fig, ax = plt.subplots()  # 同时创建了 图 figure 和 坐标系 axesx = np.linspace(0, 2, 100)ax.plot(x, x, label=&#x27;linear&#x27;)   ax.plot(x, x**2, label=&#x27;quadratic&#x27;)   ax.plot(x, x**3, label=&#x27;cubic&#x27;)   ax.set_xlabel(&#x27;x label&#x27;)  ax.set_ylabel(&#x27;y label&#x27;)  ax.set_title(&quot;Simple Plot&quot;)  ax.legend()  plt.show()\n\n\n\n\n\n\n# 不同类型fig, ax = plt.subplots()  # 同时创建了 图 figure 和 坐标系 axesx = np.arange(0,20)ax.plot(x, x, label=&#x27;line&#x27;)   ax.scatter(x, x/2, label=&#x27;scatter&#x27;)ax.bar(x, x/3, label=&#x27;bar&#x27;)  ax.set_xlabel(&#x27;x label&#x27;)  ax.set_ylabel(&#x27;y label&#x27;)  ax.set_title(&quot;Simple Plot&quot;)  ax.legend()  plt.show()\n\n\n\n\n\n3.3.3 如何绘制双坐标图？利用**Axes.twinx()** 或 Axes.twiny()  创建 双轴图，一般适用于分别绘制不同数量级的数据。\nt = np.arange(0.01, 10.0, 0.01)data1 = np.exp(t)data2 = np.sin(2 * np.pi * t)fig, ax1 = plt.subplots()color = &#x27;tab:red&#x27;ax1.set_xlabel(&#x27;time (s)&#x27;)ax1.set_ylabel(&#x27;exp&#x27;, color=color)ax1.plot(t, data1, color=color)ax1.tick_params(axis=&#x27;y&#x27;, labelcolor=color)ax2 = ax1.twinx()  # 创建 x 轴的兄弟轴  color = &#x27;tab:blue&#x27;ax2.set_ylabel(&#x27;sin&#x27;, color=color)  ax2.plot(t, data2, color=color)ax2.tick_params(axis=&#x27;y&#x27;, labelcolor=color)fig.tight_layout()  plt.show()\n\n\n\n\n\n3.4 如何设置图中的图例和标题（基于OO-style）3.4.1 设置图例\n图例有关概念：\n图例条目（legend entry）：图例由一个或多个图例条目组成，一项条目仅由一个键和一个标签组成；\n图例键（legend key）：每个图例标签左侧的图案标记；\n图例标签（legend label）：描述图例键的标签；\n图例手柄（legend handle）：用于在图例中生成相应条目的原始对象。\n\n方式1：通过设置绘图函数中的 label 参数， 调用 ax.legend() 自动检测要在图例中显示的元素；\n\n方式2：在**ax.legend(label)** 中设置图标签，在图形较多时，容易混淆，不建议采用这种方式；\n\n方式3：通过定义图例中的元素来设置图例：ax.legend(handles, labels)，可通过 ax.get_legend_handles_labels() 获取图例的 handle 和 label：\n\n\n\n\n handles, labels = ax.get_legend_handles_labels()ax.legend(handles, labels)\n\n\n图例设置教程官方文档：https://matplotlib.org/tutorials/intermediate/legend_guide.html#sphx-glr-tutorials-intermediate-legend-guide-py\n\nlegend() 函数官方文档：https://matplotlib.org/api/_as_gen&#x2F;matplotlib.pyplot.legend.html#matplotlib.pyplot.legend\n\n\n&#x27;k--&#x27;\n\n\n\n\n&#39;k--&#39;\n\n# 方式1：自动检测并显示# Make some fake data.a = b = np.arange(0, 3, .02)c = np.exp(a)d = c[::-1]# Create plots with pre-defined labels.fig, ax = plt.subplots()ax.plot(a, c, &#x27;k--&#x27;, label=&#x27;Model length&#x27;)ax.plot(a, d, &#x27;k:&#x27;, label=&#x27;Data length&#x27;)ax.plot(a, c + d, &#x27;k&#x27;, label=&#x27;Total message length&#x27;)legend = ax.legend(loc=&#x27;upper center&#x27;, shadow=True, fontsize=&#x27;x-large&#x27;)# Put a nicer background color on the legend.legend.get_frame().set_facecolor(&#x27;C0&#x27;)plt.show()\n\n\n\n\n\n\n# 方式2：单独设置图例标签# Make some fake data.a = b = np.arange(0, 3, .02)c = np.exp(a)d = c[::-1]# Create plots with pre-defined labels.fig, ax = plt.subplots()ax.plot(a, c, &#x27;k--&#x27; )ax.plot(a, d, &#x27;k:&#x27; )ax.plot(a, c + d, &#x27;k&#x27;)legend = ax.legend([&#x27;Model length&#x27;,&#x27;Data length&#x27;,&#x27;Total message length&#x27; ],loc=&#x27;upper center&#x27;, shadow=True, fontsize=&#x27;x-large&#x27;)legend.get_frame().set_facecolor(&#x27;C0&#x27;)plt.show()\n\n\n\n\n\n\n# 方式3：通过 handle 和 label 设置图例t = np.arange(0.01, 10.0, 0.01)data1 = np.exp(t)data2 = np.sin(2 * np.pi * t)fig, ax1 = plt.subplots()color = &#x27;tab:red&#x27;ax1.set_xlabel(&#x27;time (s)&#x27;)ax1.set_ylabel(&#x27;exp&#x27;, color=color)ax1.plot(t, data1, color=color,label=&#x27;exp&#x27;)ax2 = ax1.twinx()  # 创建 x 轴的兄弟轴  color = &#x27;tab:blue&#x27;ax2.set_ylabel(&#x27;sin&#x27;, color=color)  ax2.plot(t, data2, color=color,label=&#x27;sin&#x27;)h1,b1 = ax1.get_legend_handles_labels()h2,b2 = ax2.get_legend_handles_labels()plt.legend(h1+h2,b1+b2)#fig.legend(h1+h2,b1+b2)fig.tight_layout()  plt.show()\n\n\n\n\n\n3.4.2 设置图标题\n情况1：给坐标系添加标题（也适用于子图）\n\n\n Axes.set_title(self, label, fontdict=None, loc=None, pad=None, **kwargs)\n\n情况2：给整幅图添加总标题（适用于图）\n\n\n\nFigure.suptitle(self, t, **kwargs)\n\n\nx = np.linspace(0, 2*np.pi, 400)y = np.sin(x**2)fig, (ax1, ax2) = plt.subplots(1, 2)ax1.plot(x, y)ax1.set_title(&#x27;子图1标题&#x27;)# ax2.scatter(x, y)# ax2.set_title(&#x27;子图2标题&#x27;)# fig.suptitle(&#x27;总标题&#x27;,fontsize=20,c=&#x27;r&#x27;)# fig.tight_layout()# plt.show()\n\n\n---------------------------------------------------------------------------\n\nNameError                                 Traceback (most recent call last)\n\n&lt;ipython-input-1-04cf4757ef1e&gt; in &lt;module&gt;\n----&gt; 1 x = np.linspace(0, 2*np.pi, 400)\n      2 y = np.sin(x**2)\n      3 fig, (ax1, ax2) = plt.subplots(1, 2)\n      4 ax1.plot(x, y)\n      5 ax1.set_title(&#39;子图1标题&#39;)\n\n\nNameError: name &#39;np&#39; is not defined\n\nimport numpy as npimport matplotlib.pyplot as pltx = np.linspace(0, 2*np.pi, 400)y = np.sin(x**2)fig, ax = plt.subplots()ax.plot(x, y)ax.set_title(&#x27;中文标题&#x27;)\n\n\n\n\nText(0.5, 1.0, &#39;中文标题&#39;)\n\n\n\n\n\nimport matplotlibmatplotlib.matplotlib_fname()\n\n‘&#x2F;opt&#x2F;anaconda3&#x2F;lib&#x2F;python3.7&#x2F;site-packages&#x2F;matplotlib&#x2F;mpl-data&#x2F;matplotlibrc’\n四、其他需注意画图事项4.1 画图风格\n画图风格展示官方文档：https://matplotlib.org/gallery/style_sheets&#x2F;style_sheets_reference.html#sphx-glr-gallery-style-sheets-style-sheets-reference-py\n\n答印所有的风格：print(plt.style.available) \n\n通过 plt.style.context(style) ，在with 代码块中进行局部设置（只对 with 代码块中绘制的图形有效）\n\n通过 plt.style.use(style)  来进行全局设置.\n\n通过保留默认风格来恢复之前的风格：plt.style.use(&#39;default&#39;)\n\n\nprint(plt.style.available)  # 打印风格\n\n[&#39;Solarize_Light2&#39;, &#39;_classic_test&#39;, &#39;bmh&#39;, &#39;classic&#39;, &#39;dark_background&#39;, &#39;fast&#39;, &#39;fivethirtyeight&#39;, &#39;ggplot&#39;, &#39;grayscale&#39;, &#39;seaborn-bright&#39;, &#39;seaborn-colorblind&#39;, &#39;seaborn-dark-palette&#39;, &#39;seaborn-dark&#39;, &#39;seaborn-darkgrid&#39;, &#39;seaborn-deep&#39;, &#39;seaborn-muted&#39;, &#39;seaborn-notebook&#39;, &#39;seaborn-paper&#39;, &#39;seaborn-pastel&#39;, &#39;seaborn-poster&#39;, &#39;seaborn-talk&#39;, &#39;seaborn-ticks&#39;, &#39;seaborn-white&#39;, &#39;seaborn-whitegrid&#39;, &#39;seaborn&#39;, &#39;tableau-colorblind10&#39;]\n\nx = np.linspace(0, 2*np.pi, 400)y = np.sin(x**2)with plt.style.context(&#x27;ggplot&#x27;):   #将use换成context    plt.plot(x,y,label=&#x27;sin&#x27;)    plt.legend()    pass\n\n\n\n\n\n\nx = np.linspace(0, 2*np.pi, 400)y = np.sin(x**2)plt.plot(x,y,label=&#x27;sin&#x27;)  # with 之外的风格仍保持不变plt.legend()\n\n\n\n\n\n\n\n\nplt.style.use(&#x27;ggplot&#x27;)   # 采用 ggplot 的绘图风格x = np.linspace(0, 2, 100)  plt.plot(x, x, label=&#x27;linear&#x27;)   plt.plot(x, x**2, label=&#x27;quadratic&#x27;)  plt.plot(x, x**3, label=&#x27;cubic&#x27;)plt.xlabel(&#x27;x label&#x27;) plt.ylabel(&#x27;y label&#x27;)plt.title(&quot;Simple Plot&quot;)   plt.legend() \n\n\n\n\n\n\n\nplt.style.use(&#x27;default&#x27;)  # 保留默认风格\n\n4.2  关于配置文件 rcParams\nrcParams 配置文件可用来自定义图形的各种默认属性，称之为 rc 配置或 rc 参数。通过 rc 参数可以修改默认的属性，包括窗体大小、每英寸的点数、线条宽度、颜色、样式、坐标轴、坐标和网络属性、文本、字体等；\n\nrcParams 和 style 配置官方文档：https://matplotlib.org/tutorials/introductory/customizing.html#customizing-with-matplotlibrc-files\n\n如何查看默认的配置文件？\n\n\n\n\n print(matplotlib.rc_params()) print(matplotlib.rcParamsDefault)   # plt.rcParamsDefaultprint(matplotlib.rcParams)    # plt.rcParams\n\n\n修改配置文件\n\n\n\nplt.rcParams[&#x27;lines.linewidth&#x27;] = 2, plt.rcParams[&#x27;lines.color&#x27;] = &#x27;r&#x27;   # 方式1\nplt.rc(&#x27;lines&#x27;, linewidth=4, color=&#x27;g&#x27;)   # 方式2\n\n\n恢复默认参数：plt.rcdefaults()\n\nprint(plt.rcParams)  # 打印默认的配置\n\n_internal.classic_mode: False\nagg.path.chunksize: 0\nanimation.avconv_args: []\nanimation.avconv_path: avconv\nanimation.bitrate: -1\nanimation.codec: h264\nanimation.convert_args: []\nanimation.convert_path: convert\nanimation.embed_limit: 20.0\nanimation.ffmpeg_args: []\nanimation.ffmpeg_path: ffmpeg\nanimation.frame_format: png\nanimation.html: none\nanimation.html_args: []\nanimation.writer: ffmpeg\naxes.autolimit_mode: data\naxes.axisbelow: True\naxes.edgecolor: white\naxes.facecolor: #E5E5E5\naxes.formatter.limits: [-7, 7]\naxes.formatter.min_exponent: 0\naxes.formatter.offset_threshold: 4\naxes.formatter.use_locale: False\naxes.formatter.use_mathtext: False\naxes.formatter.useoffset: True\naxes.grid: True\naxes.grid.axis: both\naxes.grid.which: major\naxes.labelcolor: #555555\naxes.labelpad: 4.0\naxes.labelsize: large\naxes.labelweight: normal\naxes.linewidth: 1.0\naxes.prop_cycle: cycler(&#39;color&#39;, [&#39;#E24A33&#39;, &#39;#348ABD&#39;, &#39;#988ED5&#39;, &#39;#777777&#39;, &#39;#FBC15E&#39;, &#39;#8EBA42&#39;, &#39;#FFB5B8&#39;])\naxes.spines.bottom: True\naxes.spines.left: True\naxes.spines.right: True\naxes.spines.top: True\naxes.titlepad: 6.0\naxes.titlesize: x-large\naxes.titleweight: normal\naxes.unicode_minus: True\naxes.xmargin: 0.05\naxes.ymargin: 0.05\naxes3d.grid: True\nbackend: module://ipykernel.pylab.backend_inline\nbackend_fallback: True\nboxplot.bootstrap: None\nboxplot.boxprops.color: black\nboxplot.boxprops.linestyle: -\nboxplot.boxprops.linewidth: 1.0\nboxplot.capprops.color: black\nboxplot.capprops.linestyle: -\nboxplot.capprops.linewidth: 1.0\nboxplot.flierprops.color: black\nboxplot.flierprops.linestyle: none\nboxplot.flierprops.linewidth: 1.0\nboxplot.flierprops.marker: o\nboxplot.flierprops.markeredgecolor: black\nboxplot.flierprops.markeredgewidth: 1.0\nboxplot.flierprops.markerfacecolor: none\nboxplot.flierprops.markersize: 6.0\nboxplot.meanline: False\nboxplot.meanprops.color: C2\nboxplot.meanprops.linestyle: --\nboxplot.meanprops.linewidth: 1.0\nboxplot.meanprops.marker: ^\nboxplot.meanprops.markeredgecolor: C2\nboxplot.meanprops.markerfacecolor: C2\nboxplot.meanprops.markersize: 6.0\nboxplot.medianprops.color: C1\nboxplot.medianprops.linestyle: -\nboxplot.medianprops.linewidth: 1.0\nboxplot.notch: False\nboxplot.patchartist: False\nboxplot.showbox: True\nboxplot.showcaps: True\nboxplot.showfliers: True\nboxplot.showmeans: False\nboxplot.vertical: True\nboxplot.whiskerprops.color: black\nboxplot.whiskerprops.linestyle: -\nboxplot.whiskerprops.linewidth: 1.0\nboxplot.whiskers: 1.5\ncontour.corner_mask: True\ncontour.negative_linestyle: dashed\ndatapath: /opt/conda/lib/python3.6/site-packages/matplotlib/mpl-data\ndate.autoformatter.day: %Y-%m-%d\ndate.autoformatter.hour: %m-%d %H\ndate.autoformatter.microsecond: %M:%S.%f\ndate.autoformatter.minute: %d %H:%M\ndate.autoformatter.month: %Y-%m\ndate.autoformatter.second: %H:%M:%S\ndate.autoformatter.year: %Y\ndocstring.hardcopy: False\nerrorbar.capsize: 0.0\nexamples.directory: \nfigure.autolayout: False\nfigure.constrained_layout.h_pad: 0.04167\nfigure.constrained_layout.hspace: 0.02\nfigure.constrained_layout.use: False\nfigure.constrained_layout.w_pad: 0.04167\nfigure.constrained_layout.wspace: 0.02\nfigure.dpi: 72.0\nfigure.edgecolor: 0.50\nfigure.facecolor: white\nfigure.figsize: [6.0, 4.0]\nfigure.frameon: True\nfigure.max_open_warning: 20\nfigure.subplot.bottom: 0.125\nfigure.subplot.hspace: 0.2\nfigure.subplot.left: 0.125\nfigure.subplot.right: 0.9\nfigure.subplot.top: 0.88\nfigure.subplot.wspace: 0.2\nfigure.titlesize: large\nfigure.titleweight: normal\nfont.cursive: [&#39;Apple Chancery&#39;, &#39;Textile&#39;, &#39;Zapf Chancery&#39;, &#39;Sand&#39;, &#39;Script MT&#39;, &#39;Felipa&#39;, &#39;cursive&#39;]\nfont.family: [&#39;sans-serif&#39;]\nfont.fantasy: [&#39;Comic Sans MS&#39;, &#39;Chicago&#39;, &#39;Charcoal&#39;, &#39;Impact&#39;, &#39;Western&#39;, &#39;Humor Sans&#39;, &#39;xkcd&#39;, &#39;fantasy&#39;]\nfont.monospace: [&#39;DejaVu Sans Mono&#39;, &#39;Bitstream Vera Sans Mono&#39;, &#39;Computer Modern Typewriter&#39;, &#39;Andale Mono&#39;, &#39;Nimbus Mono L&#39;, &#39;Courier New&#39;, &#39;Courier&#39;, &#39;Fixed&#39;, &#39;Terminal&#39;, &#39;monospace&#39;]\nfont.sans-serif: [&#39;SimHei&#39;, &#39;DejaVu Sans&#39;, &#39;Bitstream Vera Sans&#39;, &#39;Computer Modern Sans Serif&#39;, &#39;Lucida Grande&#39;, &#39;Verdana&#39;, &#39;Geneva&#39;, &#39;Lucid&#39;, &#39;Arial&#39;, &#39;Helvetica&#39;, &#39;Avant Garde&#39;, &#39;sans-serif&#39;]\nfont.serif: [&#39;DejaVu Serif&#39;, &#39;Bitstream Vera Serif&#39;, &#39;Computer Modern Roman&#39;, &#39;New Century Schoolbook&#39;, &#39;Century Schoolbook L&#39;, &#39;Utopia&#39;, &#39;ITC Bookman&#39;, &#39;Bookman&#39;, &#39;Nimbus Roman No9 L&#39;, &#39;Times New Roman&#39;, &#39;Times&#39;, &#39;Palatino&#39;, &#39;Charter&#39;, &#39;serif&#39;]\nfont.size: 10.0\nfont.stretch: normal\nfont.style: normal\nfont.variant: normal\nfont.weight: normal\ngrid.alpha: 1.0\ngrid.color: white\ngrid.linestyle: -\ngrid.linewidth: 0.8\nhatch.color: black\nhatch.linewidth: 1.0\nhist.bins: 10\nimage.aspect: equal\nimage.cmap: viridis\nimage.composite_image: True\nimage.interpolation: nearest\nimage.lut: 256\nimage.origin: upper\nimage.resample: True\ninteractive: True\nkeymap.all_axes: [&#39;a&#39;]\nkeymap.back: [&#39;left&#39;, &#39;c&#39;, &#39;backspace&#39;, &#39;MouseButton.BACK&#39;]\nkeymap.copy: [&#39;ctrl+c&#39;, &#39;cmd+c&#39;]\nkeymap.forward: [&#39;right&#39;, &#39;v&#39;, &#39;MouseButton.FORWARD&#39;]\nkeymap.fullscreen: [&#39;f&#39;, &#39;ctrl+f&#39;]\nkeymap.grid: [&#39;g&#39;]\nkeymap.grid_minor: [&#39;G&#39;]\nkeymap.help: [&#39;f1&#39;]\nkeymap.home: [&#39;h&#39;, &#39;r&#39;, &#39;home&#39;]\nkeymap.pan: [&#39;p&#39;]\nkeymap.quit: [&#39;ctrl+w&#39;, &#39;cmd+w&#39;, &#39;q&#39;]\nkeymap.quit_all: [&#39;W&#39;, &#39;cmd+W&#39;, &#39;Q&#39;]\nkeymap.save: [&#39;s&#39;, &#39;ctrl+s&#39;]\nkeymap.xscale: [&#39;k&#39;, &#39;L&#39;]\nkeymap.yscale: [&#39;l&#39;]\nkeymap.zoom: [&#39;o&#39;]\nlegend.borderaxespad: 0.5\nlegend.borderpad: 0.4\nlegend.columnspacing: 2.0\nlegend.edgecolor: 0.8\nlegend.facecolor: inherit\nlegend.fancybox: True\nlegend.fontsize: medium\nlegend.framealpha: 0.8\nlegend.frameon: True\nlegend.handleheight: 0.7\nlegend.handlelength: 2.0\nlegend.handletextpad: 0.8\nlegend.labelspacing: 0.5\nlegend.loc: best\nlegend.markerscale: 1.0\nlegend.numpoints: 1\nlegend.scatterpoints: 1\nlegend.shadow: False\nlegend.title_fontsize: None\nlines.antialiased: True\nlines.color: C0\nlines.dash_capstyle: butt\nlines.dash_joinstyle: round\nlines.dashdot_pattern: [6.4, 1.6, 1.0, 1.6]\nlines.dashed_pattern: [3.7, 1.6]\nlines.dotted_pattern: [1.0, 1.65]\nlines.linestyle: -\nlines.linewidth: 1.5\nlines.marker: None\nlines.markeredgecolor: auto\nlines.markeredgewidth: 1.0\nlines.markerfacecolor: auto\nlines.markersize: 6.0\nlines.scale_dashes: True\nlines.solid_capstyle: projecting\nlines.solid_joinstyle: round\nmarkers.fillstyle: full\nmathtext.bf: sans:bold\nmathtext.cal: cursive\nmathtext.default: it\nmathtext.fallback_to_cm: True\nmathtext.fontset: dejavusans\nmathtext.it: sans:italic\nmathtext.rm: sans\nmathtext.sf: sans\nmathtext.tt: monospace\npatch.antialiased: True\npatch.edgecolor: #EEEEEE\npatch.facecolor: #348ABD\npatch.force_edgecolor: False\npatch.linewidth: 0.5\npath.effects: []\npath.simplify: True\npath.simplify_threshold: 0.1111111111111111\npath.sketch: None\npath.snap: True\npdf.compression: 6\npdf.fonttype: 3\npdf.inheritcolor: False\npdf.use14corefonts: False\npgf.preamble: \npgf.rcfonts: True\npgf.texsystem: xelatex\npolaraxes.grid: True\nps.distiller.res: 6000\nps.fonttype: 3\nps.papersize: letter\nps.useafm: False\nps.usedistiller: False\nsavefig.bbox: None\nsavefig.directory: ~\nsavefig.dpi: figure\nsavefig.edgecolor: white\nsavefig.facecolor: white\nsavefig.format: png\nsavefig.frameon: True\nsavefig.jpeg_quality: 95\nsavefig.orientation: portrait\nsavefig.pad_inches: 0.1\nsavefig.transparent: False\nscatter.edgecolors: face\nscatter.marker: o\nsvg.fonttype: path\nsvg.hashsalt: None\nsvg.image_inline: True\ntext.antialiased: True\ntext.color: black\ntext.hinting: auto\ntext.hinting_factor: 8\ntext.latex.preamble: \ntext.latex.preview: False\ntext.latex.unicode: True\ntext.usetex: False\ntimezone: UTC\ntk.window_focus: False\ntoolbar: toolbar2\nverbose.fileo: sys.stdout\nverbose.level: silent\nwebagg.address: 127.0.0.1\nwebagg.open_in_browser: True\nwebagg.port: 8988\nwebagg.port_retries: 50\nxtick.alignment: center\nxtick.bottom: True\nxtick.color: #555555\nxtick.direction: out\nxtick.labelbottom: True\nxtick.labelsize: medium\nxtick.labeltop: False\nxtick.major.bottom: True\nxtick.major.pad: 3.5\nxtick.major.size: 3.5\nxtick.major.top: True\nxtick.major.width: 0.8\nxtick.minor.bottom: True\nxtick.minor.pad: 3.4\nxtick.minor.size: 2.0\nxtick.minor.top: True\nxtick.minor.visible: False\nxtick.minor.width: 0.6\nxtick.top: False\nytick.alignment: center_baseline\nytick.color: #555555\nytick.direction: out\nytick.labelleft: True\nytick.labelright: False\nytick.labelsize: medium\nytick.left: True\nytick.major.left: True\nytick.major.pad: 3.5\nytick.major.right: True\nytick.major.size: 3.5\nytick.major.width: 0.8\nytick.minor.left: True\nytick.minor.pad: 3.4\nytick.minor.right: True\nytick.minor.size: 2.0\nytick.minor.visible: False\nytick.minor.width: 0.6\nytick.right: False\n\nplt.rcParams[&#x27;font.sans-serif&#x27;] = [&#x27;SimHei&#x27;]  # 设置字体plt.rcParams[&#x27;axes.unicode_minus&#x27;]=False    # 显示中文和特殊符号\n\n4.4 绘图宝典\ngithub 上有用的绘图工具：https://github.com/rougier/matplotlib-cheatsheet\n\n\nimport numpy as npimport matplotlib.pyplot as pltfrom matplotlib.ticker import AutoMinorLocator, MultipleLocator, FuncFormatternp.random.seed(19680801)X = np.linspace(0.5, 3.5, 100)Y1 = 3+np.cos(X)Y2 = 1+np.cos(1+X/0.75)/2Y3 = np.random.uniform(Y1, Y2, len(X))fig = plt.figure(figsize=(8, 8), dpi=120,facecolor=&#x27;#f7f7f7&#x27;)ax = fig.add_subplot(1, 1, 1, aspect=1, facecolor=&#x27;#e7dff3&#x27;)def minor_tick(x, pos):    if not x % 1.0:        return &quot;&quot;    return &quot;%.2f&quot; % xax.xaxis.set_major_locator(MultipleLocator(1.000))ax.xaxis.set_minor_locator(AutoMinorLocator(4))ax.yaxis.set_major_locator(MultipleLocator(1.000))ax.yaxis.set_minor_locator(AutoMinorLocator(4))ax.xaxis.set_minor_formatter(FuncFormatter(minor_tick))ax.set_xlim(0, 4)ax.set_ylim(0, 4)ax.tick_params(which=&#x27;major&#x27;, width=1.0)ax.tick_params(which=&#x27;major&#x27;, length=10)ax.tick_params(which=&#x27;minor&#x27;, width=1.0, labelsize=10)ax.tick_params(which=&#x27;minor&#x27;, length=5, labelsize=10, labelcolor=&#x27;0.25&#x27;)ax.grid(linestyle=&quot;--&quot;, linewidth=0.5, color=&#x27;.25&#x27;, zorder=-10)ax.plot(X, Y1, c=(0.25, 0.25, 1.00), lw=2, label=&quot;Blue signal&quot;, zorder=10)ax.plot(X, Y2, c=(1.00, 0.25, 0.25), lw=2, label=&quot;Red signal&quot;)ax.plot(X, Y3, linewidth=0,        marker=&#x27;o&#x27;, markerfacecolor=&#x27;w&#x27;, markeredgecolor=&#x27;k&#x27;)ax.set_title(&quot;Anatomy of a figure&quot;, fontsize=20, verticalalignment=&#x27;bottom&#x27;)ax.set_xlabel(&quot;X axis label&quot;)ax.set_ylabel(&quot;Y axis label&quot;)ax.legend()def circle(x, y, radius=0.15):    from matplotlib.patches import Circle    from matplotlib.patheffects import withStroke    circle = Circle((x, y), radius, clip_on=False, zorder=10, linewidth=1,                    edgecolor=&#x27;black&#x27;, facecolor=(0, 0, 0, .0125),                    path_effects=[withStroke(linewidth=5, foreground=&#x27;w&#x27;)])    ax.add_artist(circle)def text(x, y, text):    ax.text(x, y, text, backgroundcolor=&quot;white&quot;,            ha=&#x27;center&#x27;, va=&#x27;top&#x27;, weight=&#x27;bold&#x27;, color=&#x27;blue&#x27;)# Minor tickcircle(0.50, -0.10)text(0.50, -0.32, &quot;Minor tick label 次刻度标签&quot;)# Major tickcircle(-0.03, 4.00)text(0.03, 3.80, &quot;Major tick 主刻度&quot;)# Minor tickcircle(0.00, 3.50)text(0.00, 3.30, &quot;Minor tick 次刻度&quot;)# Major tick labelcircle(-0.15, 3.00)text(-0.15, 2.80, &quot;Major tick label 主刻度标签&quot;)# X Labelcircle(1.80, -0.27)text(1.80, -0.45, &quot;X axis label X轴标签&quot;)# Y Labelcircle(-0.27, 1.80)text(-0.27, 1.6, &quot;Y axis label Y轴标签&quot;)# Titlecircle(1.60, 4.13)text(1.60, 3.93, &quot;Title 图标题&quot;)# Blue plotcircle(1.75, 2.80)text(1.75, 2.60, &quot;Line\\n(line plot) 线图&quot;)# Red plotcircle(1.20, 0.60)text(1.20, 0.40, &quot;Line\\n(line plot) 线图&quot;)# Scatter plotcircle(3.20, 1.75)text(3.20, 1.55, &quot;Markers\\n(scatter plot) 散点图&quot;)# Gridcircle(3.00, 3.00)text(3.00, 2.80, &quot;Grid 网格&quot;)# Legendcircle(3.70, 3.80)text(3.70, 3.60, &quot;Legend 图例&quot;)# Axescircle(0.5, 0.5)text(0.5, 0.3, &quot;Axes 坐标系&quot;)# Figurecircle(-0.3, 0.65)text(-0.3, 0.45, &quot;Figure 图&quot;)color = &#x27;blue&#x27;ax.annotate(&#x27;Spines 轴脊线&#x27;, xy=(4.0, 0.35), xytext=(3.3, 0.5),            weight=&#x27;bold&#x27;, color=color,            arrowprops=dict(arrowstyle=&#x27;-&gt;&#x27;,                            connectionstyle=&quot;arc3&quot;,                            color=color))ax.annotate(&#x27;&#x27;, xy=(3.15, 0.0), xytext=(3.45, 0.45),            weight=&#x27;bold&#x27;, color=color,            arrowprops=dict(arrowstyle=&#x27;-&gt;&#x27;,                            connectionstyle=&quot;arc3&quot;,                            color=color))ax.text(4.0, -0.4, &quot;Made with http://matplotlib.org&quot;,        fontsize=10, ha=&quot;right&quot;, color=&#x27;.5&#x27;)plt.show()\n\n\n\n\n","categories":["Python"],"tags":["python","matplotlib"]},{"title":"线性代数的本质Essence of Linear Algebra","url":"/2020/04/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0%E7%9A%84%E6%9C%AC%E8%B4%A8Essence-of-Linear-Algebra/","content":"\n","categories":["Maths"],"tags":["线性代数"]},{"title":"解决RabbitMQ SimpleMessageConverter only supports String, byte[] and Serializable payloads","url":"/2020/04/%E8%A7%A3%E5%86%B3RabbitMQ-SimpleMessageConverter-only-supports-String-byte-and-Serializable-payload/","content":"RabbitMQ 和 Spring Boot 整合，使用 rabbitTemplate.convertAndSend 方法发送消息到队列时，报如下错误：\njava.lang.IllegalArgumentException: SimpleMessageConverter only supports String, byte[] and Serializable payloads, received: gy.finolo.springbootmybatisplus.entity.Order\tat org.springframework.amqp.support.converter.SimpleMessageConverter.createMessage(SimpleMessageConverter.java:161)\tat org.springframework.amqp.support.converter.AbstractMessageConverter.createMessage(AbstractMessageConverter.java:88)\tat org.springframework.amqp.support.converter.AbstractMessageConverter.toMessage(AbstractMessageConverter.java:70)\tat org.springframework.amqp.support.converter.AbstractMessageConverter.toMessage(AbstractMessageConverter.java:58)\tat org.springframework.amqp.rabbit.core.RabbitTemplate.convertMessageIfNecessary(RabbitTemplate.java:1758)\tat org.springframework.amqp.rabbit.core.RabbitTemplate.convertAndSend(RabbitTemplate.java:1075)\tat gy.finolo.springbootmybatisplus.producer.OrderSender.send(OrderSender.java:19)\tat gy.finolo.springbootmybatisplus.rabbitmq.RabbitMQTest.testSend(RabbitMQTest.java:27)\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\tat java.lang.reflect.Method.invoke(Method.java:498)\tat org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:675)\tat org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60)...\n\n是因为 Order 这个类没有实现 implements Serializable 接口。\n","categories":["Middleware","RabbitMQ"],"tags":["rabbitmq"]},{"title":"解决mac matplotlib中文乱码问题","url":"/2020/04/%E8%A7%A3%E5%86%B3mac-matplotlib%E4%B8%AD%E6%96%87%E4%B9%B1%E7%A0%81%E9%97%AE%E9%A2%98/","content":"我们通过 matplotlib 画图时，比如：\nimport numpy as npimport matplotlib.pyplot as pltx = np.linspace(0, 2*np.pi, 400)y = np.sin(x**2)fig, ax = plt.subplots()ax.plot(x, y)ax.set_title(&#x27;中文标题&#x27;)\n\n中文会出现乱码。如图：\n\n\n同时，伴随着如下错误信息：\n/opt/anaconda3/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:211: RuntimeWarning: Glyph 39064 missing from current font.  font.set_text(s, 0.0, flags=flags)/opt/anaconda3/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:180: RuntimeWarning: Glyph 20013 missing from current font.  font.set_text(s, 0, flags=flags)\n\n临时解决办法，可以执行如下两条语句：\n第一句是设置字体，设置了字体后，负号会变成乱码。第二句是让负号的乱码正常显示。\nplt.rcParams[&#x27;font.sans-serif&#x27;] = [&#x27;SimHei&#x27;]plt.rcParams[&#x27;axes.unicode_minus&#x27;] = False\n\n查看配置文件位置 \nimport matplotlibmatplotlib.matplotlib_fname()&#x27;/opt/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/matplotlibrc&#x27;\n\n编辑修改文件 /opt/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/matplotlibrc 内容。\n删除注释 #font.family         : sans-serif\n删除注释 #，并加入 SimHei\nfont.sans-serif     : SimHei, DejaVu Sans, Bitstream Vera Sans, Computer Modern Sans Serif, Lucida Grande, Verdana,  Geneva, Lucid, Arial, Helvetica, Avant Garde, sans-serif\n删除注释 #，True 改为 False\naxes.unicode_minus  : False\n清除缓存\n$ rm -rf ~/.matplotlib\n\n刷新 Jupyter Notebook 的页面，发现设置已经生效了。\n","categories":["Python"],"tags":["python","matplotlib"]},{"title":"解决IntelliJ IDEA Could not autowire. No beans of UserMapper type found","url":"/2020/04/%E8%A7%A3%E5%86%B3IntelliJ-IDEA-Could-not-autowire-No-beans-of-UserMapper-type-found/","content":"使用 IntelliJ IDEA 开发 Spring Boot 项目。在自动注入时，会出现红色波浪线的错误提示。如图：\n\n\n如果代码正确，能够正常运行。那我们可以通过如下两种方法解决。\n添加@Repository注解查看一下是不是 Mapper 类忘了写 @Repository 注解。\n设置IDEA进入 Editor -&gt; Inspections -&gt; Spring -&gt; Spring Core -&gt; Autowiring for Bean Class\n把选中的勾去掉就可以了。\n\n","categories":["Java","Spring Boot"],"tags":["intellij idea","autowire"]},{"title":"解决java.sql.SQLException: The server time zone value is unrecognized or represents more than one time zone","url":"/2020/04/%E8%A7%A3%E5%86%B3java-sql-SQLException-The-server-time-zone-value-is-unrecognized-or-represents-more-than-one-time-zone/","content":"我使用了 mysql-connector-java-8.0.19，运行程序时报错：\njava.sql.SQLException: The server time zone value &#x27;�й���׼ʱ��&#x27; is unrecognized or represents more than one time zone.You must configure either the server or JDBC driver (via the &#x27;serverTimezone&#x27; configuration property) to use a more specifc time zone value if you want to utilize time zone support.\n\n按错误提示，需要在 spring.datasource.url 添加 serverTimezone=Asia/Shanghai 参数或者 GMT%2b8 表示 GMT+8 ，有些网友说写 CTT，我不知道写台湾的时区有啥意义。\nspring:# jdbc配置  datasource:    url: jdbc:mysql://localhost:3306/db?serverTimezone=Asia/Shanghai&amp;useUnicode=true&amp;characterEncoding=utf8&amp;useSSL=false\n\n如果mysql-connector-java用的6.0以上的，需要把com.mysql.jdbc.Driver 改为com.mysql.cj.jdbc.Driver，不然每次启动项目，都会有红色提示。\n","categories":["Java"],"tags":["jdbc","timezone"]},{"title":"解决调用Spring注解式事务方法不生效的问题","url":"/2020/04/%E8%A7%A3%E5%86%B3%E8%B0%83%E7%94%A8Spring%E6%B3%A8%E8%A7%A3%E5%BC%8F%E4%BA%8B%E5%8A%A1%E6%96%B9%E6%B3%95%E4%B8%8D%E7%94%9F%E6%95%88%E7%9A%84%E9%97%AE%E9%A2%98/","content":"看如下代码\n@Componentpublic class FooServiceImpl implements FooService &#123;    @Autowired    private JdbcTemplate jdbcTemplate;    @Override    @Transactional(rollbackFor = RollbackException.class)    public void insertThenRollback() throws RollbackException &#123;        jdbcTemplate.execute(&quot;INSERT INTO FOO (BAR) VALUES (&#x27;BBB&#x27;)&quot;);        throw new RollbackException();    &#125;    @Override    public void invokeInsertThenRollback() throws RollbackException &#123;        insertThenRollback();    &#125;&#125;\n\n方法 invokeInsertThenRollback() 调用一个有 @Transactional 注释的方法，这时，事务未生效。\n有两种方法可以解决。\n方法一，把自身 FooService 注入进来。\n@Componentpublic class FooServiceImpl implements FooService &#123;    @Autowired    private JdbcTemplate jdbcTemplate;    @Autowired    private FooService fooService;    @Override    @Transactional(rollbackFor = RollbackException.class)    public void insertThenRollback() throws RollbackException &#123;        jdbcTemplate.execute(&quot;INSERT INTO FOO (BAR) VALUES (&#x27;BBB&#x27;)&quot;);        throw new RollbackException();    &#125;    @Override    public void invokeInsertThenRollback() throws RollbackException &#123;//        insertThenRollback();        fooService.insertThenRollback();           &#125;&#125;\n\n方法二，调用 AopContext.currentProxy() 获取代理类\n@Componentpublic class FooServiceImpl implements FooService &#123;    @Autowired    private JdbcTemplate jdbcTemplate;    @Override    @Transactional(rollbackFor = RollbackException.class)    public void insertThenRollback() throws RollbackException &#123;        jdbcTemplate.execute(&quot;INSERT INTO FOO (BAR) VALUES (&#x27;BBB&#x27;)&quot;);        throw new RollbackException();    &#125;    @Override    public void invokeInsertThenRollback() throws RollbackException &#123;//        insertThenRollback();        ((FooService) (AopContext.currentProxy())).invokeInsertThenRollback();    &#125;&#125;\n","categories":["Java","Spring"],"tags":["spring","transaction"]},{"title":"Spring Boot JUnit测试","url":"/2020/07/Spring-Boot-JUnit%E6%B5%8B%E8%AF%95/","content":"Springboot 2.1在测试类上面写的注解如下：\n@RunWith(SpringRunner.class)@SpringBootTest(classes = &#123;Application.class&#125;)public class AppTests &#123;        @Test    public void test() &#123;&#125;&#125;\n\n需要注意的是，Application 是 Spring boot 的启动类，test() 方法必须是 public 类型。\nSpringboot 2.2注解如下：\n@SpringBootTestpublic class AppTests &#123;        @Test    void test() &#123;&#125;&#125;\n\n不需要写 @RunWith 注解和指定 SpringBootTest 的 classes 属性。test() 方法不用写 public 类型。\n","categories":["Java","Spring Boot"],"tags":["spring boot","junit"]},{"title":"java LocalDateTime转Unix timestamp","url":"/2020/07/java-LocalDateTime%E8%BD%ACUnix-timestamp/","content":"现在尽量使用新的API，把Date换成LocalDateTime。\n使用LocalDateTime时，我们可以通过如下方法把LocalDateTime转换为Unix Timestamp。\npublic static void main(String[] args) &#123;    LocalDateTime now = LocalDateTime.now();    System.out.println(now);    System.out.println(System.currentTimeMillis());    System.out.println(now.toEpochSecond(ZoneOffset.of(&quot;+8&quot;)));    System.out.println(now.toInstant(ZoneOffset.of(&quot;+8&quot;)).toEpochMilli());    System.out.println(now.toInstant(ZoneOffset.UTC).toEpochMilli());    System.out.println(now.plusDays(1).toInstant(ZoneOffset.of(&quot;+8&quot;)).toEpochMilli());&#125;2021-03-10T13:45:39.41116153551394221615355139161535513941116153839394111615441539411\n\nLocalDateTime格式化为指定格式：\nDateTimeFormatter formatter = DateTimeFormatter.ofPattern(&quot;YYYY-MM-dd hh:mm:ss&quot;);String timeStr = LocalDateTime.now().atZone(ZoneOffset.of(&quot;+8&quot;)).format(formatter);\n\n举个例子，假如我要显示当前时间往后推4小时后，UTC的时间表示。\nLocalDateTime now = LocalDateTime.now();ZonedDateTime zonedDateTime = now.plusHours(4).atZone(ZoneOffset.ofHours(8));String future = zonedDateTime.withZoneSameInstant(ZoneOffset.UTC).format(DateTimeFormatter.ofPattern(&quot;YYYY-MM-dd hh:mm:ss&quot;));","categories":["Java"],"tags":["timestamp","localdatetime"]},{"title":"IntelliJ IDEA插件市场Marketplace打不开的解决方法","url":"/2020/07/IntelliJ-IDEA%E6%8F%92%E4%BB%B6%E5%B8%82%E5%9C%BAMarketplace%E6%89%93%E4%B8%8D%E5%BC%80%E7%9A%84%E8%A7%A3%E5%86%B3%E6%96%B9%E6%B3%95/","content":"在某些网络下，IDEA 的插件市场打不开，就安装不了所需要的插件。造成不能访问的原因可能 jetbrains 相关域名解析的 ip 无法访问。\n我们可以通过站长工具，查询一下 jetbrains.com 的 ip 地址，再看能否 ping 通这个地址。\n$ ping 99.84.238.36PING 99.84.238.36 (99.84.238.36): 56 data bytes64 bytes from 99.84.238.36: icmp_seq=0 ttl=238 time=283.909 ms64 bytes from 99.84.238.36: icmp_seq=1 ttl=238 time=277.733 ms64 bytes from 99.84.238.36: icmp_seq=2 ttl=238 time=287.680 ms64 bytes from 99.84.238.36: icmp_seq=3 ttl=238 time=282.584 ms\n\n然后修改电脑的 hosts 文件，加入如下两行：\n99.84.238.36    plugins.jetbrains.com99.84.238.36    jetbrains.com\n\n这时，重启 IDEA，再次进入插件市场，就可以打开了。\n","categories":["Tools","IntelliJ IDEA"],"tags":["intellij idea"]},{"title":"java中Map转List并排序","url":"/2020/07/java%E4%B8%ADMap%E8%BD%ACList%E5%B9%B6%E6%8E%92%E5%BA%8F/","content":"把 Map 中的元素按 key 进行排序，然后把 value 取出来组成一个 List。可以按如下方法操作。\npublic class MapSort2ListDemo &#123;    public static void main(String[] args) &#123;        Map&lt;String, String&gt; map = new HashMap&lt;&gt;();        map.put(&quot;1&quot;, &quot;A&quot;);        map.put(&quot;3&quot;, &quot;C&quot;);        map.put(&quot;2&quot;, &quot;B&quot;);        map.put(&quot;10&quot;, &quot;G&quot;);        map.put(&quot;11&quot;, &quot;H&quot;);        List&lt;String&gt; list = map.entrySet().stream()                .sorted(Comparator.comparing((Map.Entry&lt;String, String&gt; entry) -&gt; Integer.parseInt(entry.getKey())).reversed())                .map(x -&gt; x.getValue())                .collect(Collectors.toList());        System.out.println(list);    &#125;&#125;\n\n结果：[H, G, C, B, A]\n","categories":["Java"],"tags":["hashMap"]},{"title":"Spring中处理集合和字符串","url":"/2020/07/Spring%E4%B8%AD%E5%A4%84%E7%90%86%E9%9B%86%E5%90%88%E5%92%8C%E5%AD%97%E7%AC%A6%E4%B8%B2/","content":"大多数 java 项目都会使用 Spring 框架，所以推荐在处理判断集合和字符串时使用 Spring 自带的工具类。\norg.springframework.util.StringUtils\nhasText() 方法可以判断是否有非空白字符的字符串\norg.springframework.util.CollectionUtils\n","categories":["Java","Spring"],"tags":["spring"]},{"title":"java中如何对Map中的元素进行排序","url":"/2020/07/java%E4%B8%AD%E5%A6%82%E4%BD%95%E5%AF%B9Map%E4%B8%AD%E7%9A%84%E5%85%83%E7%B4%A0%E8%BF%9B%E8%A1%8C%E6%8E%92%E5%BA%8F/","content":"我们经常使用 HashMap, 但有时候需要根据 key 进行排序。\n通过 Collections.sort()Map 的 key 是字符串类型的，需要先把字符串转成数字型。使用 reversed() 方法可以让序列倒序。\npublic class MapSortDemo &#123;    public static void main(String[] args) &#123;        Map&lt;String, String&gt; map = new HashMap&lt;&gt;();        map.put(&quot;1&quot;, &quot;A&quot;);        map.put(&quot;3&quot;, &quot;C&quot;);        map.put(&quot;2&quot;, &quot;B&quot;);        map.put(&quot;10&quot;, &quot;G&quot;);        map.put(&quot;11&quot;, &quot;H&quot;);        List&lt;Map.Entry&lt;String, String&gt;&gt; entries = new ArrayList&lt;&gt;(map.entrySet());        System.out.println(entries);        Collections.sort(entries, Comparator.comparing((Map.Entry&lt;String, String&gt; entry) -&gt; Integer.parseInt(entry.getKey())));        System.out.println(entries);        Collections.sort(entries, Comparator.comparing((Map.Entry&lt;String, String&gt; entry) -&gt; Integer.parseInt(entry.getKey())).reversed());        System.out.println(entries);    &#125;&#125;\n\n通过 TreeSet 方式遍历 HashMap 中所有元素，放入 TreeSet 中，就可以排好序了。例子下回再讲。\n","categories":["Java"],"tags":["hashMap"]},{"title":"Java如何在String.format中显示百分号&#37;","url":"/2020/05/Java%E5%A6%82%E4%BD%95%E5%9C%A8String-format%E4%B8%AD%E6%98%BE%E7%A4%BA%E7%99%BE%E5%88%86%E5%8F%B7/","content":"如果要显示 %，我们需要在前面再加一个 %，做为转义用。\nString.format(&quot;%d%%&quot;, 100);\n\n这时才会正确显示 100%。\n","categories":["Java"],"tags":["java"]},{"title":"Hexo博客开启LaTex编写数学公式","url":"/2020/05/Hexo%E5%8D%9A%E5%AE%A2%E5%BC%80%E5%90%AFLaTex%E7%BC%96%E5%86%99%E6%95%B0%E5%AD%A6%E5%85%AC%E5%BC%8F/","content":"在 Hexo 博客中，我们往往会编写一些数学公式，所以不得不启动 LaTex。LaTeX（LATEX，音译“拉泰赫”）是一种基于 ΤΕΧ 的排版系统，可以生成非常复杂的数学公式。\n安装依赖npm uninstall hexo-renderer-marked –savenpm install hexo-renderer-kramed –save\n\n解决语义冲突在博客根目录下，修改 node_modules\\kramed\\lib\\rules\\inline.js。\n第11行和第20行，修改如下内容：\n//  escape: /^\\\\([\\\\`*&#123;&#125;\\[\\]()#$+\\-.!_&gt;])/,  escape: /^\\\\([`*\\[\\]()#$+\\-.!_&gt;])/,  //  em: /^\\b_((?:__|[\\s\\S])+?)_\\b|^\\*((?:\\*\\*|[\\s\\S])+?)\\*(?!\\*)/,  em: /^\\*((?:\\*\\*|[\\s\\S])+?)\\*(?!\\*)/,\n\n更新主题里面的配置mathjax 设置为 true。\nmath:  mathjax:    enable: true\n\n在博客中声明在博客头部，声明如下内容。\n--mathjax: true--\n\n这时就可以插入数学公式了。\n","categories":["Tools","Hexo"],"tags":["hexo"]},{"title":"微信公众号开发","url":"/2020/07/%E5%BE%AE%E4%BF%A1%E5%85%AC%E4%BC%97%E5%8F%B7%E5%BC%80%E5%8F%91/","content":"简单记录一下微信公众号开发流程，很多年前就开发过，然后过了一段时间，又完全忘了。还得一点一点看文档。所以这次先做一个记录。开发的功能就是，当用户发送消息以后，回复一条消息给用户。\n申请微信开发测试号先申请一个测试号，不需要注册什么，有微信，扫码一下就注册了。\nhttps://mp.weixin.qq.com/debug/cgi-bin/sandbox?t=sandbox/login\n进去后，我们可以获取到 appID, appsecret。Token是自己设置的，随便设置一个就行。aesKey在正式生产环境里面才需要。\n内网穿透微信开发后台需要连接到我们开发的电脑，一般情况下这台电脑是没有公网IP的，而且都是在局域网内，所以要把这台机器的端口映射出去。\n在 ngrok.com 去注册一个账号就可以了。\n然后按使用说明运行命令，把token存入本地。然后启动 ngrok，我是把本地 8080 映射要外网。\n$ ./ngrok authtoken [token]$ ./ngrok http 8080ngrok by @inconshreveable                                                                                                                                                (Ctrl+C to quit)                                                                                                                                                                                         Session Status                online                                                                                                                                                     Account                       S (Plan: Free)                                                                                                                                             Version                       2.3.35                                                                                                                                                     Region                        United States (us)                                                                                                                                         Web Interface                 http://127.0.0.1:4040                                                                                                                                      Forwarding                    http://020170f28e33.ngrok.io -&gt; http://localhost:8080                                                                                                      Forwarding                    https://020170f28e33.ngrok.io -&gt; http://localhost:8080                                                                                                                                                                                                                                                                                              Connections                   ttl     opn     rt1     rt5     p50     p90                                                                                                                                              0       0       0.00    0.00    0.00    0.00  \n\n配置微信平台接口信息按 weixin-java-mp-demo-springboot 项目要求，配置微信公众号中的接口地址：http:&#x2F;&#x2F;公网可访问域名&#x2F;wx&#x2F;portal&#x2F;xxxxx （注意，xxxxx为对应公众号的appid值）\n几年前，一般还要点两下才会成功，现在一下就成功了。\n自动回复消息我打算以回复一张图片。首先需要先上传一张图片，并得到其mediaId。\n我们先看看这个项目的开发文档。\nhttps://github.com/Wechat-Group/WxJava/wiki\n找到微信公众号开发文档，永久素材管理页面，https://github.com/Wechat-Group/WxJava/wiki/MP_永久素材管理\n然后写一段上传永久素材图片的代码。\nWxMpMaterial wxMpMaterial = new WxMpMaterial();wxMpMaterial.setFile(new File(&quot;/Users/simon/Desktop/logo.png&quot;));wxMpMaterial.setName(&quot;logo&quot;);WxMpMaterialUploadResult res = wxMpService.getMaterialService().materialFileUpload(&quot;png&quot;, wxMpMaterial);System.out.println(JsonUtils.toJson(res));\n\n这时，我们就可以获得这个图片的 mediaId 了，然后修改 MsgHandler 的逻辑，返回一个 ImageBuilder 就可以了。\nredirect_uri 参数错误调试微信前公众号的时候，往往会遇到这个错误，这时，需要在 OAuth2.0 网页授权页面配置好授权回调页面域名。沙盒号回调地址支持域名和ip，正式公众号回调地址只支持域名。所以这里不能使用localhost，但写成本地的ip地址是可以的。还需要特别注意的是，这里只能写ip和端口号，不能把协议http加入，不然还是会出现这个错误。\n调试平台我们可以通过调试平台来创建菜单，获取token等。获取token时，可能会遇到invalid ip, not in whitelist错误。\n这时，需要把获取access_token的机器的ip添加到白名单中。位置在 基础配置 -&gt; IP白名单。\n","categories":["Tools"],"tags":["公众号"]},{"title":"Java如何获取到本地的ip地址","url":"/2020/05/Java%E5%A6%82%E4%BD%95%E8%8E%B7%E5%8F%96%E5%88%B0%E6%9C%AC%E5%9C%B0%E7%9A%84ip%E5%9C%B0%E5%9D%80/","content":"在 Java 程序中，我们可以通过如下代码获取本地的 ip 地址：\nInetAddress.getLocalHost().getHostAddress()\n\n但由于安装了虚拟机，或者由于本地回环网的问题，获取的 ip 地址可能不是想要的那个。可以采用如下方法获取到所有的 ip 地址列表，再做筛选。\npackage gy.finolo.ipdemo;import java.net.*;import java.util.Enumeration;public class IPDemo &#123;    public static void main(String[] args) throws UnknownHostException, SocketException &#123;        System.out.println(InetAddress.getLocalHost().getHostAddress());        Enumeration&lt;NetworkInterface&gt; networkInterfaces = NetworkInterface.getNetworkInterfaces();        while (networkInterfaces.hasMoreElements()) &#123;            NetworkInterface networkInterface = networkInterfaces.nextElement();            Enumeration&lt;InetAddress&gt; inetAddresses = networkInterface.getInetAddresses();            while (inetAddresses.hasMoreElements()) &#123;                InetAddress inetAddress = inetAddresses.nextElement();                if (inetAddress instanceof Inet4Address) &#123;                    System.out.println(inetAddress.getHostAddress() + &quot; loopback: &quot; + inetAddress.isLoopbackAddress() +                            &quot; linklocal: &quot; + inetAddress.isLinkLocalAddress() +                            &quot; sitelocal: &quot; + inetAddress.isSiteLocalAddress());                &#125;                                if (inetAddress instanceof Inet6Address) &#123;                    System.out.println(((Inet6Address) inetAddress).getScopedInterface());                &#125;            &#125;        &#125;    &#125;&#125;\n\n如果还不能区分，那就可能需要通过网卡名字来过滤了，这个是需要用到 Inet6Address 了。\n","categories":["Java"],"tags":["java"]},{"title":"Java模拟并发场景","url":"/2020/05/Java%E6%A8%A1%E6%8B%9F%E5%B9%B6%E5%8F%91%E5%9C%BA%E6%99%AF/","content":"多线程环境下，我们要如何测试自己写的业务代码是否是线程安全的？\n可以用到 CountDownLatch 这个类，让所有线程都 await, 然后 countDown 以后，所有线程共同执行。\npackage gy.finolo.concurrent;import java.util.concurrent.CountDownLatch;import java.util.concurrent.ExecutorService;import java.util.concurrent.Executors;import java.util.concurrent.TimeUnit;public class ConcurrentDemo &#123;    // 线程数    private static final int WORKER_COUNT = 100;    // 竞争资源    private static int TOTAL = 100;    public static void main(String[] args) &#123;        ExecutorService executorService = Executors.newCachedThreadPool();        CountDownLatch cdl = new CountDownLatch(1);        for (int i = 0; i &lt; WORKER_COUNT; i++) &#123;            Worker worker = new Worker(cdl);            executorService.execute(worker);        &#125;        try &#123;            TimeUnit.SECONDS.sleep(2);        &#125; catch (InterruptedException e) &#123;            e.printStackTrace();        &#125;        cdl.countDown();        try &#123;            TimeUnit.SECONDS.sleep(2);        &#125; catch (InterruptedException e) &#123;            e.printStackTrace();        &#125;        executorService.shutdown();        System.out.println(&quot;TOTAL should be ZERO, but it&#x27;s : &quot; + TOTAL);    &#125;    static class Worker implements Runnable &#123;        private CountDownLatch countDownLatch;        public Worker(CountDownLatch countDownLatch) &#123;            this.countDownLatch = countDownLatch;        &#125;        @Override        public void run() &#123;            try &#123;                countDownLatch.await();            &#125; catch (InterruptedException e) &#123;                e.printStackTrace();            &#125;            this.executeTask();        &#125;        // 需要并发处理的逻辑        private void executeTask() &#123;            int a = TOTAL - 1;            try &#123;                TimeUnit.MILLISECONDS.sleep(1);            &#125; catch (InterruptedException e) &#123;                e.printStackTrace();            &#125;            TOTAL = a;        &#125;    &#125;&#125;","categories":["Java"],"tags":["java"]},{"title":"Mac日语键盘如何输入反斜线&#92;","url":"/2020/05/Mac%E6%97%A5%E8%AF%AD%E9%94%AE%E7%9B%98%E5%A6%82%E4%BD%95%E8%BE%93%E5%85%A5%E5%8F%8D%E6%96%9C%E7%BA%BF/","content":"Mac 日语键盘，输入反斜线 \\，Option + ￥\n"},{"title":"Python itertools模拟排列组合","url":"/2020/05/Python-itertools%E6%A8%A1%E6%8B%9F%E6%8E%92%E5%88%97%E7%BB%84%E5%90%88/","content":"公式回顾高中所学的排列组合，我们用程序来模拟一下。\n如果是从 n 个元素中取出 n 个元素进行排列，那一共有 n! 种排法。\n因为，有 n 个元素，放入 n 个盒子里，第1个盒子有 n 种取法，第2个盒子有 n - 1 种取法，第 n 个盒子有 1 种取法，根据分步算法，n * (n - 1) * (n - 2) * … * 1 &#x3D; n! 。\n那如果是 n 个元素，任取 m 个排列，那同理能得出如下公式。\n$ P_n^m&#x3D;\\frac{n!}{(n-m)!} $\n如果是组合，我们把 m 个元素的阶乘除掉即可。\n$ C_n^m&#x3D;\\frac{n!}{(n-m)!m!} $\n同时，排列组合还有如下两个重要公式：\n$ P_{n+1}^m&#x3D;P_n^m+mP_n^{m-1} $\n$ C_{n+1}^m&#x3D;C_n^m+C_n^{m-1} $\n排列import itertoolslist = range(3)p = itertools.permutations(list, 2)for i in p:    print(i)    (0, 1)(0, 2)(1, 0)(1, 2)(2, 0)(2, 1)    \n\n组合c = itertools.combinations(list, 2)for i in c:    print(i)    (0, 1)(0, 2)(1, 2)\n","categories":["Python"],"tags":["python"]},{"title":"MySQL筛选多列重复的记录","url":"/2020/05/MySQL%E7%AD%9B%E9%80%89%E5%A4%9A%E5%88%97%E9%87%8D%E5%A4%8D%E7%9A%84%E8%AE%B0%E5%BD%95/","content":"单列去重比较简单，如果多列要去重，怎么找出重复的值？\nSELECT     field1, COUNT(field1),    field2, COUNT(field2),    ...FROM    table_nameGROUP BY    field1,     field2,    ...HAVING     (COUNT(field1) &gt; 1) AND     (COUNT(field2) &gt; 1) AND      ...","categories":["Database","MySQL"],"tags":["mysql"]},{"title":"Python matplotlib绘图时x轴标签重叠覆盖","url":"/2020/05/Python-matplotlib%E7%BB%98%E5%9B%BE%E6%97%B6x%E8%BD%B4%E6%A0%87%E7%AD%BE%E9%87%8D%E5%8F%A0%E8%A6%86%E7%9B%96/","content":"当标签文本很长时，或者是某维度的数据列表长度很长时，都会造成标签的重叠覆盖。\n\n\n解决方法有以下几种。我们使用的数据如下：\nimport pandas as pdimport matplotlib.pyplot as pltindustry = [&#x27;交通运输&#x27;, &#x27;休闲服务&#x27;, &#x27;传媒&#x27;, &#x27;公用事业&#x27;, &#x27;农林牧渔&#x27;, &#x27;化工&#x27;,            &#x27;医药生物&#x27;, &#x27;商业贸易&#x27;, &#x27;国防军工&#x27;, &#x27;家用电器&#x27;, &#x27;建筑材料&#x27;, &#x27;建筑装饰&#x27;,            &#x27;房地产&#x27;, &#x27;有色金属&#x27;, &#x27;机械设备&#x27;, &#x27;汽车&#x27;, &#x27;电子&#x27;, &#x27;电气设备&#x27;, &#x27;纺织服装&#x27;,            &#x27;综合&#x27;, &#x27;计算机&#x27;, &#x27;轻工制造&#x27;, &#x27;采掘&#x27;, &#x27;银行&#x27;, &#x27;非银金融&#x27;, &#x27;食品饮料&#x27;]cap = [4572, 253, 84, 119, 23, 352, 1059, 631, 900, 95, 43, 430, 1524, 52, 1254,       2581, 3012, 195, 70, 31, 1338, 61, 111, 4964, 2782, 291]# 生成Seriess = pd.Series(cap, industry)# 排序sort_s = series.sort_values(ascending=False)# 生成x轴 y轴序列x = sort_s.indexy = sort_s.valuesfig, axs = plt.subplots()axs.bar(x, y)\n\n拉长画布查看画布默认大小\nplt.rcParams[&#x27;figure.figsize&#x27;][6.0, 4.0]\n\n我们把画布拉长一倍，设置 figsize 参数。\nfig, axs = plt.subplots(figsize=(12, 4))axs.bar(x, y)\n\n\n\n拉长以后，bar 的宽度也变大了。\n由于 x 轴的数据太多了，还是有覆盖现象。\n调整标签字体字号fig, axs = plt.subplots()# 调整x轴标签的大小axs.tick_params(axis=&#x27;x&#x27;, labelsize=6)axs.bar(x, y)\n\n\n\nx轴和y轴互换fig, axs = plt.subplots()axs.barh(x, y)\n\n\n\n标签旋转目前比较好的解决方案可能是标签旋转，再适当的放大x轴。\nfig, axs = plt.subplots(figsize=(12, 4))# x轴标签旋转axs.tick_params(axis=&#x27;x&#x27;, labelrotation=-60)axs.bar(x, y)\n\n\n","categories":["Python","Matplotlib"],"tags":["python","matplotlib"]},{"title":"Spring Boot Test无法自动注入@Autowired","url":"/2020/05/Spring-Boot-Test%E6%97%A0%E6%B3%95%E8%87%AA%E5%8A%A8%E6%B3%A8%E5%85%A5-Autowired/","content":"Springboot 版本 2.1.5.RELEASE\n引用的 spring-boot-starter-test 版本为：2.1.2.RELEASE\n依赖的 junit 版本为 4.12\n必须要按如下写注解，才可以把类注入成功。\n@RunWith(SpringRunner.class)@SpringBootTest(classes = SpringBootApplication.class)public class ControllerTests &#123;    @Autowired    private UserController userController;    @Test    public void test() &#123;        System.out.println(userController);    &#125;&#125;\n\n\n需要写 @RunWith 注解\n@SpringBootTest 注解需要添加 classes 属性，值为启动类的 class 对象。\ntest() 方法必须为 public\n\n上述几点在新版本的 Springboot 中（比如2.2及以上）可能有变化。\n","categories":["Java","Spring Boot"],"tags":["spring boot"]},{"title":"Spring boot启动时不加载RabbitMQ","url":"/2020/05/Spring-boot%E5%90%AF%E5%8A%A8%E6%97%B6%E4%B8%8D%E5%8A%A0%E8%BD%BDRabbitMQ/","content":"Spring boot 项目中，集成了 RabbitMQ。但有时候，做测试的时候，可能 RabbitMQ 并未启动，这时就会导致项目启动时间很长，还会报如下错误信息：\n020-05-05 18:00:22.267 ERROR 78855 --- [           main] o.s.a.r.l.SimpleMessageListenerContainer : Consumer failed to start in 60000 milliseconds; does the task executor have enough threads to support the container concurrency?2020-05-05 18:00:22.305 ERROR 78855 --- [ntContainer#0-2] o.s.a.r.l.SimpleMessageListenerContainer : Failed to check/redeclare auto-delete queue(s).org.springframework.amqp.AmqpIOException: java.net.SocketTimeoutException: connect timed out\tat org.springframework.amqp.rabbit.support.RabbitExceptionTranslator.convertRabbitAccessException(RabbitExceptionTranslator.java:70) ~[spring-rabbit-2.2.5.RELEASE.jar:2.2.5.RELEASE]\tat org.springframework.amqp.rabbit.connection.AbstractConnectionFactory.createBareConnection(AbstractConnectionFactory.java:510) ~[spring-rabbit-2.2.5.RELEASE.jar:2.2.5.RELEASE]\tat org.springframework.amqp.rabbit.connection.CachingConnectionFactory.createConnection(CachingConnectionFactory.java:751) ~[spring-rabbit-2.2.5.RELEASE.jar:2.2.5.RELEASE]\tat org.springframework.amqp.rabbit.connection.ConnectionFactoryUtils.createConnection(ConnectionFactoryUtils.java:214) ~[spring-rabbit-2.2.5.RELEASE.jar:2.2.5.RELEASE]\tat org.springframework.amqp.rabbit.core.RabbitTemplate.doExecute(RabbitTemplate.java:2095) ~[spring-rabbit-2.2.5.RELEASE.jar:2.2.5.RELEASE]\tat org.springframework.amqp.rabbit.core.RabbitTemplate.execute(RabbitTemplate.java:2068) ~[spring-rabbit-2.2.5.RELEASE.jar:2.2.5.RELEASE]\tat org.springframework.amqp.rabbit.core.RabbitTemplate.execute(RabbitTemplate.java:2048) ~[spring-rabbit-2.2.5.RELEASE.jar:2.2.5.RELEASE]\tat org.springframework.amqp.rabbit.core.RabbitAdmin.getQueueInfo(RabbitAdmin.java:407) ~[spring-rabbit-2.2.5.RELEASE.jar:2.2.5.RELEASE]\tat org.springframework.amqp.rabbit.core.RabbitAdmin.getQueueProperties(RabbitAdmin.java:391) ~[spring-rabbit-2.2.5.RELEASE.jar:2.2.5.RELEASE]\tat org.springframework.amqp.rabbit.listener.AbstractMessageListenerContainer.attemptDeclarations(AbstractMessageListenerContainer.java:1830) ~[spring-rabbit-2.2.5.RELEASE.jar:2.2.5.RELEASE]\tat org.springframework.amqp.rabbit.listener.AbstractMessageListenerContainer.redeclareElementsIfNecessary(AbstractMessageListenerContainer.java:1811) ~[spring-rabbit-2.2.5.RELEASE.jar:2.2.5.RELEASE]\tat org.springframework.amqp.rabbit.listener.SimpleMessageListenerContainer$AsyncMessageProcessingConsumer.initialize(SimpleMessageListenerContainer.java:1342) [spring-rabbit-2.2.5.RELEASE.jar:2.2.5.RELEASE]\tat org.springframework.amqp.rabbit.listener.SimpleMessageListenerContainer$AsyncMessageProcessingConsumer.run(SimpleMessageListenerContainer.java:1188) [spring-rabbit-2.2.5.RELEASE.jar:2.2.5.RELEASE]\tat java.lang.Thread.run(Thread.java:745) [na:1.8.0_92]Caused by: java.net.SocketTimeoutException: connect timed out\tat java.net.PlainSocketImpl.socketConnect(Native Method) ~[na:1.8.0_92]\tat java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[na:1.8.0_92]\tat java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[na:1.8.0_92]\tat java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[na:1.8.0_92]\tat java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[na:1.8.0_92]\tat java.net.Socket.connect(Socket.java:589) ~[na:1.8.0_92]\tat com.rabbitmq.client.impl.SocketFrameHandlerFactory.create(SocketFrameHandlerFactory.java:60) ~[amqp-client-5.7.3.jar:5.7.3]\tat com.rabbitmq.client.ConnectionFactory.newConnection(ConnectionFactory.java:1113) ~[amqp-client-5.7.3.jar:5.7.3]\tat com.rabbitmq.client.ConnectionFactory.newConnection(ConnectionFactory.java:1063) ~[amqp-client-5.7.3.jar:5.7.3]\tat org.springframework.amqp.rabbit.connection.AbstractConnectionFactory.connect(AbstractConnectionFactory.java:526) ~[spring-rabbit-2.2.5.RELEASE.jar:2.2.5.RELEASE]\tat org.springframework.amqp.rabbit.connection.AbstractConnectionFactory.createBareConnection(AbstractConnectionFactory.java:473) ~[spring-rabbit-2.2.5.RELEASE.jar:2.2.5.RELEASE]\t... 12 common frames omitted\n\n如何不启动 RabbitMQ 呢？\n我们只需要在启动类或者是测试启动类上添加如下注解：\n@EnableAutoConfiguration(exclude = &#123;RabbitAutoConfiguration.class&#125;)\n\n同时在注入了 RabbitTemplate 的类上面添加如下注解：\n@Profile(&quot;no_rabbit&quot;)\n\nno_rabbit 任意写，只要不是我们启动类的 Profile 就可以了。\n","categories":["Middleware","RabbitMQ"],"tags":["spring boot","rabbitmq"]},{"title":"udp端口监测","url":"/2020/05/udp%E7%AB%AF%E5%8F%A3%E7%9B%91%E6%B5%8B/","content":"CentOS 环境安装 udp 监测工具 netcat\n$ sudo yum install nc\n\n监听端口\n$ nc -vul localhost 1080Ncat: Version 7.50 ( https://nmap.org/ncat )Ncat: Listening on ::1:1080Ncat: Connection from ::1.1231233\n\n发送数据\n$ nc -vu localhost 1080Ncat: Version 7.50 ( https://nmap.org/ncat )Ncat: Connected to ::1:1080.1231233"},{"title":"依赖注入的原理","url":"/2020/05/%E4%BE%9D%E8%B5%96%E6%B3%A8%E5%85%A5%E7%9A%84%E5%8E%9F%E7%90%86/","content":"我们使用 @Autowired 注解来注入依赖，下面通过代码，来简单演示一下其最基础的注入逻辑。\n这里我将创建一个新的注解 @MyAutowired\nMyAutowired.java\npackage gy.finolo.autowireddemo;import java.lang.annotation.*;@Retention(RetentionPolicy.RUNTIME)@Target(ElementType.FIELD)@Inherited@Documentedpublic @interface MyAutowired &#123;&#125;\n\n一个 Service 实现类，里面没有任何方法，主要用于测试。\nUserService.java\npackage gy.finolo.autowireddemo;public class UserService &#123;&#125;\n\n一个 Controller 类，我们把上面的 Service 类注入此类中。测试 main 方法也在这里面，方便测试。\nUserController.java\npackage gy.finolo.autowireddemo;import java.util.stream.Stream;public class UserController &#123;    @MyAutowired    private UserService userService;    public static void main(String[] args) &#123;        UserController userController = new UserController();        Class&lt;? extends UserController&gt; clazz = userController.getClass();        Stream.of(clazz.getDeclaredFields()).forEach(field -&gt; &#123;            // check the member variable whether annotated with @MyAutowired            MyAutowired annotation = field.getAnnotation(MyAutowired.class);            if (annotation != null) &#123;                field.setAccessible(true);                // get the class type of the field                Class&lt;?&gt; type = field.getType();                try &#123;                    Object o = type.newInstance();                    field.set(userController, o);                &#125; catch (InstantiationException e) &#123;                    e.printStackTrace();                &#125; catch (IllegalAccessException e) &#123;                    e.printStackTrace();                &#125;            &#125;        &#125;);        System.out.println(userController.userService);    &#125;&#125;\n\n运行后，可以看到非 null 的输出，证明已经成功把 UserService 依赖注入。\n","categories":["Java","Spring"],"tags":["java","spring"]},{"title":"千千阙歌吉他谱","url":"/2020/05/%E5%8D%83%E5%8D%83%E9%98%99%E6%AD%8C%E5%90%89%E4%BB%96%E8%B0%B1/","content":"\n\n","categories":["吉他谱"]},{"title":"用Hexo搭建属于自己的免费博客","url":"/2020/05/%E7%94%A8Hexo%E6%90%AD%E5%BB%BA%E5%B1%9E%E4%BA%8E%E8%87%AA%E5%B7%B1%E7%9A%84%E5%85%8D%E8%B4%B9%E5%8D%9A%E5%AE%A2/","content":"之前一直使用 hexo 和 next 的主题，后来做了一些自定义的修改，然后发现 hexo 和 next theme 官方都做了升级，我也跟着升级了。但升级的部分和我自定义修改的 layout 部分就产生冲突了。搞了很久，也没能修复，所以还是重新搭建一套吧，然后把再自定义部分添加上去。\n安装 hexo-cli 客户端根据官方文档，安装最新的 hexo-cli。\n$ npm install hexo-cli -g/usr/local/bin/hexo -&gt; /usr/local/lib/node_modules/hexo-cli/bin/hexo+ hexo-cli@3.1.0added 27 packages from 12 contributors, removed 238 packages and updated 38 packages in 6.648s\n\n搭建博客初始化博客创建一个你将用于放置博客系统的目录，然后在此目录下执行如下命令：\n$ hexo init blogINFO  Cloning hexo-starter https://github.com/hexojs/hexo-starter.gitCloning into &#x27;/Users/simon/Development/workspace/blog&#x27;...remote: Enumerating objects: 30, done.remote: Counting objects: 100% (30/30), done.remote: Compressing objects: 100% (24/24), done.remote: Total 161 (delta 12), reused 12 (delta 4), pack-reused 131Receiving objects: 100% (161/161), 31.79 KiB | 5.00 KiB/s, done.Resolving deltas: 100% (74/74), done.Submodule &#x27;themes/landscape&#x27; (https://github.com/hexojs/hexo-theme-landscape.git) registered for path &#x27;themes/landscape&#x27;Cloning into &#x27;/Users/simon/Development/workspace/blog/themes/landscape&#x27;...remote: Enumerating objects: 4, done.remote: Counting objects: 100% (4/4), done.remote: Compressing objects: 100% (4/4), done.remote: Total 1067 (delta 0), reused 0 (delta 0), pack-reused 1063Receiving objects: 100% (1067/1067), 3.22 MiB | 3.00 KiB/s, done.Resolving deltas: 100% (585/585), done.Submodule path &#x27;themes/landscape&#x27;: checked out &#x27;73a23c51f8487cfcd7c6deec96ccc7543960d350&#x27;INFO  Install dependenciesyarn install v1.21.1info No lockfile found.[1/4] 🔍  Resolving packages...[2/4] 🚚  Fetching packages...[3/4] 🔗  Linking dependencies...[4/4] 🔨  Building fresh packages...success Saved lockfile.warning Your current version of Yarn is out of date. The latest version is &quot;1.22.4&quot;, while you&#x27;re on &quot;1.21.1&quot;.info To upgrade, run the following command:$ curl --compressed -o- -L https://yarnpkg.com/install.sh | bash✨  Done in 5.11s.INFO  Start blogging with Hexo!\n\n提示说 Yarn 版本低了，所以按提示方法更新了 Yarn。\n安装依赖包$ cd blog$ npm install\n\n本地运行博客hexo s\n\n就可以通过 http://localhost:4000/ 本地访问到博客了。\n安装 Next 主题主题各有所爱，我在这里演示安装 Next 主题。\n$ cd blog$ git clone https://github.com/theme-next/hexo-theme-next themes/next\n","categories":["Tools","Hexo"],"tags":["hexo"]},{"title":"可微的充分必要条件","url":"/2020/05/%E5%8F%AF%E5%BE%AE%E7%9A%84%E5%85%85%E5%88%86%E5%BF%85%E8%A6%81%E6%9D%A1%E4%BB%B6/","content":"对于这个充分和必要条件，现在梳理一下。\n如果 A &#x3D;&gt; B 且 A &lt;&#x2F;&#x3D; B，即如果 A 能推出 B，且 B 不能推出 A，则 A 是 B 的充分（不必要）条件。\n如果 A &#x3D;&#x2F;&gt; B 且 A &lt;&#x3D; B，即如果 A 不能推出 B，且 B 能推出 A，则 A 是 B 的必要（不充分）条件。\n可微条件必要条件即可微，能推出什么。\n若函数在某点可微分，则函数在该点必连续；\n若二元函数在某点可微分，则该函数在该点对x和y的偏导数必存在。\n充分条件即什么能推出可微。\n若函数对x和y的偏导数在这点的某一邻域内都存在，且均在这点连续，则该函数在这点可微。\n","categories":["Maths"],"tags":["maths"]},{"title":"微信小程序移动设备分辨率与rpx","url":"/2020/05/%E5%BE%AE%E4%BF%A1%E5%B0%8F%E7%A8%8B%E5%BA%8F%E7%A7%BB%E5%8A%A8%E8%AE%BE%E5%A4%87%E5%88%86%E8%BE%A8%E7%8E%87%E4%B8%8Erpx/","content":"开发小程序时，需要对不同设备做分辨率的自适应。\n设计稿以 iphone 6 的物理像素 750 * 1334 来设计。\n使用 rpx 为单位，小程序会在不同分辨率的设备里面进行换算。\niphone 6 下面，1px &#x3D; 1rpx &#x3D; 0.5pt\npt 是逻辑像素单位，跟设备的物理长宽（3.5英寸）有关系。\n","categories":["小程序"],"tags":["小程序"]},{"title":"搜狗输入法小米版删除工具栏上的广告按钮","url":"/2020/05/%E6%90%9C%E7%8B%97%E8%BE%93%E5%85%A5%E6%B3%95%E5%B0%8F%E7%B1%B3%E7%89%88%E5%88%A0%E9%99%A4%E5%B7%A5%E5%85%B7%E6%A0%8F%E4%B8%8A%E7%9A%84%E5%B9%BF%E5%91%8A%E6%8C%89%E9%92%AE/","content":"使用安卓搜狗输入法小米版时，经常会误点到如图所示的广告。\n\n\n一般会启动小米的一个应用。关掉应用再回到原来页面，要耗费好几秒钟时间，非常恶心。\n我原以为通过下图所示方法，修改工具栏选项就可以解决。\n\n\n其实是不行的，这个方法只能修改这个广告左边的那些按钮。\n\n\n要去掉这个广告按钮，得按如下方法操作：\n设置 -&gt; 更多设置 -&gt; 语言和输入法 -&gt; 搜狗输入法小米版 -&gt; 输入习惯 -&gt; 节日活动提醒 -&gt; 关闭开关，搞定。\n"},{"title":"Linux OpenSUSE安装字体字库","url":"/2020/10/Linux-OpenSUSE%E5%AE%89%E8%A3%85%E5%AD%97%E4%BD%93%E5%AD%97%E5%BA%93/","content":"最近在 Linux OpenSUSE 上运行 pdfbox 工具，将 Pdf 转成图片，会遇到找不到字体的警告提示。\n[15:02:34.732] WARN  org.apache.pdfbox.pdmodel.font.PDTrueTypeFont 220 &lt;init&gt; - Using fallback font &#x27;LiberationSans&#x27; for &#x27;Arial,Bold&#x27;\n\n所以这篇文章讲述一下如何在 OpenSUSE 系统上安装字体字库。\n查看 OpenSUSE 版本号\n# cat /etc/os-release NAME=&quot;openSUSE Leap&quot;VERSION=&quot;42.3&quot;ID=opensuseID_LIKE=&quot;suse&quot;VERSION_ID=&quot;42.3&quot;PRETTY_NAME=&quot;openSUSE Leap 42.3&quot;ANSI_COLOR=&quot;0;32&quot;CPE_NAME=&quot;cpe:/o:opensuse:leap:42.3&quot;BUG_REPORT_URL=&quot;https://bugs.opensuse.org&quot;HOME_URL=&quot;https://www.opensuse.org/&quot;\n\n查看当前系统已经安装的字体# fc-list | less/usr/share/fonts/truetype/DejaVuSans-ExtraLight.ttf: DejaVu Sans,DejaVu Sans Light:style=ExtraLight/usr/share/fonts/truetype/OpenSans-CondLightItalic.ttf: Open Sans,Open Sans Condensed Light:style=Condensed Light Italic,Italic...\n\n获取字体大多数的字体，在 Windows 系统上都可以找到。\n字体目录所在位置：C:\\Windows\\Fonts\n安装字体OpenSUSE 的字库位置在 /usr/share/fonts/truetype，通过刚才命令也可以看出来。\n然后再把 Windows 下面的字体拷贝到上述目录中。\n重新加载新添加的字体：\nfc-cache -fv\n\n最后，要让java应用程序生效，还得重新启动一个应用。\n","categories":["Tools"],"tags":["pdfbox"]},{"title":"百度网盘网页版倍速播放","url":"/2020/05/%E7%99%BE%E5%BA%A6%E7%BD%91%E7%9B%98%E7%BD%91%E9%A1%B5%E7%89%88%E5%80%8D%E9%80%9F%E6%92%AD%E6%94%BE/","content":"百度网盘的宣传是可以倍速播放的，除了在手机上可以用外，我用 mac chrome 浏览器，是找不到倍速播放的功能的。\n今天突然发现通过这句脚本，竟然能神奇的控制播放速度，极大的提高了工作效率。\n进入百度网盘播放页面的控制台，执行如下代码：\nvideojs.getPlayers(&quot;video-player&quot;).html5player.tech_.setPlaybackRate(1.5)\n\n即可以把播放速度变为正常速度的 1.5 倍。\n但是如何调整网页版的画质清晰度，目前还没有找到方法。\n","tags":["百度"]},{"title":"Springboot多语言国际化","url":"/2020/10/Springboot%E5%A4%9A%E8%AF%AD%E8%A8%80%E5%9B%BD%E9%99%85%E5%8C%96/","content":"只要客户里面有老外，那都会涉及到多语言国际化的问题。我们看看在 Java Spring Boot 项目中如何设置多语言。\nyaml配置通过查看 org.springframework.boot.autoconfigure.context.MessageSourceAutoConfiguration，我们可以知道需要配置如下属性：spring.messages.basename。\nspring:  messages:    basename: i18n/message\n\n需要在src\\main\\resources\\i18n目录下添加如下文件：\n默认配置文件：message.properties英文：message_en_US.properties中文：message_zh_CN.properties\n内容都是 key=value 的格式，一行一条。\n语言处理类通过此设置，当请求头里面的Accept-Lanuage没有值时，以中文显示，否则按设置的值来处理。\npackage cn.com.wind.Wind.RTC.WebService.config;import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.Configuration;import org.springframework.util.StringUtils;import org.springframework.web.servlet.LocaleResolver;import org.springframework.web.servlet.config.annotation.WebMvcConfigurer;import org.springframework.web.servlet.i18n.AcceptHeaderLocaleResolver;import javax.servlet.http.HttpServletRequest;import java.util.Locale;@Configurationpublic class CustomLocaleResolver extends AcceptHeaderLocaleResolver implements WebMvcConfigurer &#123;    @Override    public Locale resolveLocale(HttpServletRequest request) &#123;        if (!StringUtils.hasText(request.getHeader(&quot;Accept-Language&quot;))) &#123;            return Locale.SIMPLIFIED_CHINESE;        &#125;        return super.resolveLocale(request);    &#125;    @Bean    public LocaleResolver localeResolver() &#123;        return this;    &#125;&#125;\n\n语言工具类package cn.com.wind.convertserver.utils;import org.springframework.context.MessageSource;import org.springframework.context.NoSuchMessageException;import org.springframework.context.i18n.LocaleContextHolder;import org.springframework.stereotype.Component;import java.util.Locale;@Componentpublic class I18nMessageUtils &#123;    private static MessageSource messageSource;    public MessageUtils(MessageSource messageSource) &#123;        MessageUtils.messageSource = messageSource;    &#125;    public static String getMessage(String messageKey, Locale locale) throws NoSuchMessageException &#123;        if (locale == null) &#123;            locale = LocaleContextHolder.getLocale();        &#125;        return messageSource.getMessage(messageKey, null, locale);    &#125;&#125;\n\n这样就可以通过这个工具类，做国际化的语言转换了。\n我们还发现一个问题，通过tomcat容器去运行的应用程序，在request请求里面未能获取到Locale的信息，但把tomcat改为jetty后，是可以成功获取到的。\npom.xml设置如下：\n&lt;dependency&gt;    &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;    &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;    &lt;exclusions&gt;        &lt;exclusion&gt;            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;            &lt;artifactId&gt;spring-boot-starter-tomcat&lt;/artifactId&gt;        &lt;/exclusion&gt;    &lt;/exclusions&gt;&lt;/dependency&gt;&lt;dependency&gt;    &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;    &lt;artifactId&gt;spring-boot-starter-jetty&lt;/artifactId&gt;&lt;/dependency&gt;","categories":["Java"],"tags":["i18n"]},{"title":"Python解析式推导式","url":"/2020/10/Python%E8%A7%A3%E6%9E%90%E5%BC%8F%E6%8E%A8%E5%AF%BC%E5%BC%8F/","content":"[ 值 for 元素 in 可迭代对象 if 条件 ]  # 值可以是函数表达式\n[ 值1 if 条件1 else 值2 for 元素 in 可迭代对象]   # 将 if-else 语句写入列表解析式\nret = [0.37, -1.9, 0.6, 2.54, 0.51, 1.09, 1.79, -1.2, -0.14, -0.5, -2.37, 0.0, -0.22, -0.15, 4.28, -0.85, -1.79, -0.44, -0.15, 2.34]# 例：从收益率序列中提取出负的收益率[i for i in ret if i &lt; 0][-1.9, -1.2, -0.14, -0.5, -2.37, -0.22, -0.15, -0.85, -1.79, -0.44, -0.15]# 正收益率赋值为1，负收益率赋值为0[1 if i &gt; 0 else 0 for i in ret][1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1]\n\nret_set = &#123;(0.37, -1.9, 0.6, 2.54, 0.51), (1.09, 1.79, -1.2, -0.14, -0.5), (-2.37, 0.0, -0.22, -0.15, 4.28), (-0.85, -1.79, -0.44, -0.15, 2.34)&#125;# 如何将元素打平[i for t in ret_set for i in t]\n\n内循环在外循环后面\n","categories":["Python"],"tags":["python"]},{"title":"腾迅云直播API调用","url":"/2020/05/%E8%85%BE%E8%BF%85%E4%BA%91%E7%9B%B4%E6%92%ADAPI%E8%B0%83%E7%94%A8/","content":"使用腾迅云产品时，一般情况下，腾迅都会提供一个 SDK 供我们调用。\n拿云直播里面的混流做为案例，讲解一下 SDK 的使用方法。通过官方的代码生成器，可以生成一个 Demo 做初步验证。\nimport com.tencentcloudapi.common.Credential;import com.tencentcloudapi.common.profile.ClientProfile;import com.tencentcloudapi.common.profile.HttpProfile;import com.tencentcloudapi.common.exception.TencentCloudSDKException;import com.tencentcloudapi.live.v20180801.LiveClient;import com.tencentcloudapi.live.v20180801.models.CreateCommonMixStreamRequest;import com.tencentcloudapi.live.v20180801.models.CreateCommonMixStreamResponse;public class CreateCommonMixStream &#123;    public static void main(String [] args) &#123;        try &#123;            Credential cred = new Credential(&quot;SecretId&quot;, &quot;SecretKey&quot;);                        HttpProfile httpProfile = new HttpProfile();            httpProfile.setEndpoint(&quot;live.tencentcloudapi.com&quot;);            ClientProfile clientProfile = new ClientProfile();            clientProfile.setHttpProfile(httpProfile);                        LiveClient client = new LiveClient(cred, &quot;ap-shanghai&quot;, clientProfile);                        // 混流参数            String params = &quot;&#123;&#125;&quot;;            CreateCommonMixStreamRequest req = CreateCommonMixStreamRequest.fromJsonString(params, CreateCommonMixStreamRequest.class);                        CreateCommonMixStreamResponse resp = client.CreateCommonMixStream(req);                        System.out.println(CreateCommonMixStreamRequest.toJsonString(resp));        &#125; catch (TencentCloudSDKException e) &#123;            System.out.println(e.toString());        &#125;    &#125;    &#125;\n\n我们可以把一些配置信息提取到 yml 配置文件中，同时让 Spring IOC 容器来管理 LiveClient。\ntencent-cloud:  secretId:  secretKey:  endpoint: live.tencentcloudapi.com  clientRegion: ap-shanghai\n\nimport com.tencentcloudapi.common.Credential;import com.tencentcloudapi.common.profile.ClientProfile;import com.tencentcloudapi.common.profile.HttpProfile;import com.tencentcloudapi.live.v20180801.LiveClient;import lombok.Data;import org.springframework.boot.context.properties.ConfigurationProperties;import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.Configuration;@Configuration@ConfigurationProperties(&quot;tencent-cloud&quot;)@Datapublic class TencentCloudConfig &#123;    private String secretId;    private String secretKey;    // 混流腾迅服务地址: live.tencentcloudapi.com    private String endpoint;    private String clientRegion;        @Bean    public LiveClient getLiveClient() &#123;        Credential cred = new Credential(secretId, secretKey);        HttpProfile httpProfile = new HttpProfile();        httpProfile.setEndpoint(endpoint);        // 还可以设置代理        // httpProfile.setProxyHost(host);        // httpProfile.setProxyPort(port);        ClientProfile clientProfile = new ClientProfile();        clientProfile.setHttpProfile(httpProfile);        LiveClient client = new LiveClient(cred, clientRegion, clientProfile);        return client;    &#125;&#125;\n\n在 Service 的实现中，我们就可以直接注入 LiveClient 了。\n@Servicepublic class MixStreamServiceImpl implements MixStreamService &#123;    @Autowired    private LiveClient liveClient;        @Override    public CreateCommonMixStreamResponse mixStream() &#123;        CreateCommonMixStreamRequest mixStreamRequest = new CreateCommonMixStreamRequest();        // 设置混流参数        // mixStreamRequest.setXXX        CreateCommonMixStreamResponse response;        try &#123;            response = liveClient.CreateCommonMixStream(mixStreamRequest);        &#125; catch (TencentCloudSDKException e) &#123;            e.printStackTrace();        &#125;        return response;    &#125;&#125;","categories":["Java","Spring Boot"],"tags":["java","云直播","腾迅云"]},{"title":"Mybatis开发中的一些最佳实践","url":"/2020/08/Mybatis%E5%BC%80%E5%8F%91%E4%B8%AD%E7%9A%84%E4%B8%80%E4%BA%9B%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/","content":"安装插件在 IntelliJ IDEA 中使用 Mybatis 可以安装 Free Mybatis Plugin 插件。\n这个时候可以很方便在 Mapper DAO 文件和 xml 文件中切换。 \n在 DAO 中写 @Param 注解@Mapperpublic interface UserMapper &#123;        User getById(@Param(&quot;id&quot;) Integer id);    &#125;\n\n这样在 xml 文件中，这个id才会有智能提示。会提示id = #&#123;id&#125; 而非 id = #&#123;param&#125;。\n插入记录后获取主键id在使用 mysql 的自增主键时，要插入成功后，才能知道主键的值。如果要在插入后，获取id的值，需要在 insert 标签里面添加两个属性。\n&lt;insert id=&quot;insertSelective&quot; useGeneratedKeys=&quot;true&quot; keyProperty=&quot;id&quot;&gt;    INSERT INTO table...&lt;/insert&gt;","categories":["Java"],"tags":["mybatis"]},{"title":"python模块的引入和调用","url":"/2020/10/python%E6%A8%A1%E5%9D%97%E7%9A%84%E5%BC%95%E5%85%A5%E5%92%8C%E8%B0%83%E7%94%A8/","content":"模块，以.py结尾，可以定义变量，函数或者类。\n引入模块import module1 [, module2 [, ...moduleN]]\n\nimport  module1[,module2[,....moduleN]] as ...  # 对模块重新命名，一般用于简化模块名\n\n调用模块通过import语句引入后调用调用函数，必须通过模块名.函数名的形式来调用。\nimport timetime.tzname(&#x27;CST&#x27;, &#x27;CST&#x27;)\n\n通过from…import语句只导入一个指定的部分from module1 import [,funName1 [, ...funNameN]]\n\nfrom time import tznametzname(&#x27;CST&#x27;, &#x27;CST&#x27;)\n\n通过from…import *语句导入所有内容这时调用函数不需要使用模块名.函数名的方式来调用。\nfrom time import *tzname(&#x27;CST&#x27;, &#x27;CST&#x27;)\n","categories":["Python"],"tags":["python"]},{"title":"IntelliJ IDEA新建一个Anaconda Python项目","url":"/2021/02/IntelliJ-IDEA%E6%96%B0%E5%BB%BA%E4%B8%80%E4%B8%AAAnaconda-Python%E9%A1%B9%E7%9B%AE/","content":"这篇文章讲讲在macOS系统中，如何一步一步创建一个基于Anaconda的项目。\n首先我们得确定已经在macOS中安装好了Anaconda。\n新建Python环境先查看一下当前环境，只有一个base。\n$ conda env list# conda environments:#base                  *  /opt/anaconda3\n\n以克隆方式创建一个和Anaconda base(root)环境一样的环境，取名叫py37。\n$ conda create --name py37 --clone baseSource:      /opt/anaconda3Destination: /opt/anaconda3/envs/py37\n\n按提示激活命令激活环境。\n$ conda activate py37\n\n新建Python项目File -&gt; New -&gt; Project…\n\n\n选择Python项目，New一个Project SDK。\n\n\n添加Python解析器\n\n\n选择 Conda Environment，使用Conda来管理包。\n我们前面使用命令行的方式创建一个env，所以选择Existing Environment。\n指定Interpreter，为刚才新建环境py37目录下的python。\n/opt/anaconda3/envs/py37/bin/python\n然后下一步，确定就创建好了。\n检查创建好项目后，我们写两行python代码。\nimport pandas as pdprint(&#x27;hello world&#x27;)\n\n检查，pandas, print下面没有红线，按Command，同时鼠标点击pandas或print，均能查看得到源代码，运行时，也会正常运行，说明配置一切正常。\n我们再看看项目结构。\n\n\n\n\n\n\n\n","categories":["Python"],"tags":["python","anaconda"]},{"title":"fasterxml jackson无法实例化非静态内部类","url":"/2020/08/fasterxml-jackson%E6%97%A0%E6%B3%95%E5%AE%9E%E4%BE%8B%E5%8C%96%E9%9D%9E%E9%9D%99%E6%80%81%E5%86%85%E9%83%A8%E7%B1%BB/","content":"com.fasterxml.jackson.databind.exc.MismatchedInputException: Cannot construct instance of gy.finolo.Outer$Inner (although at least one Creator exists): can only instantiate non-static inner class by using default, no-argument constructor\nfasterxml无法实例化非静态内部类。把内部类改为 static 的静态类就可以了。\n@Datapublic class Outer &#123;        private Inner inner;        @Data    public static class Inner &#123;            &#125;&#125;","categories":["Java"],"tags":["json"]},{"title":"IntelliJ IDEA配置Anaconda base(root)相同环境","url":"/2021/02/IntelliJ-IDEA%E9%85%8D%E7%BD%AEAnaconda-base-root-%E7%9B%B8%E5%90%8C%E7%8E%AF%E5%A2%83/","content":"Anaconda的基本操作，可以参考这篇文章 macOS下Anaconda的安装及环境切换（Python2&#x2F;Python3）\n这篇文章讲讲如何clone一个和Anaconda base(root)一样的环境，也就是和base(root)有一样的多的包的环境。\n复制的命令如下：\nconda create --name &lt;new_env_name&gt; --clone &lt;copied_env_name&gt;\n\nconda create --name new_env --clone base\n\n创建Python项目的时候，\nNew Project -&gt; Project SDK (New…) -&gt; Add Interpreter -&gt; Conda Environment -&gt; Existing Environment，这里面就填刚才新建的新环境路径。\n安装包首先要切换环境，linux是source activate &lt;env&gt;，而Windows系统是activate &lt;env&gt;。\n安装包命令：\nconda install &lt;package&gt;，如果没有的话，那就用pip来安装。\npip install &lt;package&gt;\n","categories":["Python"],"tags":["anaconda"]},{"title":"Python列表推导式中if else判断","url":"/2021/02/Python%E5%88%97%E8%A1%A8%E6%8E%A8%E5%AF%BC%E5%BC%8F%E4%B8%ADif-else%E5%88%A4%E6%96%AD/","content":"if[fun(item) for item in list if condition]\n\nif else[fun1(item) if condition else fun2(item) for item in list]\n\nif else if[fun1(item) if condition1 else fun2(item) if condition2 fun3(item) else fun4(item) for item in list]\n","categories":["Python"]},{"title":"Python DataFrame Series中小数精度精确到两位小数","url":"/2021/02/Python-DataFrame-Series%E4%B8%AD%E5%B0%8F%E6%95%B0%E7%B2%BE%E5%BA%A6%E7%B2%BE%E7%A1%AE%E5%88%B0%E4%B8%A4%E4%BD%8D%E5%B0%8F%E6%95%B0/","content":"在DataFrame或Series中，如果使用df.round(2)，就会保留最多两位小数。\n比如，2.00显示为2，1.90显示为1.9。\n这样显示就会存在精度缺失的问题。\n可以使用如下方法改善显示。\npd.options.display.float_format = &#x27;$&#123;:,.2f&#125;&#x27;.format\n\n也可以恢复默认：\npd.options.display.float_format = None\n","categories":["Python"],"tags":["round"]},{"title":"Spring Cloud Alibaba限流降级Sentinel入门搭建","url":"/2021/02/Spring-Cloud-Alibaba%E9%99%90%E6%B5%81%E9%99%8D%E7%BA%A7Sentinel%E5%85%A5%E9%97%A8%E6%90%AD%E5%BB%BA/","content":"启动 sentinel dashboard在官网下载 sentinel-dashboard-1.8.1.jar，并运行\njava -Dserver.port=8081 -Dcsp.sentinel.dashboard.server=localhost:8081 -Dproject.name=sentinel-dashboard -jar sentinel-dashboard-1.8.1.jar\n\n8081端口是指访问dashboard的端口。\n启动以后，可以通过链接 http://localhost:8081 访问到页面，用户名和密码都是 sentinel。\n\n\n把项目加入到 dashboard写一个标准的Springboot项目。\npom.xml文件里面添加如下依赖：\n&lt;dependency&gt;    &lt;groupId&gt;com.alibaba.cloud&lt;/groupId&gt;    &lt;artifactId&gt;spring-cloud-starter-alibaba-sentinel&lt;/artifactId&gt;    &lt;version&gt;$&#123;sentinel.version&#125;&lt;/version&gt;&lt;/dependency&gt;\n\n在项目的 application.yml 配置文件里面添加如下内容：\nspring:  cloud:    sentinel:      transport:        dashboard: localhost:8081\n\n在需要加入限流的业务逻辑里面添加如下内容：\n@Servicepublic class UserServiceImpl implements UserService &#123;    // 限流的规则设置    @PostConstruct    private void initFlowRules() &#123;        List&lt;FlowRule&gt; rules = new ArrayList&lt;&gt;();        FlowRule flowRule = new FlowRule();        flowRule.setResource(&quot;UserFlowRule&quot;);        // 设置QPS为1        flowRule.setGrade(RuleConstant.FLOW_GRADE_QPS);        flowRule.setCount(1);        rules.add(flowRule);        FlowRuleManager.loadRules(rules);    &#125;    @Override    @SentinelResource(value = &quot;UserFlowRule&quot;, blockHandler = &quot;degradeMethod&quot;)    public String sayHello(String name) &#123;        System.out.println(&quot;executed...&quot;);        return &quot;hello, &quot; + name;    &#125;    /**     * 降级方法     * @param name     * @param blockException     * @return     */    public String degradeMethod(String name, BlockException blockException) &#123;        return &quot;限流&quot; + blockException.getRule().getResource();    &#125;&#125;\n\n","categories":["Java"],"tags":["sentinel"]},{"title":"websocket测试工具wscat","url":"/2020/11/websocket%E6%B5%8B%E8%AF%95%E5%B7%A5%E5%85%B7wscat/","content":"WebSocket Cat (wscat)是一个连接websocket的小工具。\n安装方法npm install -g wscat\n\n使用方法Usage: wscat [options] (--listen &lt;port&gt; | --connect &lt;url&gt;)Options:  -V, --version                       output the version number  -l, --listen &lt;port&gt;                 listen on port  -c, --connect &lt;url&gt;                 connect to a WebSocket server  -p, --protocol &lt;version&gt;            optional protocol version  -o, --origin &lt;origin&gt;               optional origin  -x, --execute &lt;command&gt;             execute command after connecting  -w, --wait &lt;seconds&gt;                wait given seconds after executing command  -P, --show-ping-pong                print a notification when a ping or pong is received  --host &lt;host&gt;                       optional host  -s, --subprotocol &lt;protocol&gt;        optional subprotocol (default: [])  -n, --no-check                      do not check for unauthorized certificates  -H, --header &lt;header:value&gt;         set an HTTP header. Repeat to set multiple (--connect only) (default: [])  --auth &lt;username:password&gt;          add basic HTTP authentication header (--connect only)  --ca &lt;ca&gt;                           specify a Certificate Authority (--connect only)  --cert &lt;cert&gt;                       specify a Client SSL Certificate (--connect only)  --key &lt;key&gt;                         specify a Client SSL Certificate&#x27;s key (--connect only)  --passphrase [passphrase]           specify a Client SSL Certificate Key&#x27;s passphrase (--connect only). If you don&#x27;t provide a value, it will be prompted for  --no-color                          run without color  --slash                             enable slash commands for control frames (/ping, /pong, /close [code [, reason]])  --proxy &lt;[protocol://]host[:port]&gt;  connect via a proxy. Proxy must support CONNECT method  -h, --help       \n\n使用案例在终端1启动一个websocket服务，监听9000端口。\nwscat --listen 9000Listening on port 9000 (press CTRL+C to quit)\n\n在终端2连接上websocket服务。\nwscat -c ws://localhost:9000/echoConnected (press CTRL+C to quit)\n\n连上以后，在服务端，会提示信息：\nClient connected\n然后就可以两端互相发送消息了。\n","categories":["Tools"],"tags":["websocket"]},{"title":"常用颜色名字RGB和十六进制","url":"/2021/02/%E5%B8%B8%E7%94%A8%E9%A2%9C%E8%89%B2%E5%90%8D%E5%AD%97RGB%E5%92%8C%E5%8D%81%E5%85%AD%E8%BF%9B%E5%88%B6/","content":"\n\n    \n    \n        颜色\n        英文代码\n        形象描述\n        十六进制\n        RGB\n    \n    \n        　\n        LightPink\n        浅粉红\n        #FFB6C1\n        255,182,193\n    \n    \n        　\n        Pink\n        粉红\n        #FFC0CB\n        255,192,203\n    \n    \n        　\n        Crimson\n        猩红\n        #DC143C\n        220,20,60\n    \n    \n        　\n        LavenderBlush\n        脸红的淡紫色\n        #FFF0F5\n        255,240,245\n    \n    \n        　\n        PaleVioletRed\n        苍白的紫罗兰红色\n        #DB7093\n        219,112,147\n    \n    \n        　\n        HotPink\n        热情的粉红\n        #FF69B4\n        255,105,180\n    \n    \n        　\n        DeepPink\n        深粉色\n        #FF1493\n        255,20,147\n    \n    \n        　\n        MediumVioletRed\n        适中的紫罗兰红色\n        #C71585\n        199,21,133\n    \n    \n        　\n        Orchid\n        兰花的紫色\n        #DA70D6\n        218,112,214\n    \n    \n        　\n        Thistle\n        蓟\n        #D8BFD8\n        216,191,216\n    \n    \n        　\n        plum\n        李子\n        #DDA0DD\n        221,160,221\n    \n    \n        　\n        Violet\n        紫罗兰\n        #EE82EE\n        238,130,238\n    \n    \n        　\n        Magenta\n        洋红\n        #FF00FF\n        255,0,255\n    \n    \n        　\n        Fuchsia\n        灯笼海棠(紫红色)\n        #FF00FF\n        255,0,255\n    \n    \n        　\n        DarkMagenta\n        深洋红色\n        #8B008B\n        139,0,139\n    \n    \n        　\n        Purple\n        紫色\n        #800080\n        128,0,128\n    \n    \n        　\n        MediumOrchid\n        适中的兰花紫\n        #BA55D3\n        186,85,211\n    \n    \n        　\n        DarkVoilet\n        深紫罗兰色\n        #9400D3\n        148,0,211\n    \n    \n        　\n        DarkOrchid\n        深兰花紫\n        #9932CC\n        153,50,204\n    \n    \n        　\n        Indigo\n        靛青\n        #4B0082\n        75,0,130\n    \n    \n        　\n        BlueViolet\n        深紫罗兰的蓝色\n        #8A2BE2\n        138,43,226\n    \n    \n        　\n        MediumPurple\n        适中的紫色\n        #9370DB\n        147,112,219\n    \n    \n        　\n        MediumSlateBlue\n        适中的板岩暗蓝灰色\n        #7B68EE\n        123,104,238\n    \n    \n        　\n        SlateBlue\n        板岩暗蓝灰色\n        #6A5ACD\n        106,90,205\n    \n    \n        　\n        DarkSlateBlue\n        深岩暗蓝灰色\n        #483D8B\n        72,61,139\n    \n    \n        　\n        Lavender\n        熏衣草花的淡紫色\n        #E6E6FA\n        230,230,250\n    \n    \n        　\n        GhostWhite\n        幽灵的白色\n        #F8F8FF\n        248,248,255\n    \n    \n        　\n        Blue\n        纯蓝\n        #0000FF\n        0,0,255\n    \n    \n        　\n        MediumBlue\n        适中的蓝色\n        #0000CD\n        0,0,205\n    \n    \n        　\n        MidnightBlue\n        午夜的蓝色\n        #191970\n        25,25,112\n    \n    \n        　\n        DarkBlue\n        深蓝色\n        #00008B\n        0,0,139\n    \n    \n        　\n        Navy\n        海军蓝\n        #000080\n        0,0,128\n    \n    \n        　\n        RoyalBlue\n        皇家蓝\n        #4169E1\n        65,105,225\n    \n    \n        　\n        CornflowerBlue\n        矢车菊的蓝色\n        #6495ED\n        100,149,237\n    \n    \n        　\n        LightSteelBlue\n        淡钢蓝\n        #B0C4DE\n        176,196,222\n    \n    \n        　\n        LightSlateGray\n        浅石板灰\n        #778899\n        119,136,153\n    \n    \n        　\n        SlateGray\n        石板灰\n        #708090\n        112,128,144\n    \n    \n        　\n        DoderBlue\n        道奇蓝\n        #1E90FF\n        30,144,255\n    \n    \n        　\n        AliceBlue\n        爱丽丝蓝\n        #F0F8FF\n        240,248,255\n    \n    \n        　\n        SteelBlue\n        钢蓝\n        #4682B4\n        70,130,180\n    \n    \n        　\n        LightSkyBlue\n        淡蓝色\n        #87CEFA\n        135,206,250\n    \n    \n        　\n        SkyBlue\n        天蓝色\n        #87CEEB\n        135,206,235\n    \n    \n        　\n        DeepSkyBlue\n        深天蓝\n        #00BFFF\n        0,191,255\n    \n    \n        　\n        LightBLue\n        淡蓝\n        #ADD8E6\n        173,216,230\n    \n    \n        　\n        PowDerBlue\n        火药蓝\n        #B0E0E6\n        176,224,230\n    \n    \n        　\n        CadetBlue\n        军校蓝\n        #5F9EA0\n        95,158,160\n    \n    \n        　\n        Azure\n        蔚蓝色\n        #F0FFFF\n        240,255,255\n    \n    \n        　\n        LightCyan\n        淡青色\n        #E1FFFF\n        225,255,255\n    \n    \n        　\n        PaleTurquoise\n        苍白的绿宝石\n        #AFEEEE\n        175,238,238\n    \n    \n        　\n        Cyan\n        青色\n        #00FFFF\n        0,255,255\n    \n    \n        　\n        Aqua\n        水绿色\n        #D4F2E7\n        212,242,231\n    \n    \n        　\n        DarkTurquoise\n        深绿宝石\n        #00CED1\n        0,206,209\n    \n    \n        　\n        DarkSlateGray\n        深石板灰\n        #2F4F4F\n        47,79,79\n    \n    \n        　\n        DarkCyan\n        深青色\n        #008B8B\n        0,139,139\n    \n    \n        　\n        Teal\n        水鸭色\n        #008080\n        0,128,128\n    \n    \n        　\n        MediumTurquoise\n        适中的绿宝石\n        #48D1CC\n        72,209,204\n    \n    \n        　\n        LightSeaGreen\n        浅海洋绿\n        #20B2AA\n        32,178,170\n    \n    \n        　\n        Turquoise\n        绿宝石\n        #40E0D0\n        64,224,208\n    \n    \n        　\n        Auqamarin\n        绿玉\\碧绿色\n        #7FFFAA\n        127,255,170\n    \n    \n        　\n        MediumAquamarine\n        适中的碧绿色\n        #00FA9A\n        0,250,154\n    \n    \n        　\n        MediumSpringGreen\n        适中的春天的绿色\n        #00FF7F\n        0,255,127\n    \n    \n        　\n        MintCream\n        薄荷奶油\n        #F5FFFA\n        245,255,250\n    \n    \n        　\n        SpringGreen\n        春天的绿色\n        #3CB371\n        60,179,113\n    \n    \n        　\n        SeaGreen\n        海洋绿\n        #2E8B57\n        46,139,87\n    \n    \n        　\n        Honeydew\n        蜂蜜\n        #F0FFF0\n        240,255,240\n    \n    \n        　\n        LightGreen\n        淡绿色\n        #90EE90\n        144,238,144\n    \n    \n        　\n        PaleGreen\n        苍白的绿色\n        #98FB98\n        152,251,152\n    \n    \n        　\n        DarkSeaGreen\n        深海洋绿\n        #8FBC8F\n        143,188,143\n    \n    \n        　\n        LimeGreen\n        酸橙绿\n        #32CD32\n        50,205,50\n    \n    \n        　\n        Lime\n        酸橙色\n        #00FF00\n        0,255,0\n    \n    \n        　\n        ForestGreen\n        森林绿\n        #228B22\n        34,139,34\n    \n    \n        　\n        Green\n        纯绿\n        #008000\n        0,128,0\n    \n    \n        　\n        DarkGreen\n        深绿色\n        #006400\n        0,100,0\n    \n\n    \n        　\n        Chartreuse\n        查特酒绿\n        #7FFF00\n        127,255,0\n    \n    \n        　\n        LawnGreen\n        草坪绿\n        #7CFC00\n        124,252,0\n    \n    \n        　\n        GreenYellow\n        绿黄色\n        #ADFF2F\n        173,255,47\n    \n    \n        　\n        OliveDrab\n        橄榄土褐色\n        #556B2F\n        85,107,47\n    \n    \n        　\n        Beige\n        米色(浅褐色)\n        #F5F5DC\n        245,245,220\n    \n    \n        　\n        LightGoldenrodYellow\n        浅秋麒麟黄\n        #FAFAD2\n        250,250,210\n    \n    \n        　\n        Ivory\n        象牙\n        #FFFFF0\n        255,255,240\n    \n    \n        　\n        LightYellow\n        浅黄色\n        #FFFFE0\n        255,255,224\n    \n    \n        　\n        Yellow\n        纯黄\n        #FFFF00\n        255,255,0\n    \n    \n        　\n        Olive\n        橄榄\n        #808000\n        128,128,0\n    \n    \n        　\n        DarkKhaki\n        深卡其布\n        #BDB76B\n        189,183,107\n    \n    \n        　\n        LemonChiffon\n        柠檬薄纱\n        #FFFACD\n        255,250,205\n    \n    \n        　\n        PaleGodenrod\n        灰秋麒麟\n        #EEE8AA\n        238,232,170\n    \n    \n        　\n        Khaki\n        卡其布\n        #F0E68C\n        240,230,140\n    \n    \n        　\n        Gold\n        金\n        #FFD700\n        255,215,0\n    \n    \n        　\n        Cornislk\n        玉米色\n        #FFF8DC\n        255,248,220\n    \n    \n        　\n        GoldEnrod\n        秋麒麟\n        #DAA520\n        218,165,32\n    \n    \n        　\n        FloralWhite\n        花的白色\n        #FFFAF0\n        255,250,240\n    \n    \n        　\n        OldLace\n        老饰带\n        #FDF5E6\n        253,245,230\n    \n    \n        　\n        Wheat\n        小麦色\n        #F5DEB3\n        245,222,179\n    \n    \n        　\n        Moccasin\n        鹿皮鞋\n        #FFE4B5\n        255,228,181\n    \n    \n        　\n        Orange\n        橙色\n        #FFA500\n        255,165,0\n    \n    \n        　\n        PapayaWhip\n        番木瓜\n        #FFEFD5\n        255,239,213\n    \n    \n        　\n        BlanchedAlmond\n        漂白的杏仁\n        #FFEBCD\n        255,235,205\n    \n    \n        　\n        NavajoWhite\n        纳瓦霍白\n        #FFDEAD\n        255,222,173\n    \n    \n        　\n        AntiqueWhite\n        古代的白色\n        #FAEBD7\n        250,235,215\n    \n    \n        　\n        Tan\n        晒黑\n        #D2B48C\n        210,180,140\n    \n    \n        　\n        BrulyWood\n        结实的树\n        #DEB887\n        222,184,135\n    \n    \n        　\n        Bisque\n        (浓汤)乳脂,番茄等\n        #FFE4C4\n        255,228,196\n    \n    \n        　\n        DarkOrange\n        深橙色\n        #FF8C00\n        255,140,0\n    \n    \n        　\n        Linen\n        亚麻布\n        #FAF0E6\n        250,240,230\n    \n    \n        　\n        Peru\n        秘鲁\n        #CD853F\n        205,133,63\n    \n    \n        　\n        PeachPuff\n        桃色\n        #FFDAB9\n        255,218,185\n    \n    \n        　\n        SandyBrown\n        沙棕色\n        #F4A460\n        244,164,96\n    \n    \n        　\n        Chocolate\n        巧克力\n        #D2691E\n        210,105,30\n    \n    \n        　\n        SaddleBrown\n        马鞍棕色\n        #8B4513\n        139,69,19\n    \n    \n        　\n        SeaShell\n        海贝壳\n        #FFF5EE\n        255,245,238\n    \n    \n        　\n        Sienna\n        黄土赭色\n        #A0522D\n        160,82,45\n    \n    \n        　\n        LightSalmon\n        浅鲜肉(鲑鱼)色\n        #FFA07A\n        255,160,122\n    \n    \n        　\n        Coral\n        珊瑚\n        #FF7F50\n        255,127,80\n    \n    \n        　\n        OrangeRed\n        橙红色\n        #FF4500\n        255,69,0\n    \n    \n        　\n        DarkSalmon\n        深鲜肉(鲑鱼)色\n        #E9967A\n        233,150,122\n    \n    \n        　\n        Tomato\n        番茄\n        #FF6347\n        255,99,71\n    \n    \n        　\n        MistyRose\n        薄雾玫瑰\n        #FFE4E1\n        255,228,225\n    \n    \n        　\n        Salmon\n        鲜肉(鲑鱼)色\n        #FA8072\n        250,128,114\n    \n    \n        　\n        Snow\n        雪\n        #FFFAFA\n        255,250,250\n    \n    \n        　\n        LightCoral\n        淡珊瑚色\n        #F08080\n        240,128,128\n    \n    \n        　\n        RosyBrown\n        玫瑰棕色\n        #BC8F8F\n        188,143,143\n    \n    \n        　\n        IndianRed\n        印度红\n        #CD5C5C\n        205,92,92\n    \n    \n        　\n        Red\n        纯红\n        #FF0000\n        255,0,0\n    \n    \n        　\n        Brown\n        棕色\n        #A52A2A\n        165,42,42\n    \n    \n        　\n        FireBrick\n        耐火砖\n        #B22222\n        178,34,34\n    \n    \n        　\n        DarkRed\n        深红色\n        #8B0000\n        139,0,0\n    \n    \n        　\n        Maroon\n        栗色\n        #800000\n        128,0,0\n    \n    \n        　\n        White\n        纯白\n        #FFFFFF\n        255,255,255\n    \n    \n        　\n        WhiteSmoke\n        白烟\n        #F5F5F5\n        245,245,245\n    \n    \n        　\n        Gainsboro\n        亮灰色\n        #DCDCDC\n        220,220,220\n    \n    \n        　\n        LightGrey\n        浅灰色\n        #D3D3D3\n        211,211,211\n    \n    \n        　\n        Silver\n        银白色\n        #C0C0C0\n        192,192,192\n    \n    \n        　\n        DarkGray\n        深灰色\n        #A9A9A9\n        169,169,169\n    \n    \n        　\n        Gray\n        灰色\n        #808080\n        128,128,128\n    \n    \n        　\n        DimGray\n        暗淡的灰色\n        #696969\n        105,105,105\n    \n    \n        　\n        Black\n        纯黑\n        #000000\n        0,0,0\n    \n    \n\n"},{"title":"公众号文章Markdown的样式设置","url":"/2020/11/%E5%85%AC%E4%BC%97%E5%8F%B7%E6%96%87%E7%AB%A0Markdown%E7%9A%84%E6%A0%B7%E5%BC%8F%E8%AE%BE%E7%BD%AE/","content":"主题用的是”山吹”，下面自定义设置的样式。\n外边距设置为 2px ，不然外边距有些大。\n#nice &#123;  padding: 2px;&#125;\n\n字体设置为 17px, 对齐方式为两边对齐。\n#nice p &#123;  font-size: 17px;  text-align: justify;  ...&#125;\n"},{"title":"Mybatis一对多关系collection","url":"/2020/09/Mybatis%E4%B8%80%E5%AF%B9%E5%A4%9A%E5%85%B3%E7%B3%BBcollection/","content":"Mybatis中的一对多关系，用collection。\n一个学生表，Student，一个老师表，Teacher。一个老师下面有多个学生。学生表有一个字段 teacher_id。\n学生表对应的类\n@Data@Builder@AllArgsConstructor@NoArgsConstructorpublic class Student &#123;        private Integer id;    private String name;    private Integer teacherId;&#125; \n\n教师类\n@Data@Builder@AllArgsConstructor@NoArgsConstructorpublic class Teacher &#123;        private Integer id;    private Integer age;&#125; \n\n我们要查出教师及其学生的列表。\n教师及学生列表类\n@Data@Builder@AllArgsConstructor@NoArgsConstructorpublic class TeacherInfo &#123;        private Integer id;    private String name;    private List&lt;Student&gt; students;&#125; \n\n获取所以教师及学生列表信息的 DAO 层接口。\npublic interface TeacherDao &#123;        List&lt;TeacherInfo&gt; findTeachers();&#125;\n\n上述接口对应的 xml 文件的核心内容如下：\n&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;!DOCTYPE mapper PUBLIC &quot;-//mybatis.org//DTD Mapper 3.0//EN&quot; &quot;http://mybatis.org/dtd/mybatis-3-mapper.dtd&quot;&gt;&lt;mapper namespace=&quot;gy.finolo.dao.TeacherDao&quot;&gt;    &lt;resultMap id=&quot;teacherInfoMap&quot; type=&quot;gy.finolo.model.TeacherInfo&quot;&gt;        &lt;id column=&quot;id&quot; jdbcType=&quot;INTEGER&quot; property=&quot;id&quot;/&gt;        &lt;result column=&quot;age&quot; jdbcType=&quot;INTEGER&quot; property=&quot;age&quot;/&gt;        &lt;collection property=&quot;students&quot; ofType=&quot;gy.finolo.model.Student&quot;&gt;            &lt;!-- &lt;id column=&quot;student_id&quot; property=&quot;id&quot; /&gt; --&gt;            &lt;result column=&quot;name&quot; property=&quot;name&quot; /&gt;        &lt;/collection&gt;    &lt;/resultMap&gt;    &lt;select id=&quot;findTeachers&quot; resultMap=&quot;teacherInfoMap&quot;&gt;        SELECT t.id, t.age, s.name FROM t_teacher t INNER JOIN t_student s ON (t.id = s.teacher_id);    &lt;/select&gt;&lt;/mapper&gt;\n\n需要说明的有以下几点：\n\n主表和副表的 id 标签都可以不要，这时只是属性里面没有 id 的值而已。我之前以为是以 id 来判断两条记录是否相同的，现在看来并不是。\n\n如果需要获取学生id值，为了避免column字段重复，所以在SQL语句写别名，同时在xml也做相应修改。\n\n\n&lt;id column=&quot;student_id&quot; property=&quot;id&quot; /&gt;\n\n&lt;select id=&quot;findTeachers&quot; resultMap=&quot;teacherInfoMap&quot;&gt;    SELECT t.id, t.age, s.id student_id, s.name FROM t_teacher t INNER JOIN t_student s ON (t.id = s.teacher_id);&lt;/select&gt;\n\n","categories":["Java"],"tags":["mybatis"]},{"title":"java中的枚举类","url":"/2020/09/java%E4%B8%AD%E7%9A%84%E6%9E%9A%E4%B8%BE%E7%B1%BB/","content":"在 java 中经常会使用到枚举类。类名可以使用 Enum 结尾。 \n@AllArgsConstructorpublic enum UserTypeEnum &#123;    USER_TYPE_TEACHER   (1, &quot;教师&quot;),    USER_TYPE_STUDENT   (2, &quot;学生&quot;)    ;    @Setter    @Getter    private Integer type;    @Setter    @Getter    private String description;&#125;\n\n循环遍历枚举类的每一个值\nfor (UserTypeEnum userType : UserTypeEnum.values()) &#123;    // userType.getType();    // userType.getDescription();&#125;","categories":["Java"],"tags":["java"]},{"title":"java AOP切面处理","url":"/2020/09/java-AOP%E5%88%87%E9%9D%A2%E5%A4%84%E7%90%86/","content":"在返回给客户端前，通过 AOP 给返回的类做处理。\n@Aspect@Componentpublic class ResponseAspect &#123;    // 定义切点    @Pointcut(&quot;execution(public * gy.finolo..*.*(..))&quot;)    public void afterResponse() &#123;    &#125;    @AfterReturning(returning = &quot;object&quot;, pointcut = &quot;afterResponse()&quot;)    public void doAfterReturning(Object object) &#123;        if (object instanceof Response) &#123;            // logic        &#125;    &#125;&#125;\n\nAOP 有如下5种通知类型，@Around, @Before, @After, @AfterReturning 和 @AfterThrowing。\n\n如果在通知内都未调用 proceedingJoinPoint.proceed()，那执行顺序为如下：\n\nAround -&gt; After -&gt; AfterReturning (正常返回) | AfterThrowing (异常返回)\n\n如果在通知内部调用了 proceedingJoinPoint.proceed()，此句代码相当于是执行 Before（如果有的话） 和 目标对象的方法 逻辑。\n\nPointcut注解Pointcut 表达式除了使用 execution 以外，还可以使用 annotation，比如：\n定义切点\n@Pointcut(&quot;@annotation(gy.finolo.anno.AopAnnotation)&quot;)\n我们再创建一个注解\n@Target(&#123;ElementType.METHOD&#125;)@Retention(RetentionPolicy.RUNTIME)public @interface AopAnnotation &#123;&#125;\n\n在方法上面只要是被 @AopAnnotation 注解了，那逻辑都会被切入。\n","categories":["Java"],"tags":["aop"]},{"title":"JVM调试基础","url":"/2020/12/JVM%E8%B0%83%E8%AF%95%E5%9F%BA%E7%A1%80/","content":"标准参数-help\n-version\nX参数X参数是非标准化参数\n-Xint: 解释执行\n-Xcomp: 第一次使用就编译成本地代码\n-Xmixed: 混合模式，JVM自己来决定是否编译成本地代码\njava -Xcomp -version\nXX参数XX参数也是非标准化参数\n有两种类型\nBoolean: -XX:[+-]启动或禁用name属性\n-XX:+UseConcMarkSweepGC\n-XX:+UseG1GC\n非Boolean类型\n-XX:&#x3D;\n-XX:MaxGCPauseMillis&#x3D;500\n-XX:GCTimeRatio&#x3D;19\n-Xmx -Xms\n这是XX参数\n-Xms &#x3D; -XX:InitialHeapSize\n-Xmx &#x3D; -XX:MaxHeapSize\njinfo -flag MaxHeapSize 36247-XX:MaxHeapSize=4200595456\n\n-Xss &#x3D; -XX:ThreadStackSize\n-XX:+PrintFlagsInitial\n-XX:+PrintFlagFinal\n&#x3D;默认值 :&#x3D; 修改过的值\njps -l参数代表：full package name\n# jps -l42826 sun.tools.jps.Jps102271 org.elasticsearch.bootstrap.Elasticsearch\n\njstat查看JVM统计信息jstat -class   \n-gc, -compiler\nS0C, S1C, S0U, S1U: C - 总量 U - 使用量\nEC, EU, Eden\nOC, OU, Old区\nMC, MU: Metaspace区\nCCSC, CCSU, 压缩类空间总量&#x2F;使用量\nYGC, YGCT: YoungGC的次数与时间\nFGC, FGCT: FullGC次数和时间\nGCT: 总的GC时间\n导出内存映像文件内存溢出自动导出，在VM arguments里面写入如下参数\n-XX:+HeapDumpOnOutOfMemoryError\n-XX:HeapDumpPath&#x3D;.&#x2F;\n使用jmap命令手动导出\njmap -help\njmap -dump:format&#x3D;b,file&#x3D;.&#x2F;heap.hprofJV\nMAThttp://www.eclipse.org/mat\n","tags":["jvm"]},{"title":"nginx启动报错[emerg] getpwnam(nginx) failed in","url":"/2020/09/nginx%E5%90%AF%E5%8A%A8%E6%8A%A5%E9%94%99-emerg-getpwnam-nginx-failed-in/","content":"今天做了一个 nginx 项目的迁移，把老的整个 nginx 文件夹拷贝到新的机器上，启动 nginx, 结果报错，信息如下：\n# sbin/nginxnginx: [emerg] getpwnam(&quot;nginx&quot;) failed in /nginx/nginx-1.12.2/conf/nginx.conf:3\n\n说明在 nginx.conf 配置文件的第三行有错。查看文件内容：\nuser nginx nginx;\n\n新的机器没有 nginx 这个用户，添加一个便是。\n# useradd nginx\n\n重启再次启动 nginx，成功。\n","categories":["Tools","Nginx"],"tags":["nginx"]},{"title":"Python Series和DataFrame的索引和切片","url":"/2020/12/Python-Series%E5%92%8CDataFrame%E7%9A%84%E7%B4%A2%E5%BC%95%E5%92%8C%E5%88%87%E7%89%87/","content":"整理得最全的提取Series或DataFrame数据的公式。\n提取单个元素基于位置提取数据s[index]s.iat[index]df.iat[row_index, col_index]\n基于 label 提取数据s[label]s.at[label]df.at[row_label, col_label]\n提取多个元素对于 Series基于位置提取s[a:b]s[[a,b,c]]s.iloc[a:b]s.iloc[[a,b,c]]\n基于标签提取s[label1:labeln]s[[label1, label2, label3]]s.loc[label1:labeln]s.loc[[label1, label2, label3]]\n对于 DataFrame基于位置\n\n\n行列\n中括号\niloc\n\n\n\n单行\ndf[a:a+1]\ndf.iloc[a]等价于df.iloc[a,:]、df.iloc[[a]]等价于df.iloc[[a],:]\n\n\n多行\ndf[a:b]\ndf.iloc[a:b]等价于df.iloc[a:b,:]、df.iloc[[a,b,c]]等价于df.iloc[[a,b,c],:]\n\n\n单列\n-\ndf.iloc[:,a]、df.iloc[:,[a]] （：不能省略）\n\n\n多列\n-\ndf.iloc[:,a:b]、df.iloc[:,[a,b,d]]\n\n\n行列\n-\ndf.iloc[a:b,n:m]、df.iloc[[a,b,c],[n,m]]\n\n\n基于标签\n\n\n行列\n中括号\nloc\n\n\n\n单行\n-\ndf.loc[label]、df.loc[[label]]\n\n\n多行\n-\ndf.loc[lable1：lable3]、df.loc[[lable1,lable2,…]]\n\n\n单列\ndf[clo1]、df[[col1]]\ndf.loc[:,col]、df.loc[:,[col]] （：不能省略）\n\n\n多列\ndf[[clo1,col2]]\ndf.loc[:,[col1,col2]] 、df.loc[:,col1:col4]]\n\n\n行列\n-\ndf.loc[lable1：lable3,col1:col4]]、df.loc[[lable1,lable2,…],[clo1,col2]\n\n\n","categories":["Python"],"tags":["python"]},{"title":"使用mdnice自定义样式","url":"/2020/12/%E4%BD%BF%E7%94%A8mdnice%E8%87%AA%E5%AE%9A%E4%B9%89%E6%A0%B7%E5%BC%8F/","content":"使用 mdnice 来编辑公众号文章，我做了一个自定义的样式，记录一下。\n/*自定义样式，实时生效*//* simon * h2 默认大小为 font-size: 1.375em; * padding-left/right 默认为 *//* 全局属性 * 页边距 padding: 30px; * 全文字体 font-family: ptima-Regular; * 英文换行 word-break: break-all; */#nice &#123;  line-height: 27.2px;  min-height: 17px;  //font-family: -apple-system-font,BlinkMacSystemFont,&quot;Helvetica Neue&quot;,&quot;PingFang SC&quot;,&quot;Hiragino Sans GB&quot;,&quot;Microsoft YaHei UI&quot;,&quot;Microsoft YaHei&quot;,Arial,sans-serif;  padding-left: 6px;  padding-right: 6px;&#125;/* 段落，下方未标注标签参数均同此处 * 上边距 margin-top: 5px; * 下边距 margin-bottom: 5px; * 行高 line-height: 26px; * 词间距 word-spacing: 3px; * 字间距 letter-spacing: 3px; * 对齐 text-align: left; * 颜色 color: #3e3e3e; * 字体大小 font-size: 16px; * 首行缩进 text-indent: 2em; */#nice p &#123;  line-height: 27.2px;  text-align: justify;  font-size: 17px;  letter-spacing: 0.544px;  padding-top: 13px;  padding-bottom: 13px;  color: rgb(58, 58, 58);&#125;/* 一级标题 */#nice h1 &#123;  // font-size: 1.8em;  font-size: 1.5em;  //color: #009688;  color: rgb(239, 112, 96);  margin: 1.2em auto;  text-align: center;  //border-bottom: 1px solid #009688;  border-bottom: 1px solid rgb(239, 112, 96);&#125;/* 一级标题内容 */#nice h1 .content &#123;&#125;/* 一级标题修饰 请参考有实例的主题 */#nice h1:after &#123;&#125;/* 二级标题 */#nice h2 &#123;  //color: #009688;  color: rgb(239, 112, 96);  padding-left: 10px;  margin: 1em auto;  // border-left: 3px solid #009688;  border-left: 3px solid rgb(239, 112, 96);&#125;/* 二级标题内容 */#nice h2 .content &#123;&#125;/* 二级标题修饰 请参考有实例的主题 */#nice h2:after &#123;&#125;/* 三级标题 */#nice h3 &#123;  margin: 0.6em auto;  padding-left: 10px;  // border-left: 2px solid #009688;  border-left: 2px solid rgb(239, 112, 96)&#125;/* 三级标题内容 */#nice h3 .content &#123;&#125;/* 三级标题修饰 请参考有实例的主题 */#nice h3:after &#123;&#125;/* 四级标题 */#nice h4 &#123;  margin: 0.6em auto;  font-size: 1.2em;  padding-left: 10px;  // border-left: 2px dashed #009688;  border-left: 2px dashed rgb(239, 112, 96);&#125;/* 五级标题 */#nice h5 &#123;  margin: 0.6em auto;  font-size: 1.1em;  padding-left: 10px;  // border-left: 1px dashed #009688;  border-left: 1px dashed rgb(239, 112, 96);&#125;/* 六级标题 */#nice h6 &#123;  margin: 0.6em auto;  font-size: 1em;  padding-left: 10px;  // border-left: 1px dotted #009688;  border-left: 1px dotted rgb(239, 112, 96);&#125;/* 无序列表整体样式 * list-style-type: square|circle|disc; */#nice ul &#123;&#125;/* 有序列表整体样式 * list-style-type: upper-roman|lower-greek|lower-alpha; */#nice ol &#123;&#125;/* 列表内容，不要设置li */#nice li section &#123;&#125;/* 引用 * 左边缘颜色 border-left-color: black; * 背景色 background: gray; */#nice .multiquote-1 &#123;  //border-left: 2px solid #888;  //border-right: 2px solid #888;  border-left: 2px solid rgb(239, 112, 96);  border-right: 2px solid rgb(239, 112, 96);  padding-left: 1em;  color: #777;  background: rgb(255, 249, 249);&#125;/* 引用文字 */#nice .multiquote-1 p &#123;&#125;/* 链接  * border-bottom: 1px solid #009688; */#nice a &#123;//  color: #009688;  color: rgb(239, 112, 96);//  border-bottom: 1px solid #009688;  border-bottom: 1px solid rgb(239, 112, 96);&#125;/* 加粗 */#nice strong &#123;&#125;/* 斜体 */#nice em &#123;&#125;/* 加粗斜体 */#nice em strong &#123;&#125;/* 删除线 */#nice del &#123;&#125;/* 分隔线 * 粗细、样式和颜色 * border-top: 1px solid #3e3e3e; */#nice hr &#123;  margin: 20px 0;&#125;/* 图片 * 宽度 width: 80%; * 居中 margin: 0 auto; * 居左 margin: 0 0; */#nice img &#123;&#125;/* 图片描述文字 */#nice figcaption &#123;&#125;/* 行内代码 */#nice p code, #nice li code &#123;  // color: #009688;  color: rgb(239, 112, 96);&#125;/* 非微信代码块 * 代码块不换行 display: -webkit-box !important; * 代码块换行 display: block; */#nice pre code &#123;&#125;/* 表格内的单元格 * 字体大小 font-size: 16px; * 边框 border: 1px solid #ccc; * 内边距 padding: 5px 10px; */#nice table tr th &#123;//  border: 1px solid #009688;  border: 1px solid rgb(239, 112, 96);//  background-color: #009688;  background-color: rgb(239, 112, 96);  color: #f8f8f8;  border-bottom: 0;&#125;#nice table tr td &#123;  // border: 1px solid #009688;  border: 1px solid rgb(239, 112, 96);&#125;#nice table tr:nth-child(2n) &#123;  background-color: #f8f8f8;&#125;/* 脚注文字 */#nice .footnote-word &#123;  // color: #009688;  color: rgb(239, 112, 96);&#125;/* 脚注上标 */#nice .footnote-ref &#123;  //  color: #009688;  color: rgb(239, 112, 96);&#125;/* &quot;参考资料&quot;四个字  * 内容 content: &quot;参考资料&quot;; */#nice .footnotes-sep:before &#123;&#125;/* 参考资料编号 */#nice .footnote-num &#123;&#125;/* 参考资料文字 */#nice .footnote-item p &#123; &#125;/* 参考资料解释 */#nice .footnote-item p em &#123;&#125;/* 行间公式 * 最大宽度 max-width: 300% !important; */#nice .block-equation svg &#123;&#125;/* 行内公式 */#nice .inline-equation svg &#123;  &#125;\n","tags":["css"]},{"title":"Python中使用requests库发送Http请求","url":"/2021/03/Python%E4%B8%AD%E4%BD%BF%E7%94%A8requests%E5%BA%93%E5%8F%91%E9%80%81Http%E8%AF%B7%E6%B1%82/","content":"最近在使用一个性能测试框架，locust，是用Python写的。模拟测试逻辑时，可用Python来编写需要进行测试的逻辑。\n这篇文章讲讲如何使用 requests 这个库，以发送http请求。\nGet请求url = &#x27;http://localhost/api&#x27;payload = &#123;&#x27;id&#x27;: 100&#125;headers = &#123;&#x27;token&#x27;: &#x27;xxxxxxx&#x27;&#125;with self.client.get(url, params=payload, headers=headers) as response:    if response.status_code == 200:        if response.text != &#x27;&#x27;:            res_dict = json.loads(response.text)            ...\n\nPost请求当我们使用json方式提交请求时，其实是不用在代码里面指定Content-type: applicaion/json的，只需要使用json参数就可以了。\nurl = &#x27;http://localhost/api&#x27;payload = &#123;&#x27;id&#x27;: 100&#125;headers = &#123;&#x27;token&#x27;: &#x27;xxxxxxx&#x27;&#125;with self.client.post(url, json=payload, headers=headers) as response:    if response.status_code == 200:        if response.text != &#x27;&#x27;:            res_dict = json.loads(response.text)            ...\n\n上传文件有点特殊，也不需要指定Content-type，使用files参数即可。java服务端，使用MultipartFile来接收这个文件即可。\nurl = &#x27;http://localhost/api&#x27;file = &#123;&#x27;file&#x27;: open(&#x27;/opt/test.txt&#x27;, &#x27;rb&#x27;)&#125;headers = &#123;&#x27;token&#x27;: &#x27;xxxxxxx&#x27;&#125;with self.client.post(url, files=file, headers=headers) as response:    if response.status_code == 200:        if response.text != &#x27;&#x27;:            res_dict = json.loads(response.text)            ...","categories":["Python"],"tags":["requests","locust"]},{"title":"IDEA自动生成对象赋值的所有set方法","url":"/2021/06/IDEA%E8%87%AA%E5%8A%A8%E7%94%9F%E6%88%90%E5%AF%B9%E8%B1%A1%E8%B5%8B%E5%80%BC%E7%9A%84%E6%89%80%E6%9C%89set%E6%96%B9%E6%B3%95/","content":"一个对象属性很多，要设置其属性的时候很麻烦，不小心还会漏掉。\nIDEA可以使用一款插件，GenerateAllSetter，可以一键生成所有属性的set方法。\n\n\n安装好插件以后，在语句 User user = new User() 中的 user 上使用快捷键 Alt + Enter，就会出现自动生成set方法的选项了。\n","categories":["Tools","IntelliJ IDEA"]},{"title":"locust性能测试框架","url":"/2021/03/locust%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95%E6%A1%86%E6%9E%B6/","content":"locust是一款性能测试框架，比JMeter好的地方在于能产生更多的请求，因为locust是基于协程的，而JMeter是基于线程的。\n安装pip install locust\n\n安装成功后查看版本信息：\nlocust -Vlocust 1.4.3\n\n编写代码from locust import HttpUser, TaskSet, task, between, tag# 模拟n个用户，其实就是创建了n个MyTask的实例class MyTask(TaskSet):        token = &#x27;&#x27;        def on_start(self):        self.login_or_register()            def login_or_register(self):        url = &#x27;http://api&#x27;        body = &#123;&#x27;id&#x27;: 1&#125;        headers = &#123;&#x27;token&#x27;: &#x27;xxxx&#x27;&#125;        with self.client.post(url, json=body, headers=headers, name=&#x27;login_or_register&#x27;) as response:            if response.text != &#x27;&#x27;:                res_dict = json.loads(response.text)                if res_dict[&#x27;code&#x27;] == 0:                    pass        @task    @tag(&#x27;view_page&#x27;)                    def view_page(self):        pass                                    # 这个类是一个配置类, 只被初始化一次, 在这里写一些配置相关信息# 要访问哪些id, 要生成哪些用户的token, 可以在这里生成class MyUser(HttpUser):    # 测试的api的host信息    host = &#x27;&#x27;    # 每次访问之间间隔的时间    wait_time = between(1, 2)\n\n运行执行命令\nlocust -f file.py\n\n如果只执行带有tag的业务代码\nlocust -f file.py --tags view_page\n\n如果要进行分布式测试，首先启动master，master是管理节点，在这个节点上面查看统计数据。\nlocust -f file.py --tags view_page --master\n\n再开两个worker节点, 如果主节点和从节点不在一台机器上，那需要指定master-host参数。\nlocust -f file.py --tags view_page --worker --master-host=192.168.1.2\n\n当worker节点启动后，master的console里面会显示：\n[2021-03-05 18:42:39,333] /INFO/locust.runners: Client &#x27;1140b3a3ec57404194845264ee8dae1f&#x27; reported as ready. Currently 1 clients ready to swarm.[2021-03-05 18:45:13,222] /INFO/locust.runners: Client &#x27;8f1e5206c38f4f99a55641f1cf0c44d1&#x27; reported as ready. Currently 2 clients ready to swarm.\n\n打开压测页面，http://localhost:8089/\n\n","categories":["Python"],"tags":["locust"]},{"title":"Python matplotlib.rcParams常用设置","url":"/2021/07/Python-matplotlib-rcParams%E5%B8%B8%E7%94%A8%E8%AE%BE%E7%BD%AE/","content":"matplotlib是Python的画图工具。\n可以通过对 matplotlib.rcParams 字典做一些常用的设置，例如：\nimport matplotlib.pyplot as plt# 中文支持，不会显示小方框plt.rcParams[&#x27;font.sans-serif&#x27;] = [&#x27;SimHei&#x27;]# 正常显示负号plt.rcParams[&#x27;axes.unicode_minus&#x27;] = False# 设置线条宽度plt.rcParams[&#x27;lines.linewidth&#x27;] = 5# 设置线条颜色plt.rcParams[&#x27;lines.color&#x27;] = &#x27;red&#x27;# 设置线条样式，样式的各类还有 `--`为虚线，`-.`为点虚线plt.rcParams[&#x27;lines.linestyle&#x27;] = &#x27;-&#x27; # 直线\n\n其实在新的版本中，中文显示和负号显示已经默认显示正常了。\n","categories":["Python"],"tags":["matplotlib"]},{"title":"Ubuntu安装vnpy","url":"/2021/07/Ubuntu%E5%AE%89%E8%A3%85vnpy/","content":"下载安装包。注意，2.3.0和2.4.0在Ubuntu上安装不成功，2.2.0可以正常安装。\n$ sudo wget https://github.com/vnpy/vnpy/archive/refs/tags/2.2.0.zip$ sudo unzip 2.2.0.zip\n\n安装vnpy\n/usr/local/vnpy-2.2.0$ sudo bash install.sh\n\n如果我们安装的anaconda是3.8版本的，然后又想用python3.7，可以通过如下方法解决。\n$ sudo conda search --full --name python$ sudo conda install python=3.7.6\n\n安装过程中，如果遇到如下错误，那就重新安装一下Anaconda，我是用的miniconda。\nImportError: cannot import name &#x27;InvalidSchemeCombination&#x27; from &#x27;pip._internal.exceptions&#x27; (/usr/local/miniconda3/envs/py37/lib/python3.8/site-packages/pip/_internal/exceptions.py)\n","tags":["vnpy"]},{"title":"sudo command not found问题解决","url":"/2021/05/sudo-command-not-found%E9%97%AE%E9%A2%98%E8%A7%A3%E5%86%B3/","content":"在 Ubuntu 系统上面安装了 Miniconda，想克隆一个 python 的运行环境，却出现下面错误。\n$ sudo conda create --name py38 --clone basesudo: conda: command not found\n\n其实这个问题在之前的文章有讲到过，解决sudo docker报错command not found\n今天有个更好的方法去解决，就是设置环境变量。\n$ vim ~/.bashrc\n\n在文件最后面添加如下语句：\nalias sudo=&#x27;sudo env PATH=$PATH&#x27;\n\n保存退出，执行命令使其生效。\n$ source ~/.bashrc\n\n创建环境解决，然后还安装了 jupyter notebook。\nsudo conda install jupyter\n","tags":["ubuntu","sudo"]},{"title":"上期所交易仿真平台环境配置","url":"/2021/07/%E4%B8%8A%E6%9C%9F%E6%89%80%E4%BA%A4%E6%98%93%E4%BB%BF%E7%9C%9F%E5%B9%B3%E5%8F%B0%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/","content":"环境介绍\n用户名：1***81\nBrokerID统一为：9999\n产品名称：即APPID，默认的APPID为simnow_client_test\n授权编码：认证码为0000000000000000（16个0）\n第一套（支持上期所期权）：第一组：Trade Front：180.168.146.187:10201，Market Front：180.168.146.187:10211；【电信】（看穿式前置，使用监控中心生产秘钥）\n第二组：Trade Front：180.168.146.187:10202，Market Front：180.168.146.187:10212；【电信】（看穿式前置，使用监控中心生产秘钥）\n第三组：Trade Front：218.202.237.33:10203，Market Front：218.202.237.33:10213；【移动】（看穿式前置，使用监控中心生产秘钥）\n交易品种：五所所有期货品种以及上期所所有期权品种。\n账户资金：初始资金两千万，支持入金，每日最多三次。\n交易阶段(服务时间)：与实际生产环境保持一致。\n客户端软件下载：点击下载客户端。\n第二套：交易前置：180.168.146.187:10130，行情前置：180.168.146.187:10131；【7x24】（看穿式前置，使用监控中心生产秘钥）\n第二套环境仅服务于CTP API开发爱好者，仅为用户提供CTP API测试需求，不提供结算等其它服务。\n新注册用户，需要等到第三个交易日才能使用第二套环境。\n账户、钱、仓跟第一套环境上一个交易日保持一致。\n交易阶段(服务时间)：交易日，16：00～次日09：00；非交易日，16：00～次日15：00。\n用户通过SimNow的账户（上一个交易日之前注册的账户都有效）接入环境，建议通过商业终端进行模拟交易的用户使用第一套环境。\n成交规则\n1、期货交易按照交易所公布的买一卖一价对价成交；\n2、买入时：如果委托价大于等于卖一价，则成交，成交价为委托价、卖一价、最新价三价取中，如果委托价小于卖一价，不能成交，等待更优的行情才能成交；\n3、卖出时：如果委托价小于等于买一价，则成交，成交价为委托价、买一价、最新价三价取中，如果委托价大于买一价，不能成交，等待更优的行情才能成交。\n非交易时间第一套，第一组：Trade Front：180.168.146.187:10201，Market Front：180.168.146.187:10211；【电信】，亲测可用。\n第二套，交易前置：180.168.146.187:10130，行情前置：180.168.146.187:10131，亲测可用。\n"},{"title":"miniconda安装jupyter ipython","url":"/2021/05/miniconda%E5%AE%89%E8%A3%85jupyter-ipython/","content":"通过命令安装ipython。\nconda install ipython jupyter\n\n通过命令进入ipython研究环境：\nipython\n\n测试\nIn [1]: from math import sqrtIn [2]: sqrt(4)Out[2]: 2.0In [3]: exit","tags":["python"]},{"title":"CentOS 7安装Kafka","url":"/2021/08/CentOS-7%E5%AE%89%E8%A3%85Kafka/","content":"环境：CentOS 7Kafka版本：kafka_2.12-2.6.2\n下载Kafka安装文件$ sudo wget https://mirror.nodesdirect.com/apache/kafka/2.6.2/kafka_2.12-2.6.2.tgz\n\n解压安装$ sudo tar -xzf kafka_2.12-2.6.2.tgz -C /usr/local/kafka\n\n启动Zookeeper$ sudo bin/zookeeper-server-start.sh config/zookeeper.properties\n\n启动Kafka修改server.properties配置文件。\n如果要使用外部程序去调用Kafka的话，需要设置advertised.listeners参数的IP地址。advertised.listeners&#x3D;PLAINTEXT:&#x2F;&#x2F;192.168.0.2:9092\n$ sudo bin/kafka-server-start.sh config/server.properties\n\n测试创建Topic\n$ bin/kafka-topics.sh --create --topic quickstart-events --bootstrap-server localhost:9092$ bin/kafka-topics.sh --describe --topic quickstart-events --bootstrap-server localhost:9092Topic:quickstart-events  PartitionCount:1    ReplicationFactor:1 Configs:    Topic: quickstart-events Partition: 0    Leader: 0   Replicas: 0 Isr: 0S\n\n写消息\n$ bin/kafka-console-producer.sh --topic quickstart-events --bootstrap-server localhost:9092This is my first eventThis is my second event\n\n读消息\n$ bin/kafka-console-consumer.sh --topic quickstart-events --from-beginning --bootstrap-server localhost:9092This is my first eventThis is my second event\n","categories":["Middleware","Kafka"],"tags":["kafka"]},{"title":"SpringBoot中的同步和异步调用","url":"/2021/08/SpringBoot%E4%B8%AD%E7%9A%84%E5%90%8C%E6%AD%A5%E5%92%8C%E5%BC%82%E6%AD%A5%E8%B0%83%E7%94%A8/","content":"先放两张图像体会一下。\n\n\n\n","categories":["Java","Spring Boot"],"tags":["async","springboot"]},{"title":"mybatis-plus在Console控制台里面打印sql查询语句","url":"/2021/08/mybatis-plus%E5%9C%A8Console%E6%8E%A7%E5%88%B6%E5%8F%B0%E9%87%8C%E9%9D%A2%E6%89%93%E5%8D%B0sql%E6%9F%A5%E8%AF%A2%E8%AF%AD%E5%8F%A5/","content":"使用Springboot集成了 mybatis-plus，在开发时，往往希望在控制台里面能看到 sql 语句，甚至查询结果，以便于高效开发。\n在 Springboot 的yml文件里面加入如下内容：\nmybatis-plus:  configuration:     # 是否将sql打印到控制面板(该配置会将sql语句和查询的结果都打印到控制台)    log-impl: org.apache.ibatis.logging.stdout.StdOutImpl\n","categories":["Database","MySQL"],"tags":["mybatis-plus"]},{"title":"Warning: Generating equals/hashCode implementation but without a call to superclass","url":"/2021/08/Warning-Generating-equals-hashCode-implementation-but-without-a-call-to-superclass/","content":"编译项目时，遇到这样一个错误提示。\nWarning:(12, 1) Generating equals/hashCode implementation but without a call to superclass, even though this class does not extend java.lang.Object. If this is intentional, add &#x27;(callSuper=false)&#x27; to your type.\n\n这是由于我们使用lombok的@Data注解时造成的。使用@Data注解时，子类的equals和hashCode方法，默认是不比较父类的属性的。\npublic int hashCode() &#123;    int PRIME = true;    int result = super.hashCode(); // 默认这句话是没有的    Object $ext = this.getExt();    result = result * 59 + ($ext == null ? 43 : $ext.hashCode());    return result;&#125;\n\n解决方案有两个。\n在类上再添加一个注解：@EqualsAndHashCode(callSuper = true)\n这样有个问题就是，只要继承了其他类，都要加这个注解。\n还有一个通用的解决方法，在src/main/java文件夹下面，注意，不是src/main/resources下面，添加配置文件lombok.config\n文件里面的内容为：\nlombok.equalsAndHashCode.callSuper=call","categories":["Java"],"tags":["lombok"]},{"title":"解决jps command not found","url":"/2021/08/%E8%A7%A3%E5%86%B3jps-command-not-found/","content":"在 CentOS 7 ssh 下执行 jps 命令，返回 command not found。\n$ which java$ ll -l /usr/bin/javalrwxrwxrwx. 1 root root 22 Oct 12  2020 /usr/bin/java -&gt; /etc/alternatives/java$ ll -l /etc/alternatives/javalrwxrwxrwx. 1 root root 73 Oct 12  2020 /etc/alternatives/java -&gt; /usr/lib/jvm/java-1.8.0-openjdk-1.8.0.262.b10-0.el7_8.x86_64/jre/bin/java \n\n这个版本的java-1.8.0-openjdk-1.8.0.262.b10-0.el7_8.x86_64，是通过yum安装的openjdk。需要换成我们自己下载的jdk。\n$ sudo rm -rf /usr/bin/java$ sudo ln -s /usr/local/java/jdk1.8.0_231/bin/java /usr/bin/java\n\n添加环境变量，修改~/.bash_profile\nexport PATH=$PATH:/usr/local/java/jdk1.8.0_231/bin/\n\n激活\n$ source ~/.bash_profile\n\n此时再执行 jps 命令，就可以执行成功了。\n$ jps27455 Jps","categories":["Java"]},{"title":"如何获取Kafka的版本号","url":"/2021/08/%E5%A6%82%E4%BD%95%E8%8E%B7%E5%8F%96Kafka%E7%9A%84%E7%89%88%E6%9C%AC%E5%8F%B7/","content":"一般执行命令时带个-version参数，就能看到程序的版本号，但Kafka不行。\n如何查看呢，先看到kafka的进程id。\n# ps -ef | grep kafka\n\n或者\n# jps29190 Kafka29675 Jps28734 QuorumPeerMain\n\n然后看查此进程的启动位置。\n# pwdx 2919029190: /usr/local/kafka/kafka_2.12-2.6.2\n\n就能查看到Kafka的版本了，正常人在安装时，都不会修改这个路径的。\n","categories":["Middleware","Kafka"]},{"title":"关于基金的几个指标","url":"/2021/08/%E5%85%B3%E4%BA%8E%E5%9F%BA%E9%87%91%E7%9A%84%E5%87%A0%E4%B8%AA%E6%8C%87%E6%A0%87/","content":"基金规模、净值基金有A类和C类，分析数据时有时候会迷惑。\n以下是通过API获取到的数据\n  | SEC_NAME | UNIT_TOTAL | UNIT_FUNDSHARE_TOTAL | FUND_FUNDSCALE | PRT_NETASSET | PRT_FUNDNETASSET_TOTAL | PRT_STOCKVALUE– | —– | —– | —– | —– | —- | —- | —920002.OF | 中金精选A | 2.118298e+08 | 22232.232524 | 4.696938e+08 | 4.696938e+08 | 4.928548e+08 | 4.407136e+08920922.OF | 中金精选C | 1.049249e+07\t| NaN | 2.316105e+07 | 2.316105e+07 | 4.928548e+08 | 4.407136e+08\n通过这个数据，我们就能很清晰的知道那些变量代表什么意思了。\nUNIT_TOTAL 基金份额，单位为份UNIT_FUNDSHARE_TOTAL 基金份额（合计），比如A类和C类的份额数之和，一般合计到A类上面，C类上为0，单位为份FUND_FUNDSCALE 基金规模PRT_NETASSET 基金资产净值，这个值和基金规模是一个意思。PRT_FUNDNETASSET_TOTAL 基金资产净值（合计）PRT_STOCKVALUE 股票市值，这个值A类和C类相同\n资产配置里面的股票占比公式：PRT_STOCKVALUE &#x2F; PRT_FUNDNETASSET_TOTAL\n所以在统计基金时，为了不统计重复数值，需要把C类基金去掉，也就是UNIT_FUNDSHARE_TOTAL为NaN的行，同时，用合计值来进行计算。\n分行业市值分行业市值，用申万，中信行业分类是不行的，只能用证监会行业。\n[&#x27;农、林、牧、渔业&#x27;, &#x27;采矿业&#x27;, &#x27;制造业&#x27;, &#x27;电力、热力、燃气及水生产和供应业&#x27;, &#x27;建筑业&#x27;, &#x27;批发和零售业&#x27;,                 &#x27;交通运输、仓储和邮政业&#x27;, &#x27;住宿和餐饮业&#x27;, &#x27;信息传输、软件和信息技术服务业&#x27;, &#x27;金融业&#x27;, &#x27;房地产业&#x27;,                 &#x27;租赁和商务服务业&#x27;, &#x27;科学研究和技术服务业&#x27;, &#x27;水利、环境和公共设施管理业&#x27;, &#x27;居民服务、修理和其他服务业&#x27;,                 &#x27;教育&#x27;, &#x27;卫生和社会工作&#x27;, &#x27;文化、体育和娱乐业&#x27;, &#x27;综合&#x27;]\n"},{"title":"IDEA增加匹配查找记录数","url":"/2021/09/IDEA%E5%A2%9E%E5%8A%A0%E5%8C%B9%E9%85%8D%E6%9F%A5%E6%89%BE%E8%AE%B0%E5%BD%95%E6%95%B0/","content":"我们使用 IDEA 在项目中查找某个字符串时，搜索结果的数量如果超过某个阈值，那就会显示不出来。\n如图所示，100+ matches in 100+ files。\n\n\n可以对 IDEA 进行设置，以显示更多数量的结果集。\nHelp -&gt; Find Action\n输入Registry\n\n\n找到参数 ide.usages.page.size，把结果设置为你想要的值。\n","categories":["Tools","IntelliJ IDEA"]},{"title":"mybatis xml配置文件的写法","url":"/2021/09/mybatis-xml%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6%E7%9A%84%E5%86%99%E6%B3%95/","content":"在去 xml 的时代，使用 mybatis 的时候写 xml 我觉得并不是一个好的实践。不过在维护一些老的项目中，还是在使用xml配置文件，所以基本的配置还是得会。\n创建设置 useGeneratedKeys 为 true 时，当插入记录后，自增的 id 的值会赋值到 UserPo 的 id 属性里面。\n&lt;insert id=&quot;create&quot;        useGeneratedKeys=&quot;true&quot;        keyProperty=&quot;id&quot;        parameterType=&quot;UserPo&quot;&gt;    INSERT INTO user    &lt;trim prefix=&quot;(&quot; suffix=&quot;)&quot; suffixOverrides=&quot;,&quot;&gt;        &lt;if test=&quot;username != null&quot;&gt;            username,        &lt;/if&gt;        &lt;if test=&quot;age != null&quot;&gt;            age,        &lt;/if&gt;    &lt;/trim&gt;    &lt;trim prefix=&quot;VALUES (&quot; suffix=&quot;)&quot; suffixOverrides=&quot;,&quot;&gt;        &lt;if test=&quot;username != null&quot;&gt;            #&#123;username&#125;,        &lt;/if&gt;        &lt;if test=&quot;age != null&quot;&gt;            #&#123;age&#125;,        &lt;/if&gt;    &lt;/trim&gt;&lt;/insert&gt;\n\n\n更新如果 id 没有值，就不用更新。\n如果 username 和 age 都为空时，SQL语句会报错。如果你要更新，至少有一个字段一定是有值的。\n注意，SET key1 = val1, key2 = val2，字段之间是由逗号分隔的，而不是AND，在生产上执行这条语句时得非常小心。\n&lt;update id=&quot;update&quot; parameterType=&quot;UserPo&quot;&gt;    &lt;if test=&quot;id != null&quot;&gt;        UPDATE user        &lt;set&gt;            &lt;if test=&quot;username != null&quot;&gt;                username = #&#123;username&#125;,            &lt;/if&gt;            &lt;if test=&quot;age != null&quot;&gt;                age = #&#123;age&#125;,            &lt;/if&gt;        &lt;/set&gt;        WHERE id = #&#123;id&#125;    &lt;/if&gt;&lt;/update&gt;\n\n查询WHERE 字句。\n每句话前面加关键字 AND。\n&lt;where&gt;    &lt;if test=&quot;username != null&quot;&gt;        AND username = #&#123;username&#125;    &lt;/if&gt;    AND age = 18&lt;/where&gt;\n\n运算假如我要在xml配置里面进行运算，比如把 age 加 1 ，写法如下：\n$&#123;age + 1&#125;\n","categories":["Java","Mybatis"],"tags":["mysql","mybatis"]},{"title":"java图片压缩处理","url":"/2021/09/java%E5%9B%BE%E7%89%87%E5%8E%8B%E7%BC%A9%E5%A4%84%E7%90%86/","content":"获取图片的像素BufferedImage bufferedImage = ImageIO.read(file);int width = bufferedImage.getWidth();int height = bufferedImage.getHeight();\n\n压缩图片使用一个开源的图片处理库 Thumbnailator，有一直在维护的。\n&lt;dependency&gt;    &lt;groupId&gt;net.coobird&lt;/groupId&gt;    &lt;artifactId&gt;thumbnailator&lt;/artifactId&gt;    &lt;version&gt;0.4.14&lt;/version&gt;&lt;/dependency&gt;\n\n压缩时，按比例缩放，必须同时满足最大不超过size方法里面的width和height参数值。\n直接转为文件输出：\nString inputImage = &quot;input.jpeg&quot;;String outputImage = &quot;out.jpeg&quot;;Thumbnails.of(inputImage)        .size(100, 60)        .toFile(outputImage);\n\n也可以转为输出流，再转为输入流供其他使用：\nString inputImage = &quot;input.jpeg&quot;;String outputImage = &quot;out.jpeg&quot;;ByteArrayOutputStream baos = new ByteArrayOutputStream();Thumbnails.of(inputImage)        .size(100, 60)        .toOutputStream(baos);ByteArrayInputStream bais = new ByteArrayInputStream(baos.toByteArray());\n\n注意关闭文件。\n","categories":["Java"],"tags":["thumbnailator"]},{"title":"Vmware安装Windows虚拟机","url":"/2023/10/Vmware%E5%AE%89%E8%A3%85Windows%E8%99%9A%E6%8B%9F%E6%9C%BA/","content":"很少在Vmware上安装Windows系统，不过还是会遇到一些特殊情况，只能在Windows操作系统上使用某些软件。\n在网上下载了一些安装光盘iso镜像，但却不能正常安装。这些iso文件，都是不能正常引动安装启动的。\n后来在 https://next.itellyou.cn/ 这个网站上找到系统安装的iso镜像文件，最后成功安装。\n如果设置系统激活的问题，可以从链接 https://dnmjun.lanzoui.com/b0e7m2fof 下载 HEU_KMS_Activator_v41.1.0.rar 激活工具。\n在使用前需要把杀毒程序关闭，不然系统会把这个工具做为病毒而删除。\n","categories":["OS","Windows"],"tags":["vmware","windows"]},{"title":"美国国债命名方式","url":"/2023/10/%E7%BE%8E%E5%9B%BD%E5%9B%BD%E5%80%BA%E5%91%BD%E5%90%8D%E6%96%B9%E5%BC%8F/","content":"美国国债 US Treasury 的命名方式为：名称+票息+到期日\n如 UST 3 3/4 4/25 的意思是：\nUST 3 3&#x2F;4 4&#x2F;25中的3 3&#x2F;4表示票息，是3+3&#x2F;4即是3.75%的票息，4&#x2F;25表示到期的年和月，是2025年4月到期。\n","categories":["债券"],"tags":["美债"]},{"title":"Elasticsearch关联关系数据建模（二）嵌套对象","url":"/2021/12/Elasticsearch%E5%85%B3%E8%81%94%E5%85%B3%E7%B3%BB%E6%95%B0%E6%8D%AE%E5%BB%BA%E6%A8%A1%EF%BC%88%E4%BA%8C%EF%BC%89%E5%B5%8C%E5%A5%97%E5%AF%B9%E8%B1%A1/","content":"案例：一篇博客对应多个评论。\n字段类型由object改为nested即可。\nPUT /my_index&#123;  &quot;mappings&quot;: &#123;    &quot;blogpost&quot;: &#123;      &quot;properties&quot;: &#123;        &quot;comments&quot;: &#123;          &quot;type&quot;: &quot;nested&quot;,           &quot;properties&quot;: &#123;            &quot;name&quot;:    &#123; &quot;type&quot;: &quot;string&quot;  &#125;,            &quot;comment&quot;: &#123; &quot;type&quot;: &quot;string&quot;  &#125;,            &quot;age&quot;:     &#123; &quot;type&quot;: &quot;short&quot;   &#125;,            &quot;stars&quot;:   &#123; &quot;type&quot;: &quot;short&quot;   &#125;,            &quot;date&quot;:    &#123; &quot;type&quot;: &quot;date&quot;    &#125;          &#125;        &#125;      &#125;    &#125;  &#125;&#125;\n\n嵌套对象的查询嵌套对象被索引在独立隐藏的文档中，所以必须使用nested查询去获取数据。\nGET /my_index/blogpost/_search&#123;  &quot;query&quot;: &#123;    &quot;bool&quot;: &#123;      &quot;must&quot;: [        &#123;          &quot;match&quot;: &#123;            &quot;title&quot;: &quot;eggs&quot;           &#125;        &#125;,        &#123;          &quot;nested&quot;: &#123;            &quot;path&quot;: &quot;comments&quot;,             &quot;query&quot;: &#123;              &quot;bool&quot;: &#123;                &quot;must&quot;: [                   &#123;                    &quot;match&quot;: &#123;                      &quot;comments.name&quot;: &quot;john&quot;                    &#125;                  &#125;,                  &#123;                    &quot;match&quot;: &#123;                      &quot;comments.age&quot;: 28                    &#125;                  &#125;                ]              &#125;            &#125;          &#125;        &#125;      ]    &#125;  &#125;&#125;\n\nnested子句作用于嵌套字段comments。comments.name和comments.age子句操作在同一个嵌套文档中。\nnested查询还可以多层嵌套。\n默认情况下，根文档的分数是这些嵌套文档分数的平均值。可以通过设置 score_mode 参数来控制这个得分策略，相关策略有 avg (平均值), max (最大值), sum (加和) 和 none (直接返回 1.0 常数值分数)。\nGET /my_index/blogpost/_search&#123;  &quot;query&quot;: &#123;    &quot;bool&quot;: &#123;      &quot;must&quot;: [        &#123;          &quot;match&quot;: &#123;            &quot;title&quot;: &quot;eggs&quot;          &#125;        &#125;,        &#123;          &quot;nested&quot;: &#123;            &quot;path&quot;: &quot;comments&quot;,            &quot;score_mode&quot;: &quot;max&quot;,             &quot;query&quot;: &#123;              &quot;bool&quot;: &#123;                &quot;must&quot;: [                  &#123;                    &quot;match&quot;: &#123;                      &quot;comments.name&quot;: &quot;john&quot;                    &#125;                  &#125;,                  &#123;                    &quot;match&quot;: &#123;                      &quot;comments.age&quot;: 28                    &#125;                  &#125;                ]              &#125;            &#125;          &#125;        &#125;      ]    &#125;  &#125;&#125;\n\n如果nested查询放在filter子句中，则score_mode参数不再生效，因为filter不是打分查询。\n使用嵌套字段排序PUT /my_index/blogpost/2&#123;  &quot;title&quot;: &quot;Investment secrets&quot;,  &quot;body&quot;:  &quot;What they don&#x27;t tell you ...&quot;,  &quot;tags&quot;:  [ &quot;shares&quot;, &quot;equities&quot; ],  &quot;comments&quot;: [    &#123;      &quot;name&quot;:    &quot;Mary Brown&quot;,      &quot;comment&quot;: &quot;Lies, lies, lies&quot;,      &quot;age&quot;:     42,      &quot;stars&quot;:   1,      &quot;date&quot;:    &quot;2014-10-18&quot;    &#125;,    &#123;      &quot;name&quot;:    &quot;John Smith&quot;,      &quot;comment&quot;: &quot;You&#x27;re making it up!&quot;,      &quot;age&quot;:     28,      &quot;stars&quot;:   2,      &quot;date&quot;:    &quot;2014-10-16&quot;    &#125;  ]&#125;\n\n查询某个时间范围内，有评论的文章，并按stars升序排序。\nGET /_search&#123;  &quot;query&quot;: &#123;    &quot;nested&quot;: &#123;       &quot;path&quot;: &quot;comments&quot;,      &quot;filter&quot;: &#123;        &quot;range&quot;: &#123;          &quot;comments.date&quot;: &#123;            &quot;gte&quot;: &quot;2014-10-01&quot;,            &quot;lt&quot;:  &quot;2014-11-01&quot;          &#125;        &#125;      &#125;    &#125;  &#125;,  &quot;sort&quot;: &#123;    &quot;comments.stars&quot;: &#123;       &quot;order&quot;: &quot;asc&quot;,         &quot;mode&quot;:  &quot;min&quot;,         &quot;nested_path&quot;: &quot;comments&quot;,       &quot;nested_filter&quot;: &#123;        &quot;range&quot;: &#123;          &quot;comments.date&quot;: &#123;            &quot;gte&quot;: &quot;2014-10-01&quot;,            &quot;lt&quot;:  &quot;2014-11-01&quot;          &#125;        &#125;      &#125;    &#125;  &#125;&#125;\n\nsort排序子句中的nested_path和nested_filter的查询条件和上面的path、filter重复。原因在于，排序发生在查询执行之后。我们是按某一时间范围的stars数排序，而不是按所有时间的stars排序。比如blog1的在10月stars:1, blog2在10月stars:2，但blog1的所有stars：20，blog2的所有stars：10。他们最后的结果是不一样的。\n嵌套聚合https://www.elastic.co/guide/cn/elasticsearch/guide/current/nested-aggregation.html\n","categories":["大数据","Elasticsearch"],"tags":["elasticsearch"]},{"title":"Elasticsearch关联关系数据建模（三）父子关系文档","url":"/2021/12/Elasticsearch%E5%85%B3%E8%81%94%E5%85%B3%E7%B3%BB%E6%95%B0%E6%8D%AE%E5%BB%BA%E6%A8%A1%EF%BC%88%E4%B8%89%EF%BC%89%E7%88%B6%E5%AD%90%E5%85%B3%E7%B3%BB%E6%96%87%E6%A1%A3/","content":"parent-child关系，类似于nested模型，区别在于，nested objects文档中，所有对象都在同一个文档中，而parent-child关系文档中，父对象和子对象都是完全独立的文档。\n使用场景：子文档数量较多，并且子文档创建和修改的频率高时。\n与nested objects相比，parent-child关系的优势有：\n更新父文档时，不会重新索引子文档。创建，修改或删除子文档时，不会影响父文档或其他子文档。子文档可以作为搜索结果独立返回。\n对父-子文档关系有个限制条件：父文档和其所有子文档，都必须要存储在同一个分片中。\nParent-Child关系文档映射建立父-子文档映射关系时只需要指定某一个文档 type 是另一个文档 type 的父亲。 该关系可以在如下两个时间点设置：1）创建索引时；2）在子文档 type 创建之前更新父文档的 mapping。\n举例说明，有一个公司在多个城市有分公司，并且每一个分公司下面都有很多员工。\n在创建员工 employee 文档 type 时，指定分公司 branch 的文档 type 为其父亲。\nPUT /company&#123;  &quot;mappings&quot;: &#123;    &quot;branch&quot;: &#123;&#125;,    &quot;employee&quot;: &#123;      &quot;_parent&quot;: &#123;        &quot;type&quot;: &quot;branch&quot;       &#125;    &#125;  &#125;&#125;\n\nemployee 文档 是 branch 文档的子文档。\n构建Parent-Child文档索引为父文档创建索引与为普通文档创建索引没有区别。父文档并不需要知道它有哪些子文档。\nPOST /company/branch/_bulk&#123; &quot;index&quot;: &#123; &quot;_id&quot;: &quot;london&quot; &#125;&#125;&#123; &quot;name&quot;: &quot;London Westminster&quot;, &quot;city&quot;: &quot;London&quot;, &quot;country&quot;: &quot;UK&quot; &#125;&#123; &quot;index&quot;: &#123; &quot;_id&quot;: &quot;liverpool&quot; &#125;&#125;&#123; &quot;name&quot;: &quot;Liverpool Central&quot;, &quot;city&quot;: &quot;Liverpool&quot;, &quot;country&quot;: &quot;UK&quot; &#125;&#123; &quot;index&quot;: &#123; &quot;_id&quot;: &quot;paris&quot; &#125;&#125;&#123; &quot;name&quot;: &quot;Champs Élysées&quot;, &quot;city&quot;: &quot;Paris&quot;, &quot;country&quot;: &quot;France&quot; &#125;\n\n创建子文档时，用户必须要通过 parent 参数来指定该子文档的父文档 ID：\nPUT /company/employee/1?parent=london &#123;  &quot;name&quot;:  &quot;Alice Smith&quot;,  &quot;dob&quot;:   &quot;1970-10-24&quot;,  &quot;hobby&quot;: &quot;hiking&quot;&#125;\n\n当前 employee 文档的父文档 ID 是 london 。如此保证了父文档和子文档都在同一个分片上。\n分片路由的计算公式如下：\nshard &#x3D; hash(routing) % number_of_primary_shards\n如果指定了父文档的 ID，那么就会使用父文档的 ID 进行路由，而不会使用当前文档 _id。也就是说，如果父文档和子文档都使用相同的值进行路由，那么父文档和子文档都会确定分布在同一个分片上。\n在执行单文档的请求时需要指定父文档的 ID，单文档请求包括：通过 GET 请求获取一个子文档；创建、更新或删除一个子文档。而执行搜索请求时是不需要指定父文档的ID，这是因为搜索请求是向一个索引中的所有分片发起请求，而单文档的操作是只会向存储该文档的分片发送请求。因此，如果操作单个子文档时不指定父文档的 ID，那么很有可能会把请求发送到错误的分片上。\nPOST /company/employee/_bulk&#123; &quot;index&quot;: &#123; &quot;_id&quot;: 2, &quot;parent&quot;: &quot;london&quot; &#125;&#125;&#123; &quot;name&quot;: &quot;Mark Thomas&quot;, &quot;dob&quot;: &quot;1982-05-16&quot;, &quot;hobby&quot;: &quot;diving&quot; &#125;&#123; &quot;index&quot;: &#123; &quot;_id&quot;: 3, &quot;parent&quot;: &quot;liverpool&quot; &#125;&#125;&#123; &quot;name&quot;: &quot;Barry Smith&quot;, &quot;dob&quot;: &quot;1979-04-01&quot;, &quot;hobby&quot;: &quot;hiking&quot; &#125;&#123; &quot;index&quot;: &#123; &quot;_id&quot;: 4, &quot;parent&quot;: &quot;paris&quot; &#125;&#125;&#123; &quot;name&quot;: &quot;Adrien Grand&quot;, &quot;dob&quot;: &quot;1987-05-11&quot;, &quot;hobby&quot;: &quot;horses&quot; &#125;\n\n如果你想要改变一个子文档的 parent 值，仅通过更新这个子文档是不够的，因为新的父文档有可能在另外一个分片上。因此，你必须要先把子文档删除，然后再重新索引这个子文档。\n","categories":["大数据","Elasticsearch"],"tags":["elasticsearch"]},{"title":"Macbook Pro mid 2014升级更换三星SAMSUNG 980 1T硬盘","url":"/2021/10/Macbook-Pro-mid-2014%E5%8D%87%E7%BA%A7%E6%9B%B4%E6%8D%A2%E4%B8%89%E6%98%9FSAMSUNG-980-1T%E7%A1%AC%E7%9B%98/","content":"很早前买的Macbook Pro mid 2014, 15” Retina，硬盘只有256G，装几个大的软件，或者虚拟机装几个Linux操作系统做大数据集群实验，空间就不够了。\n\n\n今天来升级一下系统，由最初的256G升级到1T。\n在升级之前，我至少准备了一周的时间，做足了功课。一是因为文件都挺重要，要是弄丢了就麻烦了。还有就是把系统搞崩了，又要花大量时间去做恢复，很耽误时间。\n这里面有非常多的坑，哪怕版本不正确，都会带来各种各样的问题。\n我目前的系统是macOS High Sierra 10.13.6。\n\n\nBoot ROM版本是162.0.0.0\n\n\n老硬盘接口是PCI 2代，通道也只有2个。\n\n\n准备材料三星980硬盘我的电脑型号是Mac Pro mid 2014, 15” Retina，硬盘接口是PCI 2代，所以比较后买的硬盘是三星980，这个是PCI 3代。有些人买三星980 Pro，PCI4代的硬盘，其实完全是浪费了，因为主板的限制，发挥不出硬盘的性能，哪怕只是多个小几百块钱，也是没必要的浪费，再用两三年，可以换个新的整机了。\n\n\nssd连接卡苹果使用的接口，是不能直接插入三星的ssd的。需要使用这个转接口。\n\n\n其他工具两种螺丝刀，一个是用于开盖的，一个是用于拧固定硬盘的螺丝刀的。还有一个散热器，在苹果里面应该是用不到了，如果加上的话，太厚了。\n\n\n备份使用苹果自带的Time Machine机制来备份。\n\n\n首先准备一个移动硬盘。\n\n\n如果移动硬盘的系统类型和苹果的不同，还要抹掉硬盘上面所有内容才可以备份。\n\n\n选择备份文件的磁盘。\n\n\n然后点击立即备份。\n\n\n备份时间蛮长的，256G花了好几个小时。建议周五晚上开始备份，或者把一些不需要备份内容排除掉。\n升级固件我Macbook Pro的固件版本，由上面可知是162.0.0.0，必须要升级到427.0.0.0，大于427，比如429，也会出现睡眠问题，不过更新的版本，也没有尝试，所以也不知道会不会遇到其他问题。\n让固件升级到427.0.0.0，我现在了解的，已经被实验成功的，必须是苹果原装硬盘，系统得是Big Sur 11.01版本。但现在官方网站上，Big Sur已经到11.6版本了。\n所以不能通过App Store安装了，只能下载Big Sur 11.01的镜像来安装了。找不到这个版本镜像的可以给我留言。\n等我们把固件升级好了以后，换上新硬盘，再用上面通过Time Machine备份的内容，恢复到我现在正常的High Sierra就行了。\n在升级系统之前，我们可以把一些大文件先删除掉，因为已经备份过了，这样可以让升级操作系统时有足够的空间。\n安装Big Sur 11.01版本，版本不要错了，因为我们只需要把固件升级到427.0.0.0。\n\n\n经过大约一个小时的安装，Big Sur 11.01就安装好了。\n\n\n查看固件版本，顺利升级到427.0.0.0。\n\n\n换硬盘带上手套操作，特别是夏天的时候，一定要防止手上的汗水滴污染主板。\n\n\n拧螺丝的时候，一定要用力往下压，不然弄花掉了螺丝就松不开了。螺丝的位置摆放好，有两种长度的螺丝。\n\n\n正常情况下，打开后盖后可以看到机器里面很多灰尘，特别是出风口附近。找来吹风和刷子，把灰尘清理干净。\n\n\n然后断电，让主板和电池的接接线断开。\n\n\n换一个螺丝刀，把硬盘拆下来。\n\n\n拆开新硬盘，接上NVME转接口。\n再插入SSD插槽。\n\n\n硬盘的标签不用撕掉。\n恢复开机，同时按住 Command + Option + R，一定要有Option，之前没按Option，结果进去没找到新硬盘。\n选择硬盘工具，然后选中三星980硬盘，格式化，AFPS格式，建议不选择加密，抹掉所有内容。\n\n\n重新启动，按刚才的方法，选择通过时间机器恢复。把最初备份的High Sierra的系统恢复到新硬盘中。大约几个小时就搞定了。\n这里我就不测试硬盘的速度了。\n查看硬件信息，可以使用4条通道了。\n\n\n查看了一下硬件温度，还行。\n\n","categories":["Tools"],"tags":["macbook","ssd"]},{"title":"Elasticsearch关联关系数据建模（一）应用层联接","url":"/2021/12/Elasticsearch%E5%85%B3%E8%81%94%E5%85%B3%E7%B3%BB%E6%95%B0%E6%8D%AE%E5%BB%BA%E6%A8%A1%EF%BC%88%E4%B8%80%EF%BC%89%E5%BA%94%E7%94%A8%E5%B1%82%E8%81%94%E6%8E%A5/","content":"从关系型数据库迁移数据到Elasticsearch时，总要处理很多关联数据，如何进行数据建模，下面给出了四种方案可供大家参考。\n其中最简单的一种就是数据冗余扁平化，这个不做过多讲解。\n应用层联接有点类型关系型数据库的子查询。第一次查询的结果作为第二次查询的条件。\nPUT /my_index/user/1 &#123;  &quot;name&quot;:     &quot;John Smith&quot;,  &quot;email&quot;:    &quot;john@smith.com&quot;,  &quot;dob&quot;:      &quot;1970/10/24&quot;&#125;PUT /my_index/blogpost/2 &#123;  &quot;title&quot;:    &quot;Relationships&quot;,  &quot;body&quot;:     &quot;It&#x27;s complicated...&quot;,  &quot;user&quot;:     1 &#125;\n\nblogpost 通过用户的 id 链接到用户。\n通过用户的 ID 1 可以很容易的找到博客帖子。\nGET /my_index/blogpost/_search&#123;  &quot;query&quot;: &#123;    &quot;filtered&quot;: &#123;      &quot;filter&quot;: &#123;        &quot;term&quot;: &#123; &quot;user&quot;: 1 &#125;      &#125;    &#125;  &#125;&#125;\n\n为了找到用户叫做 John 的博客帖子，我们需要运行两次查询。先查询名字包含 John 的所有用户的 id 集合，再像上面一样根据 id 查询 blogpost。\n执行第一个查询得到的结果将填充到 terms 过滤器中。\nGET /my_index/user/_search&#123;  &quot;query&quot;: &#123;    &quot;match&quot;: &#123;      &quot;name&quot;: &quot;John&quot;    &#125;  &#125;&#125;GET /my_index/blogpost/_search&#123;  &quot;query&quot;: &#123;    &quot;filtered&quot;: &#123;      &quot;filter&quot;: &#123;        &quot;terms&quot;: &#123; &quot;user&quot;: [1, 3, 7] &#125;        &#125;    &#125;  &#125;&#125;\n\n总结：应用层联接的主要优点是可以对数据进行标准化处理。缺点就是需要2次查询，有时间消耗。如果说叫 John 的用户有很多，比如百万以上，那查询是非常没有效率的。这种方法适合于 user 只有少量文档的情况，并且最好它们很少改变，这将允许应用程序对结果进行缓存，避免经常运行第一次查询。\neg. 搜索用户名称和博客标题，展示用户及其最相关的博客列表。\n需要按用户名称进行分组，根据score进行排序选TOPN。\nhttps://www.elastic.co/guide/cn/elasticsearch/guide/current/top-hits.html\neg. 文件目录的搜索，可以参考如下链接。\nhttps://www.elastic.co/guide/cn/elasticsearch/guide/current/denormalization-concurrency.html\neg. 并发问题\nhttps://www.elastic.co/guide/cn/elasticsearch/guide/current/concurrency-solutions.html\n","categories":["大数据","Elasticsearch"],"tags":["elasticsearch"]},{"title":"pandas常用函数操作","url":"/2021/10/pandas%E5%B8%B8%E7%94%A8%E5%87%BD%E6%95%B0%E6%93%8D%E4%BD%9C/","content":"删除列df.drop([col1, col2], axis=1)df.drop(columns=[col1, col2])\n\n删除行df.drop([index1, index2], axis=0)df.drop(index=[index1, index2])\n\n查找唯一df[col].unique()\n\n按值筛选df[df[col].isin([&#x27;银行&#x27;, &#x27;保险&#x27;])]\n\n分类bins = [0, 10, 20, 30]pd.cut(df[col], bins, labels=[l1, l2, ...])\n\n分为5个桶 \npd.qcut(df[col], 5)\n\n长宽转换df.stack()\n\n原来的Index变成MultiIndex: (index1, col1), (index1, col2), (index1, col1), (index2, col2)\n宽表转长表\n平安银行 yoy_2019 yoy_2020 yoy_2021\ndf.melt(id_vars=[col1], var_name=[&#x27;year&#x27;], value_name=&#x27;yoy&#x27;)\n\n长表转宽表\ndf.pivot_table(index=[col1], columns=[&#x27;year&#x27;], values=[&#x27;yoy&#x27;])\n\napply()和applymap()apply按列、行应用函数\n分位数pd.quantile([0.1, 0.2])\n\n相关性df[col1].corr(df[col2])\n\ndate_rangepd.date_range(&#x27;2020-01-31&#x27;, &#x27;2021-12-31&#x27;, freq=&#x27;M&#x27;)\n\n就会生成每个月最后一天的日期序列。\n","categories":["Python"],"tags":["pandas"]},{"title":"Python range和arange的区别","url":"/2021/10/Python-range%E5%92%8Carange%E7%9A%84%E5%8C%BA%E5%88%AB/","content":"range 函数用于生成一个整数（integer）序列。\nrange(start, end, step)，返回类型为range，左闭右开区间。\nfor i in range(1, 10, 2):    print(i)    13579\n\narange 是 numpy 包下面的函数，生成一个 float64 的 Array。\nimport numpy as npfor i in np.arange(1, 2, 0.1):    print(i)    1.01.11.20000000000000021.30000000000000031.40000000000000041.50000000000000041.60000000000000051.70000000000000061.80000000000000071.9000000000000008\n\n这里涉及到一个问题就是 float 比较大小的问题，可以使用 math.isclose() 函数。\nif math.isclose(1.2000000000000002, 1.2, rel_tol=1e-5)\n","categories":["Python"],"tags":["range","arange"]},{"title":"Python matlibplot画图最佳实践","url":"/2021/10/Python-matlibplot%E7%94%BB%E5%9B%BE%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/","content":"之前也写过一些关于使用 Python matplotlib 画图的文章，写得都比较初级，内容繁多，有些可能根本就用不到。\n根据工作中画图的经验，总结了一些最佳实践，主要涉及到一些设置方面的，商业逻辑不包含在内。此文章会一直更新。\n设置# 导入画图工具import matplotlib.pyplot as plt# 让图表直接在juypter notebook中展示出来，高版本貌似已经不需要这条语句了# %matplotlib inline# 解决中文乱码问题，高版本貌似不需要这条语句了# plt.rcParams[&#x27;font.sans-serif&#x27;] = &#x27;SimHei&#x27;# 解决负号无法显示的问题# plt.rcParams[&#x27;axes.unicode_minus&#x27;] = False# 设置图片格式为svg矢量图，更加清晰%config InlineBackend.figure_format = &#x27;svg&#x27;\n\n画布创建画布创建一个画布，分为两行两列。\nfig, axes = plt.subplots(2, 2)\n\n展示画布的时候，只需要输出fig就行了，貌似也不需要设置%matplotlib inline。\nfig\n\n删除子图当某个子图不需要展现时，可以使用如下语句：\n删除某子图\nax = axes[0, 1]fig.delaxes(ax)\n\n隐藏某子图\nax.set_axis_off()\n\n不遮挡布局设置了x轴标签后，可能被下方的子图遮挡，使用如下语句可以让遮挡部分显示出来。\nfig.tight_layout()\n\n画图在第0行，第0列画图。\naxes[0, 0].plot(x, y)\n\n设置x, y轴标签ax.set_xlabel(&#x27;时间&#x27;)ax.set_ylabel(&#x27;销量&#x27;)\n","categories":["Python"],"tags":["matplotlib"]},{"title":"Python re模块正则表达式","url":"/2021/10/Python-re%E6%A8%A1%E5%9D%97%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F/","content":"使用 Python re 模块运用正则表达式。\nre.match 是从字符串的第一个字符开始比较，返回true或false \nimport retext = input(&#x27;请输入你的邮箱：&#x27;)if re.match(r&#x27;[0-9a-zA-Z_]&#123;1,19&#125;@[0-9a-zA-Z]&#123;1,13&#125;\\.[com,cn,net]&#123;1,3&#125;&#x27;, text):    print(&#x27;这是一个邮箱&#x27;)else:    print(&#x27;这不是一个邮箱&#x27;)\n\nre.search 是查找这个字符中是否包含被查找的字符串，如果有，则输出这个字条串。\nimport retext = input(&#x27;请输入你的邮箱：&#x27;)email = re.search(r&#x27;[0-9a-zA-Z_]&#123;1,19&#125;@[0-9a-zA-Z]&#123;1,13&#125;\\.[com,cn,net]&#123;1,3&#125;&#x27;, text)if email != None:    print(email.group())else:    print(&#x27;这不是一个邮箱&#x27;)\n\n当要复用某个正则表达式时，可以使用re.compile把这个正则保存下来。\nimport re# 在[]集合模式中，^字符出现在第一个字符时，表示取反pattern = re.compile(r&#x27;[a-zA-Z]*://[^\\s]*&#x27;)string = &#x27;测试http://www.baidu.com 空白&#x27;url = re.findall(pattern, string)print(url)","categories":["Tools"],"tags":["regex","re"]},{"title":"通过httpclient发送GET和POST请求","url":"/2021/10/%E9%80%9A%E8%BF%87httpclient%E5%8F%91%E9%80%81GET%E5%92%8CPOST%E8%AF%B7%E6%B1%82/","content":"通过httpclient POST请求上传文件。\nCloseableHttpClient httpClient = HttpClients.createDefault();HttpPost httpPost = new HttpPost(&quot;http://localhost:8080/upload&quot;);MultipartEntityBuilder builder = MultipartEntityBuilder.create();builder.setMode(HttpMultipartMode.BROWSER_COMPATIBLE);// 可以添加File或者InputStreambuilder.addBinaryBody(&quot;file&quot;, file.getInputStream(), ContentType.MULTIPART_FORM_DATA, filename);builder.addTextBody(&quot;name&quot;, name);HttpEntity entity = builder.build();httpPost.setEntity(entity);CloseableHttpResponse response = httpClient.execute(httpPost);Integer httpCode = response.getStatusLine().getStatusCode();if (HttpStatus.SC_OK != httpCode) &#123;    throw new RuntimeException(&quot;failed...&quot;);&#125;// 处理responseEntityString respEntity = EntityUtils.toString(response.getEntity());\n\nGET请求\nCloseableHttpClient httpClient = HttpClients.createDefault();// URL的拼接URI uri = new URI(&quot;http://localhost:8080&quot;).resolve(&quot;result&quot;);HttpGet httpGet = new HttpGet(uri);CloseableHttpResponse response = httpClient.execute(httpGet);Integer httpCode = response.getStatusLine().getStatusCode();if (HttpStatus.SC_OK != httpCode) &#123;    throw new RuntimeException(&quot;failed...&quot;);&#125;\n\n判断请求返回的内容类型\nContentType contentType = ContentType.getOrDefault(response.getEntity());if (MediaType.APPLICATION_JSON_VALUE.equals(contentType.getMimeType())) &#123;    &#125; else if (MediaType.TEXT_PLAIN_VALUE.equals(contentType.getMimeType())) &#123;&#125;","categories":["Java"],"tags":["httpclient"]},{"title":"如何查看MacBook各大机型的硬件规格","url":"/2021/10/%E5%A6%82%E4%BD%95%E6%9F%A5%E7%9C%8BMacBook%E5%90%84%E5%A4%A7%E6%9C%BA%E5%9E%8B%E7%9A%84%E7%A1%AC%E4%BB%B6%E8%A7%84%E6%A0%BC/","content":"如果在好几年前买了mac电脑，现在想升级，所以需要查看一下电脑的硬件规格信息。\n我的macOS是High Sierra。\n可以直接通过如下路径查看：关于本机 -&gt; 系统报告\n或者访问苹果官网：https://support.apple.com/zh-cn/HT201300\n最后附上SSD硬盘接口标准。\n\nMacBook Air (2013-2014）： PCI-E 2.0 X2（1GB&#x2F;s），不支持原生休眠MacBook Air 11” Mid 2013 (MacBookAir6,1)MacBook Air 13” Mid 2013 (MacBookAir6,2)MacBook Air 11” early 2014 (MacBookAir6,1)MacBook Air 13” early 2014 (MacBookAir6,2)\n\nMacBook Air （2015-2017）：PCI-E 2.0 X4（2GB&#x2F;s），支持原生休眠MacBook Air 13” early 2015 (MacBookAir7,1)MacBook Air 13” 2017 (MacBookAir7,2)\n\n\n3.MacBook Pro （2013-2014）：PCI-E 2.0 X4（2GB&#x2F;s），不支持原生休眠MacBook Pro Retina 13” late 2013 (MacBookPro11,1)MacBook Pro Retina 15” late 2013 (MacBookPro11,2 &amp; MacBookPro11,3)MacBook Pro Retina 13” mid 2014 (MacBookPro11,1)MacBook Pro Retina 15” mid 2014 (MacBookPro11,2 &amp; 11,3)\n4.MacBook Pro （2015）：PCI-E 2.0或者3.0 X4，支持原生休眠MacBook Pro Retina 13” early 2015 (MacBookPro12,1) ，为PCI-E 2.0规格，速度2GB&#x2F;sMacBook Pro Retina 15” mid 2015 (MacBookPro11,4-11,5)，为PCI-E 3.0规格，速度3.94GB&#x2F;s\n","categories":["OS","MacOS"],"tags":["macbook"]},{"title":"python模块import和from import","url":"/2021/10/python%E6%A8%A1%E5%9D%97import%E5%92%8Cfrom-import/","content":"模块的引入import module1[, module2[,... moduleN]]\n\n调用模块中的函数，格式：模块名.函数名\nmodule1.fun()\n\nfrom…import导入module模块中的fun1，可以导入函数，也可以是一个类。\nfrom module import fun1fun1()\n\nfrom module import clzclz.fun1()\n\n导入module模块中所有函数，这种写法不推荐使用。\nfrom module import *fun1()\n\n子模块当把一个模块放到一个文件夹下面时\nimport module1.module2\n\nmodule1就是这个文件夹的名字，module2为子模块。\n","categories":["Python"],"tags":["import"]},{"title":"Jupyter Lab环境中切换不同Python内核","url":"/2024/01/Jupyter-Lab%E7%8E%AF%E5%A2%83%E4%B8%AD%E5%88%87%E6%8D%A2%E4%B8%8D%E5%90%8CPython%E5%86%85%E6%A0%B8/","content":"我一般选择在 base 环境中安装 jupyter lab。\n(base) D:\\development\\python&gt;mamba install jupyterlab\n\n在 base 环境中，可以安装 nb_conda_kernels，这个是一个与 Conda 集成的 Jupyter 插件，它允许你在 Jupyter 中使用 Conda 环境中的不同内核。也可以不安装，而安装下面所说的ipykernel。\n创建一个新的环境，比如 py310，在这个环境里面安装 ipykernel，mamba install ipykernel。然后执行命令 python -m ipykernel install --user --name py310，其中py310是环境的名字。\nipykernel 包是 Jupyter 的一个关键组件，它提供了 IPython 内核，允许 Jupyter 与 IPython 交互式计算环境进行通信。\n在jupyter lab页面中刷新，就可以自由选择 python 环境了，如下图所示：\n\n\n这时会在Start Preferred Kernel里面显示新的环境。\n\n\n","categories":["Python"],"tags":["conda","jupyter"]},{"title":"conda安装国内源","url":"/2024/01/conda%E9%85%8D%E7%BD%AE%E5%9B%BD%E5%86%85%E6%BA%90/","content":"Anaconda 是一个用于科学计算的 Python 发行版，支持 Linux, Mac, Windows, 包含了众多流行的科学计算、数据分析的 Python 包。\nAnaconda 安装包可以到 https://mirrors.tuna.tsinghua.edu.cn/anaconda/archive/ 下载。\nTUNA 还提供了 Anaconda 仓库与第三方源（conda-forge、msys2、pytorch等，查看完整列表，更多第三方源可以前往校园网联合镜像站查看）的镜像，各系统都可以通过修改用户目录下的 .condarc 文件来使用 TUNA 镜像源。Windows 用户无法直接创建名为 .condarc 的文件，可先执行 conda config –set show_channel_urls yes 生成该文件之后再修改。\n注：由于更新过快难以同步，他们不同步pytorch-nightly, pytorch-nightly-cpu, ignite-nightly这三个包。\nchannels:  - defaultsshow_channel_urls: truedefault_channels:  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/r  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/msys2custom_channels:  conda-forge: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud  msys2: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud  bioconda: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud  menpo: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud  pytorch: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud  pytorch-lts: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud  simpleitk: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud  deepmodeling: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/\n\n即可添加 Anaconda Python 免费仓库。\n运行 conda clean -i 清除索引缓存，保证用的是镜像站提供的索引。\nMiniconda 是一个 Anaconda 的轻量级替代，默认只包含了 python 和 conda，但是可以通过 pip 和 conda 来安装所需要的包。\nMiniconda 安装包可以到 https://mirrors.tuna.tsinghua.edu.cn/anaconda/miniconda/ 下载。\n","categories":["Python"],"tags":["conda"]},{"title":"macOS安装Qlib爬坑心路历程","url":"/2024/01/macOS%E5%AE%89%E8%A3%85Qlib%E7%88%AC%E5%9D%91%E5%BF%83%E8%B7%AF%E5%8E%86%E7%A8%8B/","content":"我的操作系统是macOS Catalina 10.15.7Python 3.8\n通过源码安装Qlib，记录一下掉过的坑。\n总的来说，按照官方说明文档，执行安装命令，一般不会有问题。\ncython一定要更新，upgrade\npip要和python环境里面的版本一致，不确定是否一致，可以python -m pip install xxx的方式。\n也可以通过pip show pip查看位置。\n通过源码安装，pip install .，尽量不通过 pip install pyqlib来安装，更不能通过 python setup.py install来安装。\nqlib安装在python&#x3D;3.8下面，3.9目前支持不好。\n创建python&#x3D;3.8的虚拟环境后，想要在 jupyter lab 里面的 kernel select 中显示出来，需要在3.8的虚拟环境中执行命令：pip install ipykernel\n如果错误的添加了kernel，需要这样删除：jupyter kernelspec remove kernel_name\n在python3.8的环境里面，要安装pip install ipywidgets，还有pip install tqdm。\n安装好了以后，运行qlib&#x2F;examples&#x2F;workflow_by_code.ipynb，在train model这里，有报错：\n\n\n意思就是libomp这个库文件没有被加载到。\n后来尝试了很多方法，按网上说的，都是brew install libomp。\n我的macOS版本太旧，brew都已经不支持了。要升级到macOS 12，但我的笔记本又不支持。\n然后又找了各种办法通过源码来安装libomp。\n又去搞了半天编译工具。\n最后通过简单的两步搞定了问题。\n$ pip uninstall lightgbmFound existing installation: lightgbm 4.2.0Uninstalling lightgbm-4.2.0:  Would remove:    /opt/anaconda3/envs/py38/lib/python3.8/site-packages/lightgbm-4.2.0.dist-info/*    /opt/anaconda3/envs/py38/lib/python3.8/site-packages/lightgbm/*Proceed (Y/n)? y  Successfully uninstalled lightgbm-4.2.0\n\n$ conda install lightgbmCollecting package metadata (current_repodata.json): doneSolving environment: done## Package Plan ##  environment location: /opt/anaconda3/envs/py38  added / updated specs:    - lightgbmThe following packages will be downloaded:    package                    |            build    ---------------------------|-----------------    blas-1.0                   |         openblas          45 KB    libgfortran-5.0.0          |11_3_0_hecd8cb5_28         142 KB    libgfortran5-11.3.0        |      h9dfd629_28         1.4 MB    libopenblas-0.3.21         |       h54e7dc3_0         4.9 MB    lightgbm-4.1.0             |   py38hcec6c5f_0         1.4 MB    llvm-openmp-14.0.6         |       h0dcd299_0         288 KB    numpy-1.24.3               |   py38h57a7bef_0          12 KB    numpy-base-1.24.3          |   py38hc93c6d9_0         6.5 MB    pooch-1.7.0                |   py38hecd8cb5_0          85 KB    scipy-1.10.1               |   py38h9034365_1        20.9 MB    ------------------------------------------------------------                                           Total:        35.8 MBThe following NEW packages will be INSTALLED:  blas               pkgs/main/osx-64::blas-1.0-openblas  libgfortran        pkgs/main/osx-64::libgfortran-5.0.0-11_3_0_hecd8cb5_28  libgfortran5       pkgs/main/osx-64::libgfortran5-11.3.0-h9dfd629_28  libopenblas        pkgs/main/osx-64::libopenblas-0.3.21-h54e7dc3_0  lightgbm           pkgs/main/osx-64::lightgbm-4.1.0-py38hcec6c5f_0  llvm-openmp        pkgs/main/osx-64::llvm-openmp-14.0.6-h0dcd299_0  numpy              pkgs/main/osx-64::numpy-1.24.3-py38h57a7bef_0  numpy-base         pkgs/main/osx-64::numpy-base-1.24.3-py38hc93c6d9_0  pooch              pkgs/main/osx-64::pooch-1.7.0-py38hecd8cb5_0  scipy              pkgs/main/osx-64::scipy-1.10.1-py38h9034365_1Proceed ([y]/n)? y\n\n核心错误就出在我们之前都是通过pip来安装的，像这种有大量依赖的库，最好还是要通过conda install来安装。\n最后qlib的work_flow在macOS下成功运行。\n\n","categories":["Quant"],"tags":["qlib"]},{"title":"conda命令出现unexpected error","url":"/2024/01/conda%E5%91%BD%E4%BB%A4%E5%87%BA%E7%8E%B0unexpected-error/","content":"执行 conda 命令时，报错。\n\n\n$ /opt/anaconda3/bin/conda create -n py38 python=3.8`  environment variables:                 CIO_TEST=&lt;not set&gt;                CLASSPATH=.:/Library/Java/JavaVirtualMachines/jdk1.8.0_92.jdk/Contents/Home/lib/                          tools.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_92.jdk/Contents/H                          ome/lib/dt.jar        CONDA_DEFAULT_ENV=base                CONDA_EXE=/opt/anaconda3/bin/conda             CONDA_PREFIX=/opt/anaconda3    CONDA_PROMPT_MODIFIER=(base)         CONDA_PYTHON_EXE=/opt/anaconda3/bin/python               CONDA_ROOT=/opt/anaconda3              CONDA_SHLVL=1           CURL_CA_BUNDLE=&lt;not set&gt;                   GOPATH=/Users/simon/Development/workspace/go                     PATH=/opt/anaconda3/bin:/Users/simon/.yarn/bin:/Users/simon/.config/yarn/gl                          obal/node_modules/.bin:/Users/simon/opt/anaconda3/bin:/Users/simon/opt                          /anaconda3/bin:/anaconda3/bin:/Library/Frameworks/Python.framework/Ver                          sions/3.6/bin:/Library/Java/JavaVirtualMachines/jdk1.8.0_92.jdk/Conten                          ts/Home/bin:/Users/simon/Development/maven/apache-maven-3.6.1/bin:/Use                          rs/simon/.yarn/bin:/Users/simon/.config/yarn/global/node_modules/.bin:                          /opt/anaconda3/bin:/opt/anaconda3/condabin:/Users/simon/opt/anaconda3/                          bin:/Users/simon/opt/anaconda3/bin:/anaconda3/bin:/Library/Frameworks/                          Python.framework/Versions/3.6/bin:/Library/Java/JavaVirtualMachines/jd                          k1.8.0_92.jdk/Contents/Home/bin:/Users/simon/Development/maven/apache-                          maven-3.6.1/bin:/usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin:/Applicat                          ions/VMware Fusion.app/Contents/Public:/usr/local/go/bin:/usr/local/go                          /bin:/Users/simon/Development/workspace/go/bin:/usr/local/go/bin:/User                          s/simon/Development/workspace/go/bin       REQUESTS_CA_BUNDLE=&lt;not set&gt;            SSL_CERT_FILE=&lt;not set&gt;     active environment : base    active env location : /opt/anaconda3            shell level : 1       user config file : /Users/simon/.condarc populated config files : /Users/simon/.condarc          conda version : 4.13.0    conda-build version : 3.18.11         python version : 3.7.6.final.0       virtual packages : __osx=10.15.7=0                          __unix=0=0                          __archspec=1=x86_64       base environment : /opt/anaconda3  (writable)      conda av data dir : /opt/anaconda3/etc/conda  conda av metadata url : None           channel URLs : https://pypi.ricequant.com/simple/osx-64                          https://pypi.ricequant.com/simple/noarch                          https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch/osx-64                          https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch/noarch                          https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/osx-64                          https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/noarch                          https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/osx-64                          https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/noarch          package cache : /opt/anaconda3/pkgs                          /Users/simon/.conda/pkgs       envs directories : /opt/anaconda3/envs                          /Users/simon/.conda/envs               platform : osx-64             user-agent : conda/4.13.0 requests/2.22.0 CPython/3.7.6 Darwin/19.6.0 OSX/10.15.7                UID:GID : 501:20             netrc file : None           offline mode : FalseAn unexpected error has occurred. Conda has prepared the above report.If submitted, this report will be used by core maintainers to improvefuture releases of conda.Would you like conda to send this report to the core maintainers?[y/N]:\n\n解决方案：\n$ conda config --show-sources==&gt; /Users/somebody/.condarc &lt;==ssl_verify: Truechannels:  - https://pypi.ricequant.com/simple  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch/  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/show_channel_urls: True\n\n删除 /Users/somebody/.condarc，就可以解决了。\n","tags":["conda"]},{"title":"JVM相关命令","url":"/2023/12/JVM%E7%9B%B8%E5%85%B3%E5%91%BD%E4%BB%A4/","content":"查看JVM相关的信息。\njinfo [option] &lt;pid&gt;\n查看JVM使用的哪个垃圾回收器\njinfo -flags &lt;pid&gt;\n# jinfo -flags 58256Attaching to process ID 58256, please wait...Debugger attached successfully.Server compiler detected.JVM version is 25.212-b10Non-default VM flags: -XX:CICompilerCount=4 -XX:InitialHeapSize=264241152 -XX:MaxHeapSize=4200595456 -XX:MaxNewSize=1399848960 -XX:MinHeapDeltaBytes=524288 -XX:NewSize=88080384 -XX:OldSize=176160768  -XX:+UseCompressedClassPointers -XX:+UseCompressedOops -XX:+UseParallelGC\n\n可以看出是使用的 UseParallelGC。\n通过如下命令也可以印证。\n# jinfo -flag UseParallelGC 58256-XX:+UseParallelGC\n\n\n查看类加载的信息\n# jstat -class 58256 1000 10Loaded  Bytes  Unloaded  Bytes     Time 14582 26192.0        1     0.9      23.92 14582 26192.0        1     0.9      23.92 14582 26192.0        1     0.9      23.92 14582 26192.0        1     0.9      23.92 14582 26192.0        1     0.9      23.92 14582 26192.0        1     0.9      23.92 14582 26192.0        1     0.9      23.92\n\n\n查看垃圾回收器的信息\n# jstat -gc 58256 1000 10 S0C    S1C    S0U    S1U      EC       EU        OC         OU       MC     MU    CCSC   CCSU   YGC     YGCT    FGC    FGCT     GCT28160.0 10240.0  0.0   9808.1 597504.0 430347.2  251904.0   69700.9   78612.0 74904.0 10300.0 9577.3     17    0.643   3      1.166    1.80928160.0 10240.0  0.0   9808.1 597504.0 430347.2  251904.0   69700.9   78612.0 74904.0 10300.0 9577.3     17    0.643   3      1.166    1.80928160.0 10240.0  0.0   9808.1 597504.0 430347.2  251904.0   69700.9   78612.0 74904.0 10300.0 9577.3     17    0.643   3      1.166    1.80928160.0 10240.0  0.0   9808.1 597504.0 430347.2  251904.0   69700.9   78612.0 74904.0 10300.0 9577.3     17    0.643   3      1.166    1.80928160.0 10240.0  0.0   9808.1 597504.0 430347.2  251904.0   69700.9   78612.0 74904.0 10300.0 9577.3     17    0.643   3      1.166    1.80928160.0 10240.0  0.0   9808.1 597504.0 430347.2  251904.0   69700.9   78612.0 74904.0 10300.0 9577.3     17    0.643   3      1.166    1.809\n\n查看是否有死锁\n# jstack 58256\n\n查看堆内存数据\n# jmap -heap 58256Attaching to process ID 58256, please wait...Debugger attached successfully.Server compiler detected.JVM version is 25.212-b10using thread-local object allocation.Parallel GC with 8 thread(s)Heap Configuration:   MinHeapFreeRatio         = 0   MaxHeapFreeRatio         = 100   MaxHeapSize              = 4200595456 (4006.0MB)   NewSize                  = 88080384 (84.0MB)   MaxNewSize               = 1399848960 (1335.0MB)   OldSize                  = 176160768 (168.0MB)   NewRatio                 = 2   SurvivorRatio            = 8   MetaspaceSize            = 21807104 (20.796875MB)   CompressedClassSpaceSize = 1073741824 (1024.0MB)   MaxMetaspaceSize         = 17592186044415 MB   G1HeapRegionSize         = 0 (0.0MB)Heap Usage:PS Young GenerationEden Space:   capacity = 589299712 (562.0MB)   used     = 93162144 (88.84634399414062MB)   free     = 496137568 (473.1536560058594MB)   15.808958006074844% usedFrom Space:   capacity = 10485760 (10.0MB)   used     = 10174544 (9.703201293945312MB)   free     = 311216 (0.2967987060546875MB)   97.03201293945312% usedTo Space:   capacity = 30408704 (29.0MB)   used     = 0 (0.0MB)   free     = 30408704 (29.0MB)   0.0% usedPS Old Generation   capacity = 257949696 (246.0MB)   used     = 71381888 (68.0750732421875MB)   free     = 186567808 (177.9249267578125MB)   27.672794000889226% used29817 interned Strings occupying 3078576 bytes.","categories":["Java"],"tags":["java","jvm"]},{"title":"Faile to import cython option pricing model, please rebuild with cython in cmd.","url":"/2023/12/Faile-to-import-cython-option-pricing-model-please-rebuild-with-cython-in-cmd/","content":"运行vnpy 3.9.0时，命令行提示如下异常：\nFaile to import cython option pricing model, please rebuild with cython in cmd.\n进入模型文件夹 /vnpy_optionmaster/vnpy_optionmaster/pricing/cython_model/black_scholes_cython\n执行命令：\n$ python setup.py build_ext --inplace\n生成的.so和.pyx拷贝到目录/site-packages/vnpy_optionmaster/pricing下面就可以解决了。\n","categories":["Vnpy"],"tags":["vnpy","cython"]},{"title":"解决ANOMALY: use of REX.w is meaningless (default operand size is 64)","url":"/2021/11/%E8%A7%A3%E5%86%B3ANOMALY-use-of-REX-w-is-meaningless-default-operand-size-is-64/","content":"Windows10环境，某天突然就没法使用 git, svn了，IntelliJ IDEA下面的git, svn也没法正常使用。\n使用git desktop时，提示cannot publish unborn HEAD。\n使用 cmd 时，也会提示信息 ANOMALY: use of REX.w is meaningless (default operand size is 64)。\n导致这个事情的原因是Win10的自动更新和某监控软件产生了冲突。\n使用如下语句可以查看到这个软件的连接。\nnetstat -ano | findstr 8237\n我们可以通过修改注册表的方式解决上述问题。\n在运行中输入regedit打开注册表，然后打开到目录计算机\\HKEY_LOCAL_MACHINE\\SOFTWARE\\TEC\\Ocular.3\\agent\\config 下新建 字符串值hookapi_filterproc_external，值为cmd.exe;powershell.exe;git.exe;idea64.exe。\n重启软件，一切恢复正常。\n"},{"title":"生产者消费者模型","url":"/2021/11/%E7%94%9F%E4%BA%A7%E8%80%85%E6%B6%88%E8%B4%B9%E8%80%85%E6%A8%A1%E5%9E%8B/","content":"生产者消费者模型的特点：\n保证生产者不会在缓冲区满的时候继续向缓冲区放入数据，而消费者也不会在缓冲区空的时候，消耗数据。\n当缓冲区满的时候，生产者会进入休眠状态，当下次消费者开始消耗缓冲区的数据时，生产者才会被唤醒，开始往缓冲区中添加数据；当缓冲区空的时候，消费者也会进入休眠状态，直到生产者往缓冲区中添加数据时才会被唤醒。\nimport lombok.extern.slf4j.Slf4j;import java.util.concurrent.TimeUnit;public class ProducerAndConsumerDemo &#123;    public static void main(String[] args) &#123;        Store store = new Store(10);        new Thread(new Producer(store), &quot;producer&quot;).start();        new Thread(new Consumer(store), &quot;consumer1&quot;).start();        new Thread(new Consumer(store), &quot;consumer2&quot;).start();    &#125;&#125;@Slf4jclass Store &#123;    private Integer capacity;    private Integer count = 0;    public Store(Integer capacity) &#123;        this.capacity = capacity;    &#125;    public Boolean isFull() &#123;        return this.count &gt;= this.capacity;    &#125;    public Boolean isEmpty() &#123;        return this.count &lt;= 0;    &#125;    synchronized void produce() throws InterruptedException &#123;        if (this.isFull()) &#123;            log.info(&quot;仓库满了，停止生产&quot;);            wait();        &#125; else if (this.isEmpty()) &#123;            log.info(&quot;仓库空了, 放入第&quot; + count + &quot;号位置，并通知消费者&quot;);            this.count++;            TimeUnit.SECONDS.sleep(1);            notifyAll();        &#125; else &#123;            log.info(&quot;放入第&quot; + count + &quot;号位置&quot;);            this.count++;            TimeUnit.SECONDS.sleep(1);        &#125;    &#125;    synchronized void consume() throws InterruptedException &#123;        if (this.isEmpty()) &#123;            log.info(&quot;仓库空了，停止消费&quot;);            wait();        &#125; else if (this.isFull()) &#123;            this.count--;            log.info(&quot;仓库满了，取出第&quot; + count + &quot;号位置, 并通知生产者&quot;);            TimeUnit.SECONDS.sleep(2);            notifyAll();        &#125; else &#123;            this.count--;            log.info(&quot;取出第&quot; + count + &quot;号位置&quot;);            TimeUnit.SECONDS.sleep(2);        &#125;    &#125;&#125;class Producer implements Runnable &#123;    private Store store;    public Producer(Store store) &#123;        this.store = store;    &#125;    @Override    public void run() &#123;        while (true) &#123;            try &#123;                store.produce();            &#125; catch (InterruptedException e) &#123;                e.printStackTrace();            &#125;        &#125;    &#125;&#125;class Consumer implements Runnable &#123;    private Store store;    public Consumer(Store store) &#123;        this.store = store;    &#125;    @Override    public void run() &#123;        while (true) &#123;            try &#123;                store.consume();            &#125; catch (InterruptedException e) &#123;                e.printStackTrace();            &#125;        &#125;    &#125;&#125;","categories":["Java"]},{"title":"Java中的锁","url":"/2023/12/Java%E4%B8%AD%E7%9A%84%E9%94%81/","content":"锁的分类可重入锁、不可重入锁重入的意思，在获取到锁A以后，再次获取A锁。\n可重入锁：synchronized, ReentrantLock, ReentrantReadWriteLock\n不可重入，在获取到锁A以后，再次获取锁A之前，自己必须先释放前面加的锁。\n乐观锁、悲观锁悲观锁就是，当获取不到锁资源时，会将当前线程挂起，即进入BLOCKED或WAITING状态。\n线程的挂起会涉及到用户态和内核态的切换。\n用户态：JVM可以自行执行的指令。\n内核态：需要操作系统才可以执行。\n悲观锁：synchronized, ReentrantLock, ReentrantReadWriteLock\n乐观锁：获取不到锁资源，可以再次让CPU调度，重新尝试获取锁资源。\nAtomic原子类中，基于CAS乐观锁实现的。\n公平锁、非公平锁synchronized只能是非公平锁。\nReentrantLock, ReentrantReadWriteLock 可以实现公平与非公平锁。\n公平锁，未获取到锁时，去排队。\n非公平锁，先尝试竞争抢锁，抢不到再去排队。\n互斥锁、共享锁互斥锁：synchronized、ReentrantLock\nReentrantReadWriteLock，有互斥锁，也有共享锁。\n互斥锁：同一时间点，只有一个线程持有这个锁。\n共享锁：同一时间点，多个线程持有。\n","categories":["Java"],"tags":["java","Lock"]},{"title":"Mybatis xml一对多的写法","url":"/2023/12/Mybatis-xml%E4%B8%80%E5%AF%B9%E5%A4%9A%E7%9A%84%E5%86%99%E6%B3%95/","content":"返回类\nList&lt;StudentCourses&gt; queryStudentCourses();package xxx.StudentCourses@Datapublic class StudentCourses &#123;    private String name;    private List&lt;String&gt; courses;&#125;\n\n&lt;resultMap id=&quot;resultMap&quot; type=&quot;xxx.StudentCourses&quot;&gt;    &lt;result column=&quot;name&quot; property=&quot;name&quot; /&gt;    &lt;collection property=&quot;courses&quot; ofType=&quot;string&quot;&gt;        &lt;result column=&quot;course&quot; /&gt;    &lt;/collection&gt;&lt;/resultMap&gt;\n\n\n&lt;select id=&quot;queryStudentCourses&quot; resultMap=&quot;&quot;&gt;    SELECT s.name, sc.course FROM student_course sc    JOIN student s ON(s.id = sc.student_id)&lt;/select&gt;\n\n","categories":["Java"],"tags":["mysql","mybatis"]},{"title":"股票指标解释","url":"/2023/12/%E8%82%A1%E7%A5%A8%E6%8C%87%E6%A0%87%E8%A7%A3%E9%87%8A/","content":"有些金融指标，不太好理解，这里做一个解释说明。\n流入额流入额一般就是买入额。\n指定日内(超大单、大单、中单或小单)的买入金额。\n净买入额指定日内(超大单、大单、中单或小单)的买入金额-(超大单、大单、中单或小单)的卖出金额。\n主动买入额主动指主动去成交已经存在的对手单；被动是指挂单等待成交。\n主动买入额(全单)全单和非全单的区别在于，全单包含机构、大户、中户和散户。而非全单必须选择某一种类型单独统计。\n净主动买入额主动指主动去成交已经存在的对手单；被动是指挂单等待成交。\n净主动买入额占比当日资金净流入金额 &#x2F; 流通市值，仅主动。\n净主动买入率(金额)即净流入率，当日净流入 &#x2F; 成交额，仅主动\n开盘买入额一般指9:30 ~ 10:00\n收盘买入额一般指14:30 ~ 15:00\n注：算法中根据挂单金额来划分超大单、大单、中单或小单。具体标准如下：\n1）挂单额小于4万元，小单；\n2）挂单额4万元到20万元之间，中单；\n3）挂单额20万元至100万元之间，大单；\n4）挂单额大于100万元，超大单。\n指数数据为当天未停牌的成份股买入金额总和。\n统计数据时，求占比时，如果是单只股票，就用量的占比。如果是指数或板块，就用总金额的占比。\n","categories":["金融"],"tags":["股票"]},{"title":"SpringBoot测试类用@Value获取server.port的值","url":"/2023/12/SpringBoot%E6%B5%8B%E8%AF%95%E7%B1%BB%E7%94%A8-Value%E8%8E%B7%E5%8F%96server-port%E7%9A%84%E5%80%BC/","content":"我使用的springboot版本：2.1.2.RELEASE，在测试类中，通过@Value获取server.port的值，得到值 -1。\n解决方法，可以在测试类上面加上如下注解：\n@RunWith(SpringRunner.class)@SpringBootTest(classes = MyApplication.class,        webEnvironment = SpringBootTest.WebEnvironment.DEFINED_PORT)","categories":["Java"],"tags":["springboot"]},{"title":"Hexo及hexo next theme升级","url":"/2022/02/Hexo%E5%8F%8Ahexo-next-theme%E5%8D%87%E7%BA%A7/","content":"升级安装Hexo下载源码Hexo和我使用的主题都一直在升级，我的版本还是比较老，存在很多兼容性问题。使用一些自动化部署到互联网也会遇到一些问题，今天干脆把Hexo及我使用的Hexo Next主题都升级到较新的版本。\n这里做一下记录，以便以后在升级当中会方便一些。\n升级到Hexo 5.4.1。\n我使用的是Source Tree，直接切换到 tag 5.4.1，如果是克隆，可以直接指定tag 5.4.1。\ngit clone git@github.com:hexojs/hexo.git new-blog -b 5.4.1\n安装$ npm install hexo-cli -g\n\n初始化Blog$ hexo init new-blogINFO  Cloning hexo-starter https://github.com/hexojs/hexo-starter.gitINFO  Install dependenciesINFO  Start blogging with Hexo!$ cd new-blog\n\n启动Blog$ hexo s\n\n安装Hexo Next主题安装Hexo Next主题的v8.10.0版本。\ngit clone git@github.com:next-theme/hexo-theme-next.git themes/next -b v8.10.0\n\n主题部分要改动的东西太多了，放弃了。以后有时间的时候再弄吧。\n","tags":["hexo","next theme"]},{"title":"Pandas中lambda表达式的应用","url":"/2022/02/Pandas%E4%B8%ADlambda%E8%A1%A8%E8%BE%BE%E5%BC%8F%E7%9A%84%E5%BA%94%E7%94%A8/","content":"在一些方法当中会使用到 axis, 一开始的时候会对这个参数的意义很模糊，现在罗列出来，做一下初步的讲解。\ndef func(x, y ):    return x + y    df[&#x27;col3&#x27;] = df.apply(lambda x: func(x[&#x27;col1] + x[&#x27;col2&#x27;]), axis = 1)\n\naxis默认为0，代表参数x为一列。axis&#x3D;1时，代表参数为一行。\n筛选数据时的与或非And_df = df[(df[&#x27;Rating&#x27;]&gt;5) &amp; (df[&#x27;Votes&#x27;]&gt;100000)]# 多个条件: OR - 满足评分高于5分或者投票大于100000的Or_df = df[(df[&#x27;Rating&#x27;]&gt;5) | (df[&#x27;Votes&#x27;]&gt;100000)]# 多个条件：NOT - 将满足评分高于5分或者投票大于100000的数据排除掉Not_df = df[~((df[&#x27;Rating&#x27;]&gt;5) | (df[&#x27;Votes&#x27;]&gt;100000))]\n\ndf[len(df[&#x27;Title&#x27;].split(&quot; &quot;))&gt;=5] # 报错 AttributeError: &#x27;Series&#x27; object has no attribute &#x27;split&#x27;\n\n很容易看得出来，df[‘Title’]是一列，是一个Series，所以没有split方法。\n是否可以用 df[&#39;Title&#39;].str.split(&quot; &quot;) ？？\n可以通过如下方法来解决。\n#创建一个新的列来存储每一影片名的长度df[&#x27;num_words_title&#x27;] = df.apply(lambda x : len(x[&#x27;Title&#x27;].split(&quot; &quot;)),axis=1)#筛选出影片名长度大于5的部分new_df = df[df[&#x27;num_words_title&#x27;]&gt;=5]\n\nx代表一行了，x[‘Title’]就是一个字符串了。\n复杂筛选筛选出那些影片的票房低于当年平均水平的数据。\n我们先要对每年票房的的平均值做一个归总\nyear_revenue_dict = df.groupby([&#x27;Year&#x27;]).agg(&#123;&#x27;Revenue(Millions)&#x27;:np.mean&#125;).to_dict()[&#x27;Revenue(Millions)&#x27;]\n\n然后我们定义一个函数来判断是否存在该影片的票房低于当年平均水平的情况，返回的是布尔值\ndef bool_provider(revenue, year):    return revenue&lt;year_revenue_dict[year]\n\n然后我们通过结合apply方法和lambda方法应用到数据集当中去\nnew_df = df[df.apply(lambda x : bool_provider(x[&#x27;Revenue(Millions)&#x27;], x[&#x27;Year&#x27;]), axis = 1)]\n\n方法调用过程的可视化from tqdm import tqdm, tqdm_notebooktqdm_notebook().pandas()df[&quot;CustomRating&quot;] = df.progress_apply(lambda x: custom_rating(x[&#x27;Genre&#x27;],x[&#x27;Rating&#x27;]),axis=1)","categories":["Python"],"tags":["pandas"]},{"title":"Jupyter notebook的github版本管理","url":"/2022/02/Jupyter-notebook%E7%9A%84github%E7%89%88%E6%9C%AC%E7%AE%A1%E7%90%86/","content":"通过 Jupyter Notebook 做数据研究不错，但版本控制是个问题。后来找到一个最佳实践。\n在保存ipynb文件之前，自动做一个ipynb转到py文件的转换，然后只把py文件提交到github上面。\n生成jupyter notebook配置文件jupyter notebook --generate-config\n\n运行后会生成 ~/.jupyter/ipython_notebook_config.py 文件\n编辑配置文件添如下内容：\n### If you want to auto-save .html and .py versions of your notebook:# modified from: https://github.com/ipython/ipython/issues/8009# Solution2: https://jupyter-notebook.readthedocs.io/en/stable/extending/savehooks.htmlimport osfrom subprocess import check_callimport redef clear_prompt(dir_path, nb_fname, log_func):    &quot;&quot;&quot;remove the number in &#x27;# In[ ]:&#x27;&quot;&quot;&quot;    name, ext = os.path.splitext(nb_fname)    pattern = re.compile(r&#x27;^# In\\[\\d+\\]:&#x27;)    for n_ext in [&#x27;.py&#x27;, &#x27;.txt&#x27;]:        script_name = os.path.join(dir_path, name+n_ext)        if os.path.exists(script_name):            new_lines = []            with open(script_name, &#x27;rt&#x27;, encoding=&#x27;utf-8&#x27;) as f:                lines = f.readlines()            for line in lines:                new_line = re.sub(pattern, &#x27;# In[ ]:&#x27;, line)                new_lines.append(new_line)            with open(script_name, &#x27;wt&#x27;, encoding=&#x27;utf-8&#x27;) as f:                f.writelines(new_lines)            log_func(&#x27;Remove number in &quot;# In[ ]:&quot;! File Name: %s&#x27; % script_name)            breakdef post_save(model, os_path, contents_manager):    &quot;&quot;&quot;post-save hook for converting notebooks to .py scripts&quot;&quot;&quot;    if model[&#x27;type&#x27;] != &#x27;notebook&#x27;:        return # only do this for notebooks    d, fname = os.path.split(os_path)    check_call([&#x27;jupyter&#x27;, &#x27;nbconvert&#x27;, &#x27;--to&#x27;, &#x27;script&#x27;, fname], cwd=d)  # &#x27;--no-prompt&#x27;,    log = contents_manager.log    # log.info(&#x27;Filename:%s&#x27;%fname)    clear_prompt(d, fname, log.info)    # check_call([&#x27;ipython&#x27;, &#x27;nbconvert&#x27;, &#x27;--to&#x27;, &#x27;html&#x27;, fname], cwd=d)c.FileContentsManager.post_save_hook = post_save\n\n重启jupyter notebook，配置生效。当保存ipynb文件时，会自动生成py文件。\n配置github的.gitignore文件*.ipynb\n\n设置以后，可能会发现规则没有生效。在项目根目录，执行如下命令：\ngit rm -r --cached .\n","categories":["Python"],"tags":["github","jupyter notebook"]},{"title":"安装动态条形图bar chart race","url":"/2022/02/%E5%AE%89%E8%A3%85%E5%8A%A8%E6%80%81%E6%9D%A1%E5%BD%A2%E5%9B%BEbar-chart-race/","content":"我是在Windows Miniconda环境下面安装的。\n安装官网地址：https://www.dexplo.org/bar_chart_race\n二选一：\npip install bar_chart_race\n\nconda install -c conda-forge bar_chart_race\n\n问题ffmpeg找不到Requested MovieWriter (ffmpeg) not available\nconda install -c conda-forge ffmpeg\n\n中文乱码import matplotlib as pltplt.rcParams[&#x27;font.sans-serif&#x27;]=&#x27;SimHei&#x27;\n\n也可以通过如下方法永久解决。\n找到python使用字库的位置\nimport matplotlib as plt# 查找字体路径print(plt.matplotlib_fname())# 查找字体缓存路径print(plt.get_cachedir())\n\n字体路径：C:\\ProgramData\\Miniconda3\\lib\\site-packages\\matplotlib\\mpl-data\\matplotlibrc\n打开后进行修改。\n去掉font.family前面的“#”，让该配置生效\n去掉font.sans-serif前面的“#”，让该配置生效，并且加入SimHei字体。\n删除缓存。\n重启IPYTHON\n数据的处理这里对dataframe的处理要比较熟悉。\n","categories":["Python"],"tags":["matplotlib","bar chart race"]},{"title":"Python模拟鼠标键盘操作","url":"/2022/02/Python%E6%A8%A1%E6%8B%9F%E9%BC%A0%E6%A0%87%E9%94%AE%E7%9B%98%E6%93%8D%E4%BD%9C/","content":"安装PyUserInput前，需要安装如下依赖：\nLinux - XlibMac - Quartz, AppKitWindows - pywin32, pyHook\npip install pywin32\n\nhttps://www.lfd.uci.edu/~gohlke/pythonlibs/\n安装pyHook，找到python对应的版本，比如：pyHook‑1.5.1‑cp37‑cp37m‑win_amd64.whl\n下载到本地，安装\npip install pyHook‑1.5.1‑cp37‑cp37m‑win_amd64.whl\n\n安装PyUserInput\npip install PyUserInput\n\nfrom pymouse import PyMousefrom pykeyboard import PyKeyboardm = PyMouse()k = PyKeyboard()\n\n调用api\nx_dim, y_dim = m.screen_size()m.click(x_dim/2, y_dim/2, 1)k.type_string(&#x27;Hello, World!&#x27;)# pressing a keyk.press_key(&#x27;H&#x27;)# which you then follow with a release of the keyk.release_key(&#x27;H&#x27;)# or you can &#x27;tap&#x27; a key which does bothk.tap_key(&#x27;e&#x27;)# note that that tap_key does support a way of repeating keystrokes with a interval time between eachk.tap_key(&#x27;l&#x27;,n=2,interval=5) # and you can send a string if needed took.type_string(&#x27;o World!&#x27;)#Create an Alt+Tab combok.press_key(k.alt_key)k.tap_key(k.tab_key)k.release_key(k.alt_key)k.tap_key(k.function_keys[5])  # Tap F5k.tap_key(k.numpad_keys[&#x27;Home&#x27;])  # Tap &#x27;Home&#x27; on the numpadk.tap_key(k.numpad_keys[5], n=3)  # Tap 5 on the numpad, thrice# Mac examplek.press_keys([&#x27;Command&#x27;,&#x27;shift&#x27;,&#x27;3&#x27;])# Windows examplek.press_keys([k.windows_l_key,&#x27;d&#x27;])# Windowsk.tap_key(k.alt_key)# Mack.tap_key(&#x27;Alternate&#x27;)\n\neg.\nfrom pymouse import PyMouseEventdef fibo():    a = 0    yield a    b = 1    yield b    while True:        a, b = b, a+b        yield bclass Clickonacci(PyMouseEvent):    def __init__(self):        PyMouseEvent.__init__(self)        self.fibo = fibo()    def click(self, x, y, button, press):        &#x27;&#x27;&#x27;Print Fibonacci numbers when the left click is pressed.&#x27;&#x27;&#x27;        if button == 1:            if press:                print(self.fibo.next())        else:  # Exit if any other mouse button used            self.stop()C = Clickonacci()C.run()","categories":["Python"],"tags":["pyuserinput"]},{"title":"pydub AudioSegment各种错误的处理","url":"/2024/12/pydub-AudioSegment%E5%90%84%E7%A7%8D%E9%94%99%E8%AF%AF%E7%9A%84%E5%A4%84%E7%90%86/","content":"执行pydub下AudioSegment的api, \nfrom pydub import AudioSegmentaudiofile = AudioSegment.from_file(&quot;data/music_8k.mp3&quot;)\n\n不管是from_file, from_mp3, from_ogg等等函数，都大概会遇到如下错误：\nRuntimeWarning: Couldn’t find ffprobe or avprobe - defaulting to ffprobe, but may not workwarn(“Couldn’t find ffprobe or avprobe - defaulting to ffprobe, but may not work”,RuntimeWarning)\n\n这时需要安装ffmpeg，mamba install ffmpeg。\n这时执行代码，遇到另一个错误：\nFileNotFoundError: [WinError 2] 系统找不到指定的文件。\n\n下载mpeg可执行文件，官方地址：https://www.gyan.dev/ffmpeg/builds/\nimport pydubpydub.AudioSegment.ffmpeg = &quot;/absolute/path/to/ffmpeg&quot;sound = AudioSegment.from_mp3(&quot;test.mp3&quot;)\n\n或者把ffmpeg文件放到python文件当前目录下面。\n","categories":["Python"],"tags":["python","pydub","AudioSegment"]},{"title":"Windows 10 VSCode配置Miniforge Python环境","url":"/2024/10/Windows-10-VSCode%E9%85%8D%E7%BD%AEMiniforge-Python%E7%8E%AF%E5%A2%83/","content":"在Windows 10操作系统上面安装Miniforge，同时配置VSCode用来开发python。\n执行文件时，可能会遇到如下错误：\n\n\n然后执行命令：\nGet-ExecutionPolicyRestricted\n\n执行如下命令解决：\nSet-ExecutionPolicy -ExecutionPolicy Unrestricted -Scope CurrentUser\n\n这时会变为Unrestricted。\n配置VSCode的Python开发环境打开 VSCode，按 Ctrl + Shift + P 打开命令面板。\n输入并选择 Python: Select Interpreter 命令。\n选择Python解释器\n配置VSCode使用Miniforge终端打开 VSCode，点击左下角齿轮图标，选择 设置。\n在搜索框中搜索 terminal.integrated.profiles.windows\n\n\n添加代码在settings.json中添加如下代码：\n&quot;terminal.integrated.defaultProfile.windows&quot;: &quot;Miniforge Prompt&quot;,&quot;terminal.integrated.profiles.windows&quot;: &#123;    &quot;PowerShell&quot;: &#123;        &quot;source&quot;: &quot;PowerShell&quot;,        &quot;icon&quot;: &quot;terminal-powershell&quot;    &#125;,    &quot;Command Prompt&quot;: &#123;        &quot;path&quot;: [            &quot;$&#123;env:windir&#125;\\\\Sysnative\\\\cmd.exe&quot;,            &quot;$&#123;env:windir&#125;\\\\System32\\\\cmd.exe&quot;        ],        &quot;args&quot;: [],        &quot;icon&quot;: &quot;terminal-cmd&quot;    &#125;,    &quot;Git Bash&quot;: &#123;        &quot;source&quot;: &quot;Git Bash&quot;    &#125;,    &quot;Miniforge Prompt&quot;: &#123;        &quot;path&quot;: &quot;$&#123;env:windir&#125;\\\\system32\\\\cmd.exe&quot;,        &quot;args&quot;: [&quot;/K&quot;, &quot;D:\\\\miniforge3\\\\Scripts\\\\activate.bat D:\\\\miniforge3&quot;]    &#125;&#125;,\n\n添加 Miniforge Prompt，path和args就照抄安装Miniforge时Miniforge Prompt的属性里面的东西。\nterminal.integrated.defaultProfile.windows改成新配置的Miniforge Prompt。\n","categories":["Python"],"tags":["python","vscode","miniforge"]},{"title":"Word制作身份证扫描件复印件","url":"/2024/10/Word%E5%88%B6%E4%BD%9C%E8%BA%AB%E4%BB%BD%E8%AF%81%E6%89%AB%E6%8F%8F%E4%BB%B6%E5%A4%8D%E5%8D%B0%E4%BB%B6/","content":"身份证拍得不好，拍出来并非长方形，背影有其他内容，都没有关系。按如下步骤操作就行。\n一、打开MICROSOFT OFFICE WORD软件\n二、插入用手机拍摄的身份证图片\n三、点击【图片格式】-&gt; 【裁剪】，把边缘裁剪干净，裁剪到身份证一小部分边缘都没关系\n四、选中图片，点击【颜色】，选择【饱和度】为0%，身份证颜色立刻变成灰色。\n五、选中图片，选择【剪切】\n六、接着插入一个圆角矩形，并右键鼠标，【设置形状格式】，选择【图片或纹理填充】，图片源选择【剪贴板】，这样就把刚才进入到剪切板的身份证插入。同时，线条选择【无线条】。选中【将图片平铺为纹理】。\n七、拖动图片左上方的小黄点，调整圆角的大小\n八、调整图片大小至身份证真实大小，高为5.4cm，宽为8.56cm\n这样就搞好了，身份证反面也可以同理这样制作。\n"},{"title":"Elasticsearch多因素排序","url":"/2022/01/Elasticsearch%E5%A4%9A%E5%9B%A0%E7%B4%A0%E6%8E%92%E5%BA%8F/","content":"查询语句没有经过测试，暂时做个记录。\n&#123;\t&quot;query&quot;: &#123;\t\t&quot;function_score&quot;: &#123;\t\t\t&quot;query&quot;: &#123;\t\t\t\t&quot;match&quot;: &#123;\t\t\t\t\t&quot;title&quot;: &quot;搜索内容&quot;\t\t\t\t&#125;\t\t\t&#125;,\t\t\t&quot;functions&quot;: [\t\t\t    &#123;\t\t\t\t\t&quot;script_score&quot;: &#123;\t\t\t\t\t\t&quot;script&quot;: &quot;return doc[&#x27;flag&#x27;].value == TRUE ? 1 : 0&quot;\t\t\t\t\t&#125;\t\t\t\t&#125;,\t\t\t\t&#123;\t\t\t\t\t&quot;gauss&quot;: &#123;\t\t\t\t\t\t&quot;start_time&quot;: &#123;\t\t\t\t\t\t\t&quot;origin&quot;: &quot;$now&quot;,\t\t\t\t\t\t\t&quot;offset&quot;: &quot;1w&quot;,\t\t\t\t\t\t\t&quot;scale&quot;: &quot;1m&quot;\t\t\t\t\t\t&#125;\t\t\t\t\t&#125;\t\t\t\t&#125;\t\t\t],\t\t\t&quot;score_mode&quot;: &quot;sum&quot;,\t\t\t&quot;boost_mode&quot;: &quot;multiply&quot;\t\t&#125;\t&#125;&#125;","categories":["Middleware","Elasticsearch"],"tags":["elasticsearch"]},{"title":"股市常用指标释义","url":"/2024/10/%E8%82%A1%E5%B8%82%E5%B8%B8%E7%94%A8%E6%8C%87%E6%A0%87%E9%87%8A%E4%B9%89/","content":"净买入额指定日内(超大单、大单、中单或小单)的买入金额-(超大单、大单、中单或小单)的卖出金额注：算法中根据挂单金额来划分超大单、大单、中单或小单。具体标准如下：1）挂单额小于4万元，小单；（散户）2）挂单额4万元到20万元之间，中单；（中户）3）挂单额20万元至100万元之间，大单；（大户）4）挂单额大于100万元，超大单。（机构）\n指数数据为当天未停牌的成份股净买入额总和。\n净主动买入额主动指主动去成交已经存在的对手单；被动是指挂单等待成交。\n主力净流入额主力买入金额 - 主力卖出金额，其中“主力”指超大单与大单的合计。\n"},{"title":"容易混淆的英文单词","url":"/2022/01/%E5%AE%B9%E6%98%93%E6%B7%B7%E6%B7%86%E7%9A%84%E8%8B%B1%E6%96%87%E5%8D%95%E8%AF%8D/","content":"villa n. 别墅cottage n. 小别墅；村舍，小屋\ncookery n. 烹饪术fishery n. 渔场\ninspiration n. 灵感；鼓舞人心的人或事aspiration n. \ndedicate v. 致力，献身dedicated adj. 专注的，投入的，专用的，献身的delicate adj. 脆弱的; 虚弱的; 纤细的; 精致的; 熟练的; 娇美的; 柔和的; 鲜美的; 清香的; 微妙的; 技巧性很强的delicately adv. 优美地，精致地，微妙地\ngum n. 口香糖gym n. 健身房; 体育馆; 室内健身操; 体操\nnoticeable adj. 明显的，显著的；引人注目的；值得注意的obvious adj.\n"},{"title":"Spring Boot集成Kafka客户端","url":"/2022/01/Spring-Boot%E9%9B%86%E6%88%90Kafka%E5%AE%A2%E6%88%B7%E7%AB%AF/","content":"Spring Boot 2.1.x集成Kafka，其他版本也是类似方法。\n确定Kafka版本在Spring Boot中添加Kafka Client依赖。\n&lt;dependency&gt;    &lt;groupId&gt;org.springframework.kafka&lt;/groupId&gt;    &lt;artifactId&gt;spring-kafka&lt;/artifactId&gt;&lt;/dependency&gt;\n\n进入此依赖，查看kafka-clients版本号。\n&lt;dependency&gt;    &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt;    &lt;artifactId&gt;kafka-clients&lt;/artifactId&gt;    &lt;version&gt;2.0.1&lt;/version&gt;    &lt;scope&gt;compile&lt;/scope&gt;&lt;/dependency&gt;\n\n所以在安装Kafka的时候，选择2.0.1版本就可以了。Scala版本选择最高版本就行。\n安装Kafka集群zookeeper单机，Kafka在同一台机器上安装两个实例。\n在config/server.properties里面修改如下属性。\nbroker.id=0\nBroker的id号，第一个实例为0，第二个实例为1。\nlisteners=PLAINTEXT://:9092\n第一个实例用默认端口9092，第二个实例用9093。\nadvertised.listeners=PLAINTEXT://&#123;host-ip&#125;:9092\n第一个实例用默认端口9092，第二个实例用9093。\n如果不设置上述暴露给生产者和消费者的地址和端口，可能会出现如下错误：\njava.io.IOException: Can&#x27;t resolve address\n\n网上有说到解决这个问题的办法是在hosts里面配置主机的ip地址，个人觉得这个办法不是最佳实践。\nlog.dirs=/tmp/kafka-logs\n单机时，这个文件夹得用不同的。\n启动zookeeper和kafka我就不在这里讲了。第二个实例启动后可以看到如下信息。\nINFO [KafkaServer id=1] started (kafka.server.KafkaServer)\n\nSpring Boot集成在application.yml里面需要配置如下：\nspring:  kafka:    bootstrap-servers:      - &#123;host-ip&#125;:9092      - &#123;host-ip&#125;:9093\n\n我们再看看默认配置的值。\n[16:38:24.889] INFO  org.apache.kafka.common.config.AbstractConfig 279 logAll - ProducerConfig values: \tacks = 1\tbatch.size = 16384\tbootstrap.servers = [&#123;host-ip&#125;:9092, &#123;host-ip&#125;:9093:9093]\tbuffer.memory = 33554432\tclient.id = \tcompression.type = none\tconnections.max.idle.ms = 540000\tenable.idempotence = false\tinterceptor.classes = []\tkey.serializer = class org.apache.kafka.common.serialization.StringSerializer\tlinger.ms = 0\tmax.block.ms = 60000\tmax.in.flight.requests.per.connection = 5\tmax.request.size = 1048576\tmetadata.max.age.ms = 300000\tmetric.reporters = []\tmetrics.num.samples = 2\tmetrics.recording.level = INFO\tmetrics.sample.window.ms = 30000\tpartitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner\treceive.buffer.bytes = 32768\treconnect.backoff.max.ms = 1000\treconnect.backoff.ms = 50\trequest.timeout.ms = 30000\tretries = 0\tretry.backoff.ms = 100\tsasl.client.callback.handler.class = null\tsasl.jaas.config = null\tsasl.kerberos.kinit.cmd = /usr/bin/kinit\tsasl.kerberos.min.time.before.relogin = 60000\tsasl.kerberos.service.name = null\tsasl.kerberos.ticket.renew.jitter = 0.05\tsasl.kerberos.ticket.renew.window.factor = 0.8\tsasl.login.callback.handler.class = null\tsasl.login.class = null\tsasl.login.refresh.buffer.seconds = 300\tsasl.login.refresh.min.period.seconds = 60\tsasl.login.refresh.window.factor = 0.8\tsasl.login.refresh.window.jitter = 0.05\tsasl.mechanism = GSSAPI\tsecurity.protocol = PLAINTEXT\tsend.buffer.bytes = 131072\tssl.cipher.suites = null\tssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]\tssl.endpoint.identification.algorithm = https\tssl.key.password = null\tssl.keymanager.algorithm = SunX509\tssl.keystore.location = null\tssl.keystore.password = null\tssl.keystore.type = JKS\tssl.protocol = TLS\tssl.provider = null\tssl.secure.random.implementation = null\tssl.trustmanager.algorithm = PKIX\tssl.truststore.location = null\tssl.truststore.password = null\tssl.truststore.type = JKS\ttransaction.timeout.ms = 60000\ttransactional.id = null\tvalue.serializer = class org.apache.kafka.common.serialization.StringSerializer\n\nJava代码\n@RestControllerpublic class KafkaProducer &#123;    @Autowired    private KafkaTemplate&lt;String, Object&gt; kafkaTemplate;    @GetMapping(&quot;/kafka/&#123;message&#125;&quot;)    public void sendMessage(@PathVariable String message) &#123;        kafkaTemplate.send(&quot;test-topic&quot;, message);    &#125;&#125;\n\n通过console-consumer就可以消费消息了。\n","categories":["Middleware","Kafka"],"tags":["spring boot","kafka"]},{"title":"MySQL实践操作技巧","url":"/2022/04/MySQL%E5%AE%9E%E8%B7%B5%E6%93%8D%E4%BD%9C%E6%8A%80%E5%B7%A7/","content":"同表，根据查找出来的主键id，进行删除，会报错。\n比如：\nDELETE FROM t_user WHERE id IN(    SELECT id FROM t_user WHERE name = &quot;Hello&quot;)\n\nError Code: 1093. You can&#39;t specify target table &#39;t_user&#39; for update in FROM clause\n解决方案：\n把查出来的结果生成一个临时表。\nDELETE FROM t_user WHERE id IN(    SELECT tmp.id FROM        (SELECT id FROM t_user WHERE name = &quot;Hello&quot;) tmp)\n\n但这时有可能还会报个错，提示这是不安全的操作方法。\nError Code: 1175. You are using safe update mode and you tried to update a table without a WHERE  that uses a KEY column. To disable safe mode, toggle the option in Preference -&gt; SQL Editor and reconnect.\n这时可以通过以下语句关掉安全阀。\nSET SQL_SAFE_UPDATES = 0;\n\n再次执行删除命令就可以成功执行了。\n","categories":["MySQL"],"tags":["mysql"]},{"title":"python中的模块加载","url":"/2022/04/python%E4%B8%AD%E7%9A%84%E6%A8%A1%E5%9D%97%E5%8A%A0%E8%BD%BD/","content":"Python模块加载的时候，有三个库需要区分一下。\n内置库系统库，可以通过如下方式查看其位置。\nimport osos.__file__/opt/anaconda3/envs/py310/lib/python3.10/os.py\n\n说明内置库在/opt/anaconda3/envs/py310/lib/python3.10/\n三方库第三方开发公司开发的库，有些也是官方提供的。\n可以通过如下方法查看库的位置。\nimport pippip.__file__/opt/anaconda3/envs/py310/lib/python3.10/site-packages/pip/__init__.py\n\n说明三方库的位置在/opt/anaconda3/envs/py310/lib/python3.10/site-packages/\n本地库有些库是自己开发的，通过设置PYTHONPATH来指定模块的位置。\n环境变量可以通过os.environ来查看。\n可以通过sys.path来动态添加地址。\n模块demo比如写个demo模块，首先创建一个demo文件夹。\n模块文件夹中，需要添加__init__.py，系统即可认定为模块。\n打包的时候，需要两个配置文件。这两个文件和demo文件夹在同一个目录下。\nsetup.py\nfrom setuptools import setupsetup()\n\nsetup.cfg\n[metadata]name = demoversion = 0.0.1author = Simonauthor_email = simon@email.comdescription = A demo modulelong_description = file: README.mdlong_description_content_type = text/markdownurl = https://finolo.gylicense = MITlicense_files = LICENSEkeywords =    quant    tradingclassifiers =    Development Status :: 5 - Production/Stable    Operating System :: Microsoft :: Windows    Operating System :: POSIX :: Linux    Operating System :: Darwin :: Macos    Programming Language :: Python :: 3    Programming Language :: Python :: 3.7    Programming Language :: Python :: 3.8    Programming Language :: Python :: 3.9    Programming Language :: Python :: 3.10    Topic :: Office/Business :: Financial :: Investment    Programming Language :: Python :: Implementation :: CPython    License :: OSI Approved :: MIT License    Natural Language :: Chinese (Simplified)project_urls =\tDocumentation = https://finolo.gy/[options]packages = find:include_package_data = Truezip_safe = Falseinstall_requires =    vnpy\n\n执行安装命令\npython setup.py install\n\n安装以后，demo模块就进入到site-packages里面去了。模块的名称可能是demo_module-0.0.1-py3.10.egg，以egg结尾，进入此目录后，会看到demo_module目录。这种方式即是我们所谓的源码安装。\n也可以通过wheel方式打包。\n首先需要安装模块，wheel\npip install wheel\n\npython setup.py bdist_wheel\n\n会生成一个文件：dist/demo_module-0.0.1-py3-none-any.whl\n这个时候发给别人，别人通过pip install就可以安装了。\n安装成功后，site-packages目录下面直接就是demo_module。\n","categories":["Python"],"tags":["python","模块"]},{"title":"Python带时区时间处理","url":"/2022/04/Python%E5%B8%A6%E6%97%B6%E5%8C%BA%E6%97%B6%E9%97%B4%E5%A4%84%E7%90%86/","content":"我们从外部获取到的时间数据，是str类型的。如果时间的时区为UTC+0时区，要转成东八区来显示，可以按如下方法来操作。\nfrom datetime import datetimeimport pytzformat = &quot;%Y-%m-%d %H:%M:%S&quot;date = datetime.strptime(&quot;2022-04-01 12:00:00&quot;, format)converted_date = date.replace(tzinfo=pytz.utc).astimezone(pytz.timezone(&quot;Asia/Shanghai&quot;))print(datetime.strftime(converted_date, format))\n\n如果是在DataFrame里，比如从Excel里面读出来的数据。可用下面方法。\nimport pandas as pddf = pd.DataFrame([&quot;2022-03-22 12:00:05&quot;, &quot;2022-03-22 13:10:05&quot;], columns=[&#x27;date&#x27;])df[&#x27;date2&#x27;] = pd.to_datetime(df[&#x27;date&#x27;], format=&quot;%Y-%m-%d %H:%M:%S&quot;)df[&#x27;date3&#x27;] = df[&#x27;date2&#x27;].dt.tz_localize(pytz.utc).dt.tz_convert(&quot;Asia/Shanghai&quot;).dt.strftime(&quot;%Y-%m-%d %H:%M:%S&quot;)df\n\n这个dt不能缺少，用于Datetime上面的。\ndf[&#x27;date2&#x27;].dt&lt;pandas.core.indexes.accessors.DatetimeProperties object at 0x7fbe14fd2f10&gt;\n","categories":["Python"],"tags":["pandas","datetime","timezone","时区"]},{"title":"幂函数的图像及变化趋势","url":"/2022/01/%E5%B9%82%E5%87%BD%E6%95%B0%E7%9A%84%E5%9B%BE%E5%83%8F%E5%8F%8A%E5%8F%98%E5%8C%96%E8%B6%8B%E5%8A%BF/","content":"\n"},{"title":"安装veighna3","url":"/2022/04/%E5%AE%89%E8%A3%85veighna3/","content":"这篇文章描述如何在macOS上面装Veighna 3.2.0。\n在mac机器上没法使用CTP，而tts提供了CTP的所有接口，所以这里记录一下如何在mac上安装veighna 3.2.0。\n首先要装ta-lib\npip install ta-lib\n\n然后\npip install -r requirements.txt\n\n最后，进入vnpy源码目录，安装\npip install . $@\n\n如果是Linux版本，只需\npip install .\n\nCTP没法安装，TTS模拟环境也没有办法安装，等后面发布mac版本。\npip install vnpy_tts\n\n下面文章讲的是veighna 3.0.0的安装流程，目前来看有点老了。\nmacOS在mac OS下面安装veighna 3.0.0，Linux类似。我使用的是anaconda下面的python。\n创建python 3.10的环境$ conda create -n py310 python=3.10\n\n激活py310环境。\n$ conda activate py310\n\n安装vnpy$ pip install -i https://pypi.tuna.tsinghua.edu.cn/simple vnpy\n\n不出意外，在安装ta-lib的时候会出错。\nxcrun: error: invalid active developer path (/Library/Developer/CommandLineTools), missing xcrun at: /Library/Developer/CommandLineTools/usr/bin/xcrun      error: command &#x27;/usr/bin/clang&#x27; failed with exit code 1      [end of output]  note: This error originates from a subprocess, and is likely not a problem with pip.  ERROR: Failed building wheel for ta-lib  Running setup.py clean for ta-libSuccessfully built deapFailed to build ta-libInstalling collected packages: ta-lib, pyzmq, PySide6, pyqtgraph, plotly, install, deap, seaborn, qdarkstyle, vnpy  Running setup.py install for ta-lib ... error  error: subprocess-exited-with-error\n\n解决mac OS xcrun error, missing xcrun\n\n然后重新安装vnpy，即可以成功安装。\n进入vnpy/example/veighna_trader/，执行run.py\n$ python run.py\n\n遇到如下问题：\nModuleNotFoundError: No module named &#39;importlib_metadata&#39;\n安装模块。\n$ pip install importlib_metadata\n\n最后可以成功启动veighna trader界面。不过mac OS无法加载CTP模块。\n国内市场用到的绝大部分量化交易接口（如CTP、飞马、飞创、恒生等），其开发商只提供了Windows和Linux的API开发包。没有Mac版本（无法直接使用）的同时，也没有提供API源代码（无法自行编译），因此这类接口在Mac系统上无法使用（强行加载会报错找不到C++的DLL文件）。但纯Python接口和REST&#x2F;WebSocket接口，由于不存在对于C++链接库的依赖，所以在Mac上均可以直接运行。\nWindows 10创建python3.10环境和上面相同。\n$ pip install -i https://mirrors.aliyun.com/pypi/simple vnpy\n\n安装ta-lib失败。\n按官网提供的安装方法安装。\npip install --extra-index-url http://139.196.190.180 --trusted-host 139.196.190.180 TA_LibSuccessfully installed TA-Lib-0.4.24\n\n重新执行上面的安装语句，可能会遇到一个错误，这个错误不会中断安装程序，很容易被忽略，而且最后还能显示安装成功。\n这个错误跟deap相关。\nWARNING: Discarding https://pypi.tuna.tsinghua.edu.cn/packages/af/29/e7f2ecbe02997b16a768baed076f5fc4781d7057cd5d9adf7c94027845ba/deap-1.2.2.tar.gz#sha256=95c63e66d755ec206c80fdb2908851c0bef420ee8651ad7be4f0578e9e909bcf (from https://pypi.tuna.tsinghua.edu.cn/simple/deap/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.  Using cached https://pypi.tuna.tsinghua.edu.cn/packages/b0/9c/a7404777a4cdf5857224051f2ae363d76c3d3b17d27b2caf12ed9bbe6d94/deap-1.2.1.tar.gz (936 kB)    ERROR: Command errored out with exit status 1:     command: &#x27;C:\\ProgramData\\Miniconda3\\envs\\py310\\python.exe&#x27; -c &#x27;import io, os, sys, setuptools, tokenize; sys.argv[0] = &#x27;&quot;&#x27;&quot;&#x27;C:\\\\Users\\\\Simon\\\\AppData\\\\Local\\\\Temp\\\\pip-install-w5an0dek\\\\deap_f8336f0a0f4e439d85daec3d2c91d4cb\\\\setup.py&#x27;&quot;&#x27;&quot;&#x27;; __file__=&#x27;&quot;&#x27;&quot;&#x27;C:\\\\Users\\\\Simon\\\\AppData\\\\Local\\\\Temp\\\\pip-install-w5an0dek\\\\deap_f8336f0a0f4e439d85daec3d2c91d4cb\\\\setup.py&#x27;&quot;&#x27;&quot;&#x27;;f = getattr(tokenize, &#x27;&quot;&#x27;&quot;&#x27;open&#x27;&quot;&#x27;&quot;&#x27;, open)(__file__) if os.path.exists(__file__) else io.StringIO(&#x27;&quot;&#x27;&quot;&#x27;from setuptools import setup; setup()&#x27;&quot;&#x27;&quot;&#x27;);code = f.read().replace(&#x27;&quot;&#x27;&quot;&#x27;\\r\\n&#x27;&quot;&#x27;&quot;&#x27;, &#x27;&quot;&#x27;&quot;&#x27;\\n&#x27;&quot;&#x27;&quot;&#x27;);f.close();exec(compile(code, __file__, &#x27;&quot;&#x27;&quot;&#x27;exec&#x27;&quot;&#x27;&quot;&#x27;))&#x27; egg_info --egg-base &#x27;C:\\Users\\Simon\\AppData\\Local\\Temp\\pip-pip-egg-info-jymbv_bj&#x27;         cwd: C:\\Users\\Simon\\AppData\\Local\\Temp\\pip-install-w5an0dek\\deap_f8336f0a0f4e439d85daec3d2c91d4cb\\    Complete output (1 lines):    error in deap setup command: use_2to3 is invalid.\n\n官方也提供了解决方案。\n查看setuptools版本。\npip list | findstr setuptoolssetuptools      58.0.4\n\n需要把setuptools降低到57版本。\npip install setuptools==57.0.0Collecting setuptools==57.0.0  Downloading setuptools-57.0.0-py3-none-any.whl (821 kB)     |████████████████████████████████| 821 kB 63 kB/sInstalling collected packages: setuptools  Attempting uninstall: setuptools    Found existing installation: setuptools 58.0.4    Uninstalling setuptools-58.0.4:      Successfully uninstalled setuptools-58.0.4Successfully installed setuptools-57.0.0\n\n最近再安装vnpy，就可以安装成功了，如果运行时需要什么模块，再安装就可以了。\n需要注意的是，如果使用VSCode来编辑python文件，在使用终端命令中，类型一定要选择\n","tags":["vnpy","veighna"]},{"title":"融资余额、融券余额的计算公式","url":"/2022/04/%E8%9E%8D%E8%B5%84%E4%BD%99%E9%A2%9D%E3%80%81%E8%9E%8D%E5%88%B8%E4%BD%99%E9%A2%9D%E7%9A%84%E8%AE%A1%E7%AE%97%E5%85%AC%E5%BC%8F/","content":"关于证券融资融券里面的几个概念，很容易混淆。这里把它们都清楚的讲一下。\n融资融资主要在于资，即金额。\n融资买入额当日融资买入的金额\n融资偿还额当日融资偿还金额\n当日净买入额上面两个参数之差\n融资余额当日融资余额 = 前日融资余额 + 当日融资买入额 - 当日融资偿还额\n（融资余额是指每天收盘后累计发生的向券商借钱买股票尚未偿还的资金）\n融券融券在于券，即股数。\n主要看借了多少券，跟数量有关。\n融券卖出量融券偿还量当日净卖出量净卖出量 &#x3D; 融券卖出量 - 融券偿还量\n融券余额本日融券余量余额 &#x3D; (前日融券余量 + 当日融券卖出数量 - 当日融券偿还量) * 最新收盘价\n融资融券余额融资融券余额 &#x3D; 融资余额 + 融券余额\n如果融资融券余额增加，预示着股价的波动会加大。\n","tags":["融资","融券"]},{"title":"Android手机存储空间不足及微信数据备份","url":"/2022/03/Android%E6%89%8B%E6%9C%BA%E5%AD%98%E5%82%A8%E7%A9%BA%E9%97%B4%E4%B8%8D%E8%B6%B3%E5%8F%8A%E5%BE%AE%E4%BF%A1%E6%95%B0%E6%8D%AE%E5%A4%87%E4%BB%BD/","content":"电脑和pad我都是使用的iOS的，不过手机我却一直在用Android。并不是说苹果手机不好，我想以更加开放的心态，接受更多优秀的事物。\n我目前使用的手机是小米Mi 9，存储一共128G。用了几年，发现有个问题，手机存储不够了。\n微信和QQ占用的存储非常大，大几十个G。有时候通过百度网盘看一些视频，所以缓存也非常大。\n之前我都是手工删除一些大文件，但微信占用空间的越来越大，已经影响到我使用手机的效率了，所以必须要彻底清理一下。\n备份微信数据微信数据分为聊天数据和非聊天数据（图片、视频和其他文档）\n备份非聊天数据通过USB把手机连到mac上，使用Commander One软件，把如下几个文件夹的文件拷出来备份。\nAndroid/data/com.tencent.mm/MicroMsg/Download\nPictures/WeiXin\n下面两个是老版本的微信放文件的地址\ntencent/MicroMsg/Download\ntencent/MicroMsg/WeiXin\n备份聊天数据装一个微信电脑版本，上面有备份聊天记录的功能。\n微信还有一个自带的清除缓存的功能，可以降低很多空间，而且聊天记录还保存着，只是历史的图片打不开了。\n","categories":["Tools"]},{"title":"GlusterFS Java API使用","url":"/2022/03/GlusterFS-Java-API%E4%BD%BF%E7%94%A8/","content":"我们在使用官方提供的GlusterFS Java API，可能会遇到很多坑。\nException in thread &quot;main&quot; java.lang.UnsatisfiedLinkError: Could not load library. Reasons: [no libgfapi-jni64-1.0.5-SNAPSHOT in java.library.path, no libgfapi-jni-1.0.5-SNAPSHOT in java.library.path, no libgfapi-jni in java.library.path, /tmp/liblibgfapi-jni-64-1-5094135110056410243.0: libgfapi.so.0: cannot open shared object file: No such file or directory]\tat org.fusesource.hawtjni.runtime.Library.doLoad(Library.java:182)\tat org.fusesource.hawtjni.runtime.Library.load(Library.java:140)\tat com.peircean.libgfapi_jni.internal.GLFS.&lt;clinit&gt;(GLFS.java:52)\tat com.peircean.glusterfs.GlusterFileSystemProvider.glfsNew(GlusterFileSystemProvider.java:70)\tat com.peircean.glusterfs.GlusterFileSystemProvider.newFileSystem(GlusterFileSystemProvider.java:43)\tat com.peircean.glusterfs.GlusterFileSystemProvider.getPath(GlusterFileSystemProvider.java:111)\tat java.nio.file.Paths.get(Paths.java:143)\tat com.peircean.glusterfs.example.Example.main(Example.java:43)\n\n","categories":["FileSystem"],"tags":["GlusterFS"]},{"title":"Linux进程端口互查","url":"/2022/03/Linux%E8%BF%9B%E7%A8%8B%E7%AB%AF%E5%8F%A3%E4%BA%92%E6%9F%A5/","content":"Linux下通过进程名查看进程id pid\nps -ef | grep QuorumPeerMain\n\n通过netstat, pid和port可以互查。\np - pidn - numerica - all\nnetstat -anp | grep [pid/port]","categories":["Tools"],"tags":["netstat","ps"]},{"title":"解决mac OS xcrun error, missing xcrun","url":"/2022/04/%E8%A7%A3%E5%86%B3mac-OS-xcrun-error-missing-xcrun/","content":"在mac OS安装ta-lib时，遇到xcrun error。\nxcrun: error: invalid active developer path (/Library/Developer/CommandLineTools), missing xcrun at: /Library/Developer/CommandLineTools/usr/bin/xcrun      error: command &#x27;/usr/bin/clang&#x27; failed with exit code 1      [end of output]  note: This error originates from a subprocess, and is likely not a problem with pip.  ERROR: Failed building wheel for ta-lib\n\n执行命令\n$ xcode-select --install\n\n会跳出如下窗口，点击安装。\n\n\n即解决missing xcrun的问题。\n","tags":["xcrun"]},{"title":"MySQL表名作为变量的查询","url":"/2022/03/MySQL%E8%A1%A8%E5%90%8D%E4%BD%9C%E4%B8%BA%E5%8F%98%E9%87%8F%E7%9A%84%E6%9F%A5%E8%AF%A2/","content":"在应用中，很多会用到数据库的存储过程，一般都会在程序里面实现数据库的逻辑。\n但有时候要快速的获取某个值，逻辑比较复杂的时候，可能会用到存储过程。\n比如接下来的案例，会把数据库的表作为一个变量传入。\nDROP PROCEDURE IF EXISTS getInfo;DELIMITER $$CREATE PROCEDURE getInfo(IN tableName VARCHAR(100), IN id INT, OUT name VARCHAR(100))BEGIN\tSET @stmt := CONCAT(&quot;SELECT name INTO @var FROM &quot;, tableName, &quot; WHERE id = &quot;, id);\tPREPARE stmt FROM @stmt;\tEXECUTE stmt;    DEALLOCATE PREPARE stmt;    SET name := @var;END$$DELIMITER ;\n\n当创建好存储过程以后，可以通过以下方式调用。\nSET @tableName := &quot;tb_user&quot;;CALL getInfo(@tableName, 4, @name);SELECT @name;\n\n如果不写存储过程，还可以通过执行下面代码的方式获取结果。\nSET @user := 123456;SELECT @group := `group` FROM user WHERE user = @user;SELECT * FROM user WHERE `group` = @group;\n\nSET @group = (SELECT `group` FROM user WHERE user = @user);\n\n或者\nSELECT `group` INTO @group FROM user WHERE user = @user;\n","categories":["MySQL"],"tags":["mysql"]},{"title":"Python Pandas迷惑的axis参数","url":"/2022/03/Python-Pandas%E8%BF%B7%E6%83%91%E7%9A%84axis%E5%8F%82%E6%95%B0/","content":"pandas中drop和apply方法里的axis参数，一开始的时候会让人觉得有些迷惑。国内的很多文章说这两个方法axis参数弄反了。\n我之前也是这样理解的，觉得这明显有冲突，后来才发现是我理解错了。老外的设计是没有问题的。\n\n\naxis = 0 表明 apply 一个方法到 列 column 或 行标签(index)axis = 1 表明 apply 一个方法到 行 row 或 列标签(column labels)\n通过这个定义，大家应该很清楚的知道apply(axis= ) 和drop(axis = )的axis参数怎么填了。\n","categories":["Python","Pandas"],"tags":["pandas","axis"]},{"title":"java 8 stream groupingBy分组","url":"/2022/03/java-8-stream-groupingBy%E5%88%86%E7%BB%84/","content":"有时候需要在程序里面做分组操作，类似MySQL中的group by语句。\n比如有如下数据：\n\n\n\nid\nname\n\n\n\n1\nA\n\n\n1\nB\n\n\n2\nC\n\n\n我们想把它分组成这样的形式：{1&#x3D;[A, B], 2&#x3D;[C]}\npublic class GroupingByDemo &#123;    @Data    static class IdNamePair &#123;        private int id;        private String name;    &#125;    public static void main(String[] args) &#123;        IdNamePair pair = new IdNamePair();        pair.id = 1;        pair.name = &quot;A&quot;;        IdNamePair pair2 = new IdNamePair();        pair2.id = 1;        pair2.name = &quot;B&quot;;        IdNamePair pair3 = new IdNamePair();        pair3.id = 2;        pair3.name = &quot;C&quot;;        List&lt;IdNamePair&gt; list = new ArrayList&lt;&gt;();        list.add(pair);        list.add(pair2);        list.add(pair3);        Map&lt;Integer, List&lt;String&gt;&gt; map = list.stream()                .collect(Collectors.groupingBy(IdNamePair::getId,                        Collectors.mapping(IdNamePair::getName, Collectors.toList())));                System.out.println(map);    &#125;&#125;\n\nCollectors.mapping(IdNamePair::getName, Collectors.toList()) 相当于再往里面拨一层，取对象里面的一个属性。\n","categories":["Java"],"tags":["java8"]},{"title":"TCP/IP相关初步认识","url":"/2022/03/TCP-IP%E7%9B%B8%E5%85%B3%E5%88%9D%E6%AD%A5%E8%AE%A4%E8%AF%86/","content":"nc 用于和服务器建立连接。\n在Ubuntu上面做实验\n启动一个终端\n$ nc www.baidu.com 80\n\n再开启另一个终端\n$ netstat -antp\n\nProto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name    tcp        0      0 0.0.0.0:111             0.0.0.0:*               LISTEN      -                   tcp        0      0 127.0.0.53:53           0.0.0.0:*               LISTEN      -                   tcp        0      0 0.0.0.0:22              0.0.0.0:*               LISTEN      -                   tcp        0      0 127.0.0.1:631           0.0.0.0:*               LISTEN      -                   tcp        0      0 127.0.0.1:39351         0.0.0.0:*               LISTEN      -                   tcp        0      0 172.16.64.227:33574     111.3.88.228:80         TIME_WAIT   -                   tcp        0      0 172.16.64.227:48534     36.152.44.95:80         ESTABLISHED 3366/nc \n\n可以看到最后一行，已经建立了一个TCP连接。\n在刚才连接好百度服务器的终端上面，按http协议，请求百度首页。GET / HTTP/1.0并连续输入两个回车。\n$ nc www.baidu.com 80GET / HTTP/1.0HTTP/1.0 200 OKAccept-Ranges: bytesCache-Control: no-cacheContent-Length: 9508Content-Type: text/htmlDate: Sun, 20 Mar 2022 05:48:08 GMTP3p: CP=&quot; OTI DSP COR IVA OUR IND COM &quot;P3p: CP=&quot; OTI DSP COR IVA OUR IND COM &quot;Pragma: no-cacheServer: BWS/1.1Set-Cookie: BAIDUID=D9BC4FD3680ED7E670988E2B1D948396:FG=1; expires=Thu, 31-Dec-37 23:55:55 GMT; max-age=2147483647; path=/; domain=.baidu.comSet-Cookie: BIDUPSID=D9BC4FD3680ED7E670988E2B1D948396; expires=Thu, 31-Dec-37 23:55:55 GMT; max-age=2147483647; path=/; domain=.baidu.comSet-Cookie: PSTM=1647755288; expires=Thu, 31-Dec-37 23:55:55 GMT; max-age=2147483647; path=/; domain=.baidu.comSet-Cookie: BAIDUID=D9BC4FD3680ED7E6F640384383D296BE:FG=1; max-age=31536000; expires=Mon, 20-Mar-23 05:48:08 GMT; domain=.baidu.com; path=/; version=1; comment=bdTraceid: 1647755288023296769011190918765782309790Vary: Accept-EncodingX-Frame-Options: sameoriginX-Ua-Compatible: IE=Edge,chrome=1&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt;&lt;meta http-equiv=&quot;Content-Type&quot; content=&quot;text/html; charset=UTF-8&quot;&gt;&lt;meta http-equiv=&quot;X-UA-Compatible&quot; content=&quot;IE=edge,chrome=1&quot;&gt;&lt;meta content=&quot;always&quot; name=&quot;referrer&quot;&gt;&lt;meta name=&quot;description&quot; content=&quot;全球领先的中文搜索引擎、致力于让网民更便捷地获取信息，找到所求。百度超过千亿的中文网页数据库，可以瞬间找到相关的搜索结果。&quot;&gt;...\n\n写一个SocketDemo Server端程序，开启一个监听。\n-ff 追踪线程和子线程-o output\n$ strace -ff -o ./out java SocketDemo \n\n通过jps或netstat可以看到一个pid\n$ cd /proc/[pid]\n\n在fd目录下面，可以看到开启了多少个IO。\nio流&#x2F;dev&#x2F;pts&#x2F;0 标准输入&#x2F;dev&#x2F;pts&#x2F;1 标准输出&#x2F;dev&#x2F;pts&#x2F;2 标准报错\nulimit -a\n\nopen files 1024\n在task目录，显示所有的线程\n追踪一下redis的多路复用\n$ strace -ff -o ./redis-out bin/redis-server config/server.properties\n\n通过netstat查看进程监听状态通过&#x2F;proc&#x2F;[pid]查看 fd 和 task，可以看到redis是多线程的。\n","tags":["tcp"]},{"title":"json优美打印","url":"/2022/03/json%E4%BC%98%E7%BE%8E%E6%89%93%E5%8D%B0/","content":"打印日志时，一个对象比较大的时候，json字符串比较长，阅读性不强。\n这时可以通过一些工具，把json格式化，美化一下，能更方便的阅读。\njackson方式mapper.writerWithDefaultPrettyPrinter().writeValueAsString(obj);\n\nGson方式Gson gson = new GsonBuilder().setPrettyPrinting().create();gson.toJson(obj);\n\n","tags":["json"]},{"title":"jupyter lab plotly画图空白不显示的问题","url":"/2022/03/jupyter-lab-plotly%E7%94%BB%E5%9B%BE%E7%A9%BA%E7%99%BD%E4%B8%8D%E6%98%BE%E7%A4%BA%E7%9A%84%E9%97%AE%E9%A2%98/","content":"使用jupyter lab画图时，图片不显示而显示空白。\n\n说一下目前最新版本的处理情况。\npython base环境\npython                    3.12.5jupyterlab                4.2.5\n\npython env py310环境\npython                    3.10.15plotly                    5.20.0\n\n在py310环境中安装了plotly以后，执行代码时，可能会遇到如下错误：\nValueError: Mime type rendering requires nbformat&gt;=4.2.0 but it is not installed\n\n按提示安装nbformat，mamba install nbformat，重启Kernel。再次执行代码时，程序不报错了，但图形位置会显示为空白。\n本来只需要在base环境再安装jupyterlab-plotly插件就行了，但会报错，提示版本不匹配。所以解决方案，就是在base环境再安装一下相同版本号的plotly。安装好了以后，再查看插件jupyterlab-plotly，就已经同时安装成功了。\n$ jupyter labextension listJupyterLab v4.2.5/Users/simon/miniforge3/share/jupyter/labextensions        jupyterlab_pygments v0.3.0 enabled OK (python, jupyterlab_pygments)        jupyterlab-plotly v5.24.1 enabled  X 🔒 (all plugins locked)        @jupyter-widgets/jupyterlab-manager v5.0.13 enabled OK (python, jupyterlab_widgets)\n\n这时，重新启动jupyter lab，plotly的图形就可以正常显示了。\n如果要安装@jupyter-widgets/jupyterlab-manager v5.0.13 enabled OK (python, jupyterlab_widgets)，执行下面命令：\nmamba install ipywidgets\n\n\n首先要确保的是以下两个库jupyterlab和ipywidgets已经安装完成，并且版本”jupyterlab&gt;&#x3D;3” “ipywidgets&gt;&#x3D;7.6”。\n需要安装一个扩展工具。一般来说，JupyterLab 3.x以上，会自动安装好，并且是安装在JupyterLab被安装的Python环境下面。\n$ jupyter labextension install @jupyter-widgets/jupyterlab-manager jupyterlab-plotly\n\n安装完可以查看一下。\n# Check that jupyterlab-plotly is installed$ jupyter labextension list\n\n这个工具需要依赖nodejs，如果没有安装的话，需要安装一下。\n$ mamba install nodejs\n\n重启jupyter lab，图片即可正常显示。\n有几个点需要注意的是：\njupyterlab以及jupyterlab的extension是安装在server端的，但是plotly包是安装在其他python环境里面的。\n\n","categories":["Python"],"tags":["jupyter lab","plotly"]},{"title":"pip国内源配置","url":"/2022/03/pip%E5%9B%BD%E5%86%85%E6%BA%90%E9%85%8D%E7%BD%AE/","content":"通过pip安装软件，使用默认源的时候，往往下载速度很慢，所以需要配置到国内的源。\n临时安装某个包。\npip install -i https://pypi.tuna.tsinghua.edu.cn/simple some-package\n\n设为默认配置：\npip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple\n\n会生成一个配置文件，在macOS下面的位置如下：\n~/.config/pip/pip.conf\n\n内容为：\n[global]index-url = https://pypi.tuna.tsinghua.edu.cn/simple\n\n再更新pip本身：\npip install pip -U    # 更新pip\n\n临时使用某个镜像。\npip install -i https://pypi.tuna.tsinghua.edu.cn/simple pip -U\n","categories":["Python"],"tags":["pip"]},{"title":"python plotly画两个y轴","url":"/2022/03/python-plotly%E7%94%BB%E4%B8%A4%E4%B8%AAy%E8%BD%B4/","content":"使用 plotly 画图的时候，如果有两个指标的数值相差很大时，这时候需要y轴拆分成两个，这样能更清楚的看到两个指标的关系。\n下面代码展示了跨期价差套利的图像。\n\n\nimport plotly.graph_objects as gofrom plotly.subplots import make_subplotsih2106 = go.Scatter(    x=data.index,    y=data[&quot;IH2106&quot;],    name=&quot;IH2106&quot;)ih2109 = go.Scatter(    x=data.index,    y=data[&quot;IH2109&quot;],    name=&quot;IH2109&quot;)spread = go.Scatter(    x=data.index,    y=data[&quot;spread&quot;],    name=&quot;spread&quot;)fig = make_subplots(specs=[[&#123;&quot;secondary_y&quot;: True&#125;]])fig.add_trace(ih2106)fig.add_trace(ih2109)fig.add_trace(spread, secondary_y=True)\n\n","categories":["Python"],"tags":["plotly"]},{"title":"mac录屏带内置声音","url":"/2022/05/mac%E5%BD%95%E5%B1%8F%E5%B8%A6%E5%86%85%E7%BD%AE%E5%A3%B0%E9%9F%B3/","content":"使用mac自带的quicktime player录屏，比如录视频，视频的声音是录不下来的。如果要录视频和内置声音，设置还有些麻烦。\n网上有些人推荐Sound Flower，我也尝试用了，的确能完成录屏，但有个很不好的体验就是，你在录屏的时候，你是没法听到声音的，也就是说没法监控，你得等录制结束后，才知道有没有录制成功。\n最佳方案是使用Loopback软件。网上很多人设置得很繁琐，虽然也能用，但不是最佳实践。\n下载地址：https://www.macwk.com/soft/loopback，目前为止是可以下载的，以后能不能下载就不知道了。\n安装以后，按如下图方法设置。\n如果在录屏的时候，用耳机在监听，那就在Monitors那里添加一个耳机输出设备。\n\n\n设置好了，以后，播放音乐，可以看到声浪。\n进入音频设备设置。\n把Loopback Audio Playing设备设置为输出。\n\n\n打开Mac自带的quicktime player，在录屏设置的时候，需要选择Loopback Audio Playing。\n\n\n这时就可以录屏时带内置声音了。\n","categories":["Tools"],"tags":["录屏"]},{"title":"解决cannot import name 'Qsci' from 'PyQt5'","url":"/2022/03/%E8%A7%A3%E5%86%B3cannot-import-name-Qsci-from-PyQt5/","content":"Windows 10 安装 vnpy 2.7.0，报错：\nTraceback (most recent call last):  File &quot;run.py&quot;, line 5, in &lt;module&gt;    from vnpy.trader.ui import MainWindow, create_qapp  File &quot;C:\\ProgramData\\Miniconda3\\lib\\site-packages\\vnpy\\trader\\ui\\__init__.py&quot;, line 11, in &lt;module&gt;    from .mainwindow import MainWindow  File &quot;C:\\ProgramData\\Miniconda3\\lib\\site-packages\\vnpy\\trader\\ui\\mainwindow.py&quot;, line 28, in &lt;module&gt;    from .editor import CodeEditor  File &quot;C:\\ProgramData\\Miniconda3\\lib\\site-packages\\vnpy\\trader\\ui\\editor.py&quot;, line 4, in &lt;module&gt;    from PyQt5 import QtWidgets, Qsci, QtGuiImportError: cannot import name &#x27;Qsci&#x27; from &#x27;PyQt5&#x27; (C:\\ProgramData\\Miniconda3\\lib\\site-packages\\PyQt5\\__init__.py)\n\n解决方案：\npip install QScintilla\n","categories":["Python"],"tags":["vnpy","pyqt5"]},{"title":"CentOS L2TP/IPSEC服务搭建","url":"/2022/06/CentOS-L2TP-IPSEC%E6%9C%8D%E5%8A%A1%E6%90%AD%E5%BB%BA/","content":"L2TP Layer 2 Tunneling Protocol\n在CentOS 7虚拟机环境中，搭建一个L2TP服务。\n设置虚拟机网络VMware连接方式改为桥接模式，并设定IP地址为静态方式。\nvi /etc/sysconfig/network-scripts/ifcfg-ens33\n\n修改如下内容：\nBOOTPROTO由之前的dhcp改为static\nIPADDR就填自己的内网地址\nDNS改为宿主机的DNS\nping www.163.com如果通的，表示可以联网了。\n安装软件包清缓存\n# yum clean all\n\n建缓存\n# yum makecache\n\n# yum install -y make gcc gmp-devel xmlto bison flex libpcap-devel# yum install -y epel-release# yum install xl2tpd# yum install libreswan\n\n\n# vi /etc/ipsec.confconfig setup        # Normally, pluto logs via syslog.        #logfile=/var/log/pluto.log        #        # Do not enable debug options to debug configuration issues!        #        # plutodebug=&quot;control parsing&quot;        # plutodebug=&quot;all crypt&quot;        plutodebug=none        #        # NAT-TRAVERSAL support        # exclude networks used on server side by adding %v4:!a.b.c.0/24        # It seems that T-Mobile in the US and Rogers/Fido in Canada are        # using 25/8 as &quot;private&quot; address space on their wireless networks.        # This range has never been announced via BGP (at least up to 2015)        nat_traversal=yes        virtual_private=%v4:10.0.0.0/8,%v4:192.168.0.0/16,%v4:172.16.0.0/12,%v4:25.0.0.0/8,%v4:100.64.0.0/10,%v6:fd00::/8,%v6:fe80::/10\n\n新建文件\n# vi /etc/ipsec.d/l2tp_psk.confconn L2TP-PSK-NAT    rightsubnet=vhost:%priv    also=L2TP-PSK-noNATconn L2TP-PSK-noNAT    authby=secret    pfs=no    auto=add    keyingtries=3    dpddelay=30    dpdtimeout=120    dpdaction=clear    rekey=no    ikelifetime=8h    keylife=1h    type=transport    # 192.168.1.40 服务器内网网卡IP地址, ens33的地址    left=192.168.1.40    leftprotoport=17/1701    right=%any    rightprotoport=17/%any\n\n新建预共享密钥\n# vi /etc/ipsec.d/ipsec.secrets\n\n服务器内网网卡IP地址, ens33的地址\n#include /etc/ipsec.d/*.secrets192.168.1.40 %any: PSK &quot;123456&quot;\n\n启动加密服务\nsystemctl enable ipsecsystemctl start ipsec\n\n# ipsec verifyVerifying installed system and configuration filesVersion check and ipsec on-path                   \t[OK]Libreswan 3.25 (netkey) on 3.10.0-862.el7.x86_64Checking for IPsec support in kernel              \t[OK] NETKEY: Testing XFRM related proc values         ICMP default/send_redirects              \t[NOT DISABLED]  Disable /proc/sys/net/ipv4/conf/*/send_redirects or NETKEY will act on or cause sending of bogus ICMP redirects!         ICMP default/accept_redirects            \t[NOT DISABLED]  Disable /proc/sys/net/ipv4/conf/*/accept_redirects or NETKEY will act on or cause sending of bogus ICMP redirects!         XFRM larval drop                         \t[OK]Pluto ipsec.conf syntax                           \t[OK]Two or more interfaces found, checking IP forwarding\t[OK]Checking rp_filter                                \t[ENABLED] /proc/sys/net/ipv4/conf/all/rp_filter            \t[ENABLED] /proc/sys/net/ipv4/conf/default/rp_filter        \t[ENABLED] /proc/sys/net/ipv4/conf/ens33/rp_filter          \t[ENABLED] /proc/sys/net/ipv4/conf/ip_vti0/rp_filter        \t[ENABLED]  rp_filter is not fully aware of IPsec and should be disabledChecking that pluto is running                    \t[OK] Pluto listening for IKE on udp 500               \t[OK] Pluto listening for IKE/NAT-T on udp 4500        \t[OK] Pluto ipsec.secret syntax                        \t[OBSOLETE]  003 WARNING: using a weak secret (PSK)Checking &#x27;ip&#x27; command                             \t[OK]Checking &#x27;iptables&#x27; command                       \t[OK]Checking &#x27;prelink&#x27; command does not interfere with FIPS\t[OK]Checking for obsolete ipsec.conf options          \t[OBSOLETE KEYWORD] Warning: ignored obsolete keyword &#x27;nat_traversal&#x27;ipsec verify: encountered 11 errors - see &#x27;man ipsec_verify&#x27; for help\n\n修改内核\n# vi /etc/sysctl.confnet.ipv4.ip_forward = 1net.ipv4.conf.all.send_redirects = 0net.ipv4.conf.all.accept_redirects = 0net.ipv4.conf.all.rp_filter = 0net.ipv4.conf.default.send_redirects = 0net.ipv4.conf.default.accept_redirects = 0net.ipv4.conf.default.rp_filter = 0net.ipv4.conf.ens33.rp_filter = 0net.ipv4.conf.ip_vti0.rp_filte = 0\n\n# sysctl -p\n\n再ipsec verify时，已经成功生效\n修改l2tp配置文件\n# vi /etc/xl2tpd/xl2tpd.conf[global]; 服务器内网网卡IP地址, ens33的地址listen-addr = 192.168.1.40ipsec saref = yes[lns default]; ip range 客户端连接到此服务器后分配为客户端的ip地址范围ip range = 192.168.1.50-192.168.1.60local ip = 192.168.1.99\n\n\n修改xl2tpd属性配置文件\n# vi /etc/ppp/options.xl2tpdrequire-mschap-v2ms-dns  192.168.1.1\n\n添加用户名\n# vi /etc/ppp/chap-secrets\n\n# Secrets for authentication using CHAP# client        server  secret                  IP addressestest            *       111111                  *\n\n启动xl2tpd\n# systemctl start xl2tpd# systemctl enable xl2tpd\n\n安装iptables\n# yum install iptables# yum install iptables-services\n\n禁用firewalld\nsystemctl stop firewalldsystemctl mask firewalld\n\n\n设置iptables规则\n查看现有规则\niptables -L -n\n\n允许所有\niptables -P INPUT ACCEPT\n\n清空所有默认规则\niptables -F\n\n清空所有自定义规则\niptables -X\n\n所有计数器清零\niptables -Z\n\n开启地址转换172.16.0.0&#x2F;12\niptables -t nat -A POSTROUTING -s 192.168.1.0/24 -o ens33 -j MASQUERADEiptables -I FORWARD -s 192.168.1.0/24 -j ACCEPTiptables -I FORWARD -d 192.168.1.0/24 -j ACCEPT iptables -A INPUT -p udp -m policy --dir in --pol ipsec -m udp --dport 1701 -j ACCEPTiptables -A INPUT -p udp -m udp --dport 1701 -j ACCEPTiptables -A INPUT -p udp -m udp --dport 500 -j ACCEPTiptables -A INPUT -p udp -m udp --dport 4500 -j ACCEPTiptables -A INPUT -p esp -j ACCEPTiptables -A INPUT -m policy --dir in --pol ipsec -j ACCEPT iptables -A FORWARD -i ppp+ -m state --state NEW,RELATED,ESTABLISHED -j ACCEPTiptables -A FORWARD -m state --state RELATED,ESTABLISHED -j ACCEPT\n\nservice iptables savesystemctl restart iptablessystemctl enable iptables\n\n如果要使用云服务器，阿里云的防火墙需要开通除了基础端口以外的1701, 500, 4500的udp端口。\n","categories":["Tools"]},{"title":"Mac Vmware CentOS7 IP地址一直变化","url":"/2022/06/Mac-Vmware-CentOS7-IP%E5%9C%B0%E5%9D%80%E4%B8%80%E7%9B%B4%E5%8F%98%E5%8C%96/","content":"Mac上装的Vmware Fusion，虚拟机是CentOS 7的系统。\n会经常遇到一个问题，IP地址一直在变化，每次登录到系统里面都比较麻烦，因为要修改IP地址。\n解决办法，增加IP租赁时长。\nsudo vi /Library/Preferences/VMware\\ Fusion/vmnet8/dhcpd.confdefault-lease-time 1800000;                # default is 30 minutesmax-lease-time 7200000;                    # default is 2 hours\n","categories":["OS","CentOS"],"tags":["centos","vmware","ip"]},{"title":"jupyter notebook的环境和conda环境不一致","url":"/2022/06/jupyter-notebook%E7%9A%84%E7%8E%AF%E5%A2%83%E5%92%8Cconda%E7%8E%AF%E5%A2%83%E4%B8%8D%E4%B8%80%E8%87%B4/","content":"在conda py310环境下启动 jupyter lab，发现notebook里面运行的python环境并不是conda的环境。\n解决方案：\n在my-conda-env环境下运行如下语句，安装nb_conda_kernels时，要换一个conda环境。\nconda activate base            # base环境conda install ipykernelconda deactivateconda activate my-conda-env            # this is the environment for your project and codeconda install nb_conda_kernelsjupyter lab\n\n最后再切换回my-conda-env，启动jupyter lab即可。\n\n\n如图如示，在jupyter notebook上面可以选择环境了。\n","categories":["Python"],"tags":["conda","jupyter"]},{"title":"Python Pandas Pivot Table","url":"/2022/06/Python-Pandas-Pivot-Table/","content":"发现这个pivot table和生成的MultiIndex有点类似。\n之前一直有考虑多维的数据如何保存和操作，通过MultiIndex就能解决。\n一堆数据通过pivot table也可以生成MultiIndex的DataFrame，这样分析起数据来就比较方便了。\n\n","categories":["Python"],"tags":["pandas","pivot table"]},{"title":"查看Log日志时的一些Shell常用技巧","url":"/2022/06/%E6%9F%A5%E7%9C%8BLog%E6%97%A5%E5%BF%97%E6%97%B6%E7%9A%84%E4%B8%80%E4%BA%9BShell%E5%B8%B8%E7%94%A8%E6%8A%80%E5%B7%A7/","content":"grep使用grep可以多次管道使用，相当于逻辑 AND 的功能\ngrep -P 使用正则，-o 只显示匹配字符串\n数字串\n\\d+\n\n显示匹配的子串sed &#x27;s/userId: \\(\\.*\\)/\\1/&#x27;\n\n排序sort\n\n统计uniq -c\n\n","categories":["Tools"],"tags":["shell","bash","awk","grep"]},{"title":"Plotly Tick的格式设置","url":"/2022/06/Plotly-Tick%E7%9A%84%E6%A0%BC%E5%BC%8F%E8%AE%BE%E7%BD%AE/","content":"通过Plotly画时间序列的图，细节非常多。\n这篇文章罗列一下关于x轴 Tick的内容设置。\nTickmode - Linear如果tickmode设置的linear，那就需要设置tick0首项和dtick步长。 \nfig.update_layout(    xaxis = dict(        tickmode = &#x27;linear&#x27;,        tick0 = 0.5,        dtick = 0.75    ))\n\n如果是时间序列，dtick为秒数，86400000为1天。\nTickmode - Array如果tickmode设置的array，那就需要设置数组，tickvals和ticktext。\nfig.update_layout(    xaxis = dict(        tickmode = &#x27;array&#x27;,        tickvals = [1, 3, 5, 7, 9, 11],        ticktext = [&#x27;One&#x27;, &#x27;Three&#x27;, &#x27;Five&#x27;, &#x27;Seven&#x27;, &#x27;Nine&#x27;, &#x27;Eleven&#x27;]    ))\n\n关于对日期的设置，update_xaxes(tickformat&#x3D;’%Y-%m-%d’)\n","categories":["Python","Plotly"],"tags":["plotly"]},{"title":"python中的dump, dumps, load和loads","url":"/2022/06/python%E4%B8%AD%E7%9A%84dump-dumps-load%E5%92%8Cloads/","content":"初学这几个函数的时候，有可能搞得比较懵。\ndump和load操作，是对文件流进行操作。\ndumps和loads，是对字符串进行操作，单词后的s，可以理解为string。\na_json = json.load(open(&#x27;demo.json&#x27;,&#x27;r&#x27;))a = json.loads(&#x27;&#123;&#x27;a&#x27;:&#x27;1111&#x27;,&#x27;b&#x27;:&#x27;2222&#x27;&#125;&#x27;)a_dict = &#123;&#x27;a&#x27;:&#x27;1111&#x27;,&#x27;b&#x27;:&#x27;2222&#x27;&#125;json.dump(a_dict, open(&#x27;demo.json&#x27;, &#x27;w&#x27;)a_dict = &#123;&#x27;a&#x27;:&#x27;1111&#x27;,&#x27;b&#x27;:&#x27;2222&#x27;&#125;a_str = json.dumps(a_dict)\n","categories":["Python"],"tags":["dump","dumps","load","loads"]},{"title":"沪深港通数据统计的问题","url":"/2022/06/%E6%B2%AA%E6%B7%B1%E6%B8%AF%E9%80%9A%E6%95%B0%E6%8D%AE%E7%BB%9F%E8%AE%A1%E7%9A%84%E9%97%AE%E9%A2%98/","content":"用某产品提供的数据，统计沪深港通的区间净买入股票数量或区间净买入金额，如果遇到公司送股了，统计的数据就会出现问题。\n如图所示：\n\n\n所以必须要统计出当日真实的买入股票数，再通过当日成交均价，才能算出当日净买入金额。\n"},{"title":"Java繁体中文和简体中文的互相转换","url":"/2022/08/Java%E7%B9%81%E4%BD%93%E4%B8%AD%E6%96%87%E5%92%8C%E7%AE%80%E4%BD%93%E4%B8%AD%E6%96%87%E7%9A%84%E4%BA%92%E7%9B%B8%E8%BD%AC%E6%8D%A2/","content":"判断字符串中是否包含简体、繁体中文，或者简体和繁体之间的转换，可以使用如下API。\nmaven项目，添加如下依赖。\n&lt;dependency&gt;    &lt;groupId&gt;com.github.houbb&lt;/groupId&gt;    &lt;artifactId&gt;opencc4j&lt;/artifactId&gt;    &lt;version&gt;1.7.2&lt;/version&gt;&lt;/dependency&gt;\n\nZhConverterUtil.isTraditional(&quot;中華&quot;);ZhConverterUtil.toSimple(&quot;中華&quot;);\n","categories":["Java"],"tags":["简体","繁体"]},{"title":"沪(深)港通港资持股数据","url":"/2022/06/%E6%B2%AA-%E6%B7%B1-%E6%B8%AF%E9%80%9A%E6%B8%AF%E8%B5%84%E6%8C%81%E8%82%A1%E6%95%B0%E6%8D%AE/","content":"关于统计沪(深)港通数据时，需要注意的几点。\n\n持股数据\n\n如果遇到公司转增股票时，这个持股数据是这样变化的。\n比如先导智能，2021-06-02收盘后，除权转增，10派3转增6。\n2021-06-01系统持股量: 186,258,810.00, 持股数量变动: -913,034.002021-06-02系统持股量: 298,758,880.00, 持股数量变动: 112,500,070.00\n6月2日的系统持股量数据298,758,880.00就是转增后的持股量，它包括两部分数据，其一是6月2日当天港资购买的数量 + 其二6.1日持股数量186,258,810.00转增后的。\n298,758,880.00 &#x3D; (186,258,810 + x) * 1.6x &#x3D; 465490.0，即mfp_sn_inflowamt2在6.2当天的值。这个值是经过调整以后算出来的。\n如果用这个来统计区间求和，如果遇到转增，是会出错的。\n"},{"title":"java8合并map","url":"/2022/08/java8%E5%90%88%E5%B9%B6map/","content":"有个需求是合并两个map，并对相同key的元素的value进行求和。\npublic static void main(String[] args) &#123;        List&lt;String&gt; list = new ArrayList&lt;&gt;();        list.add(&quot;A&quot;);        list.add(&quot;B&quot;);        list.add(&quot;C&quot;);        list.add(&quot;B&quot;);        list.add(&quot;B&quot;);        list.add(&quot;C&quot;);        list.add(&quot;A&quot;);        list.add(&quot;D&quot;);        Map&lt;String, Long&gt; map = list.stream().collect(Collectors.groupingBy(k -&gt; k, Collectors.counting()));        System.out.println(&quot;map: &quot; + map);        List&lt;String&gt; list2 = new ArrayList&lt;&gt;();        list2.add(&quot;B&quot;);        list2.add(&quot;B&quot;);        list2.add(&quot;B&quot;);        list2.add(&quot;B&quot;);        list2.add(&quot;B&quot;);        list2.add(&quot;C&quot;);        list2.add(&quot;A&quot;);        Map&lt;String, Long&gt; map2 = list2.stream().collect(Collectors.groupingBy(k -&gt; k, Collectors.counting()));        System.out.println(&quot;map2: &quot; + map2);        map2.forEach((k, v) -&gt; map.merge(k, v, (v1, v2) -&gt; v1 + v2));        System.out.println(&quot;map: &quot; + map);    &#125;\n\n上面这种方法会修改map的值，还有一种方法不会修改map的值，会新生成一个map。\npublic class MapMergeDemo &#123;    public static void main(String[] args) &#123;        HashMap&lt;String, Integer&gt; m = new HashMap&lt;&gt;();        HashMap&lt;String, Integer&gt; m2 = new HashMap&lt;&gt;();        m.put(&quot;apple&quot;, 2);        m.put(&quot;pear&quot;, 3);        m2.put(&quot;apple&quot;, 9);        m2.put(&quot;banana&quot;, 6);        // method1        Map&lt;String, Long&gt; mapResult = Stream.of(m, m2)                .flatMap(map -&gt; map.entrySet().stream())                .collect(Collectors.groupingBy(Map.Entry::getKey,                        Collectors.summingLong(Map.Entry::getValue)));        System.out.println(mapResult);    &#125;&#125;\n\n假如说有以下List，List的元素存放的数据为：苹果:2, 香蕉:3 类似这样的数据。要统计每类水果的数量。\nMap&lt;String, Integer&gt; fruitCountMap = fruitStrList.stream()                        // 过滤掉空字符串                        .filter(StringUtils::isNotEmpty)                        // 按逗号拆分，并压平                        .flatMap(row -&gt; Arrays.stream(row.split(&quot;,&quot;)))                        // 生成map，相同的Key，累加value                        .collect(Collectors.toMap(                                entry -&gt; entry.split(&quot;:&quot;)[0],                                entry -&gt; Integer.parseInt(entry.split(&quot;:&quot;)[1]),                                Integer::sum)                        );","categories":["Java"],"tags":["java","merge"]},{"title":"详解cron表达式","url":"/2022/08/%E8%AF%A6%E8%A7%A3cron%E8%A1%A8%E8%BE%BE%E5%BC%8F/","content":"cron表达式，一般情况下，六位。\ncron &#x3D; “* * * * * *”\n[秒] [分] [时] [日] [月] [星期几] &#123;年份&#125;\n每个字段允许的值是不一样的\n秒Second 0-59 , - * /分 0-59 , - * /小时 0-23 , - * /日期 1-31 , - * ? / L W C月份 1-12 或者 JAN-DEC , - * /星期 1-7 或者 SUN-SAT , - * ? / L C #年（可选）留空, 1970-2099 , - * /\nSeconds (秒)         ：可以用数字0－59 表示，Minutes(分)          ：可以用数字0－59 表示，Hours(时)             ：可以用数字0-23表示,Day-of-Month(天) ：可以用数字1-31 中的任一一个值，但要注意一些特别的月份Month(月)            ：可以用0-11 或用字符串  “JAN, FEB, MAR, APR, MAY, JUN, JUL, AUG, SEP, OCT, NOV and DEC” 表示Day-of-Week(每周)：可以用数字1-7表示（1 ＝ 星期日）或用字符口串“SUN, MON, TUE, WED, THU, FRI and SAT”表示\n每个符号的意义：\n\n表示所有值；? 表示未说明的值，即不关心它为何值；\n\n\n表示一个指定的范围；, 表示附加一个可能值；&#x2F; 符号前表示开始时间，符号后表示每次递增的值；L(“last”) (“last”) “L” 用在day-of-month字段意思是 “这个月最后一天”；用在 day-of-week字段, 它简单意思是 “7” or “SAT”。 如果在day-of-week字段里和数字联合使用，它的意思就是 “这个月的最后一个星期几” – 例如： “6L” means “这个月的最后一个星期五”. 当我们用“L”时，不指明一个列表值或者范围是很重要的，不然的话，我们会得到一些意想不到的结果。W(“weekday”) 只能用在day-of-month字段。用来描叙最接近指定天的工作日（周一到周五）。例如：在day-of-month字段用“15W”指“最接近这个 月第15天的工作日”，即如果这个月第15天是周六，那么触发器将会在这个月第14天即周五触发；如果这个月第15天是周日，那么触发器将会在这个月第 16天即周一触发；如果这个月第15天是周二，那么就在触发器这天触发。注意一点：这个用法只会在当前月计算值，不会越过当前月。“W”字符仅能在 day-of-month指明一天，不能是一个范围或列表。也可以用“LW”来指定这个月的最后一个工作日。\n\n只能用在day-of-week字段。用来指定这个月的第几个周几。例：在day-of-week字段用”6#3”指这个月第3个周五（6指周五，3指第3个）。如果指定的日期不存在，触发器就不会触发。C 指和calendar联系后计算过的值。例：在day-of-month 字段用“5C”指在这个月第5天或之后包括calendar的第一天；在day-of-week字段用“1C”指在这周日或之后包括calendar的第一天。\n一些cron表达式案例*&#x2F;5 * * * * ? 每隔5秒执行一次0 *&#x2F;1 * * * ? 每隔1分钟执行一次0 0 5-15 * * ? 每天5-15点整点触发0 0&#x2F;3 * * * ? 每三分钟触发一次0 0-5 14 * * ? 在每天下午2点到下午2:05期间的每1分钟触发0 0&#x2F;5 14 * * ? 在每天下午2点到下午2:55期间的每5分钟触发0 0&#x2F;5 14,18 * * ? 在每天下午2点到2:55期间和下午6点到6:55期间的每5分钟触发0 0&#x2F;30 9-17 * * ? 朝九晚五工作时间内每半小时0 0 10,14,16 * * ? 每天上午10点，下午2点，4点 \n0 0 12 ? * WED 表示每个星期三中午12点0 0 17 ? * TUES,THUR,SAT 每周二、四、六下午五点0 10,44 14 ? 3 WED 每年三月的星期三的下午2:10和2:44触发0 15 10 ? * MON-FRI 周一至周五的上午10:15触发0 0 23 L * ? 每月最后一天23点执行一次0 15 10 L * ? 每月最后一日的上午10:15触发0 15 10 ? * 6L 每月的最后一个星期五上午10:15触发0 15 10 * * ? 2005 2005年的每天上午10:15触发0 15 10 ? * 6L 2002-2005 2002年至2005年的每月的最后一个星期五上午10:15触发0 15 10 ? * 6#3 每月的第三个星期五上午10:15触发\n“30 * * * * ?” 每半分钟触发任务“30 10 * * * ?” 每小时的10分30秒触发任务“30 10 1 * * ?” 每天1点10分30秒触发任务“30 10 1 20 * ?” 每月20号1点10分30秒触发任务“30 10 1 20 10 ? *” 每年10月20号1点10分30秒触发任务“30 10 1 20 10 ? 2011” 2011年10月20号1点10分30秒触发任务“30 10 1 ? 10 * 2011” 2011年10月每天1点10分30秒触发任务“30 10 1 ? 10 SUN 2011” 2011年10月每周日1点10分30秒触发任务“15,30,45 * * * * ?” 每15秒，30秒，45秒时触发任务“15-45 * * * * ?” 15到45秒内，每秒都触发任务“15&#x2F;5 * * * * ?” 每分钟的每15秒开始触发，每隔5秒触发一次“15-30&#x2F;5 * * * * ?” 每分钟的15秒到30秒之间开始触发，每隔5秒触发一次“0 0&#x2F;3 * * * ?” 每小时的第0分0秒开始，每三分钟触发一次“0 15 10 ? * MON-FRI” 星期一到星期五的10点15分0秒触发任务“0 15 10 L * ?” 每个月最后一天的10点15分0秒触发任务“0 15 10 LW * ?” 每个月最后一个工作日的10点15分0秒触发任务“0 15 10 ? * 5L” 每个月最后一个星期四的10点15分0秒触发任务“0 15 10 ? * 5#3” 每个月第三周的星期四的10点15分0秒触发任务\n","categories":["Java"],"tags":["cron"]},{"title":"安装sklearn","url":"/2022/11/%E5%AE%89%E8%A3%85sklearn/","content":"Windows使用sklearn，需要安装Numpy+MKL或者Numpy+Vanilla。\n下载地址：\nhttps://www.lfd.uci.edu/~gohlke/pythonlibs/#numpy\n\n查看安装的版本。\npip debug --verbose...Compatible tags: 36  cp310-cp310-win_amd64  cp310-abi3-win_amd64  cp310-none-win_amd64  cp39-abi3-win_amd64  cp38-abi3-win_amd64  cp37-abi3-win_amd64\n\n所以下载版本numpy‑1.22.4+vanilla‑cp310‑cp310‑win_amd64.whl\n安装numpy+vanilla\npip install &quot;numpy-1.22.4+vanilla-cp310-cp310-win_amd64.whl&quot; --user\n\n最后安装scikit-learn\npip install scikit-learn --user","categories":["Python","ML"],"tags":["sklearn"]},{"title":"查看Windows和Linux端口占用情况","url":"/2022/08/%E6%9F%A5%E7%9C%8BWindows%E5%92%8CLinux%E7%AB%AF%E5%8F%A3%E5%8D%A0%E7%94%A8%E6%83%85%E5%86%B5/","content":"Windows查看端口占用情况netstat -ano | findstr &lt;port&gt;\n\n杀掉进程\ntaskkill /pid /f &lt;port&gt;\n\nLinux 查看端口占用情况lsof -i:&lt;port&gt;","categories":["Tools"],"tags":["port"]},{"title":"Java 8 Stream filter过滤Map<String, List<String>>","url":"/2022/12/Java-8-Stream-filter%E8%BF%87%E6%BB%A4Map-String-List-String/","content":"过滤复杂的Map，map的value是一个List，把这个List中不满足要求的元素过滤掉。\n使用java8 stream api应该这样写：\npublic static void main(String[] args) &#123;    Map&lt;String, List&lt;String&gt;&gt; map = new HashMap&lt;&gt;();    List&lt;String&gt; g1 = new ArrayList&lt;&gt;();    g1.add(&quot;M&quot;);    g1.add(&quot;F&quot;);    g1.add(&quot;M&quot;);    map.put(&quot;1&quot;, g1);    map.put(&quot;2&quot;, null);    List&lt;String&gt; g2 = new ArrayList&lt;&gt;();    g2.add(&quot;M&quot;);    g2.add(&quot;M&quot;);    g2.add(&quot;X&quot;);    g2.add(&quot;F&quot;);    g2.add(&quot;Y&quot;);    map.put(&quot;3&quot;, g2);    System.out.println(map);    Map&lt;String, List&lt;String&gt;&gt; newMap = map.entrySet().stream()            .filter(e -&gt; CollectionUtils.isNotEmpty(e.getValue()))            .collect(Collectors.toMap(                    Map.Entry::getKey,                    e -&gt; e.getValue().stream()                            .filter(i -&gt; &quot;M&quot;.equals(i))                            .collect(Collectors.toList())            ));    System.out.println(newMap);&#125;\n\n结果：\n&#123;1=[M, F, M], 2=null, 3=[M, M, X, F, Y]&#125;&#123;1=[M, M], 3=[M, M]&#125;","categories":["Java"],"tags":["java 8","stream"]},{"title":"解决github 22端口被占用或封闭的问题","url":"/2025/08/%E8%A7%A3%E5%86%B3github-22%E7%AB%AF%E5%8F%A3%E8%A2%AB%E5%8D%A0%E7%94%A8%E6%88%96%E5%B0%81%E9%97%AD%E7%9A%84%E9%97%AE%E9%A2%98/","content":"在使用 git push 或 Github Desktop 等其他 Git 操作时，可能遇到以下错误：\nssh: connect to host github.com port 22: Connection timed outfatal: Could not read from remote repository.Please make sure you have the correct access rights and the repository exists.\n\n这是由于默认的 SSH 22 端口被防火墙或网络策略限制，导致无法连接到 GitHub 的服务器。\n为了避免这一问题，可以将连接改为 SSH 的 443 端口。以下是详细的解决方法，包括 Windows 和 Linux&#x2F;Mac 的操作步骤。\nLinux&#x2F;Mac 下操作步骤\n\n修改 SSH 配置文件SSH 的配置文件通常位于 ~&#x2F;.ssh&#x2F;config，如果文件不存在，可以手动创建一个：\n\ntouch ~/.ssh/config\n打开文件并添加以下内容：\nHost github.comHostName ssh.github.com  # **这是最重要的部分**User gitPort 443PreferredAuthentications publickeyIdentityFile ~/.ssh/id_rsa\n\n\n验证 SSH 配置配置完成后，通过以下命令测试连接是否正常：\n\nssh -T git@github.com\n\n如果配置成功，应该看到类似以下输出：\nHi &lt;your-username&gt;! You&#x27;ve successfully authenticated, but GitHub does not provide shell access.\n\n\n配置 Git 使用新端口为确保 Git 使用新的 443 端口，可以运行以下命令：\n\ngit config --global url.&quot;ssh://git@ssh.github.com:443&quot;.insteadOf &quot;ssh://git@github.com&quot;\n\nWindows 下操作步骤\n\n找到 SSH 配置文件在 Windows 下，SSH 配置文件通常位于用户目录的 .ssh 文件夹中（例如：C:\\Users&lt;你的用户名&gt;.ssh\\config）。如果文件不存在，可以手动创建一个：\n\n打开资源管理器并导航到 C:\\Users&lt;你的用户名&gt;.ssh。在 .ssh 文件夹下，新建一个文件，命名为 config（没有扩展名）。2. 编辑 SSH 配置文件   用记事本或其他文本编辑器打开 config 文件，添加以下内容：\nHost github.comHostName ssh.github.com  # **这是最重要的部分**User gitPort 443PreferredAuthentications publickeyIdentityFile C:\\Users\\&lt;你的用户名&gt;\\.ssh\\id_rsa\n\n注意:IdentityFile 的路径需要根据你实际存储 SSH 密钥的位置调整，通常是 id_rsa 或 id_ed25519。\n\n验证 SSH 配置打开命令提示符或 PowerShell，运行以下命令测试连接：\n\nssh -T git@github.com\n\n如果配置正确，你应该看到以下输出：\nHi &lt;your-username&gt;! You&#x27;ve successfully authenticated, but GitHub does not provide shell access.\n\n\n配置 Git 使用新端口在命令提示符或 PowerShell 中运行以下命令：\n\n最近这一步不配置，也可以正常使用 Github Desktop 了。\ngit config --global url.&quot;ssh://git@ssh.github.com:443&quot;.insteadOf &quot;ssh://git@github.com&quot;\n\n总结当 22 端口被占用或限制 时，通过将 SSH 连接切换到 443 端口，即可解决无法访问 GitHub 的问题。这种方法适用于任何操作系统，尤其是在防火墙限制较严的网络环境中。\n"},{"title":"Java过滤utf8mb4字符串","url":"/2022/07/Java%E8%BF%87%E6%BB%A4utf8mb4%E5%AD%97%E7%AC%A6%E4%B8%B2/","content":"两个数据库迁移，有一个字符集用的utf8mb4，放入到utf8里面，就会报错。\n通过如下方法，可以把utf8mb4的字符串全部过滤掉。\nString source = &quot;your input string&quot;;String target = source.replaceAll(&quot;[^\\\\u0000-\\\\uD7FF\\\\uE000-\\\\uFFFF]&quot;, &quot;&quot;);","categories":["Java"],"tags":["utf8mb4"]},{"title":"Jupyter lab获取当前文件名及路径","url":"/2022/07/Jupyter-lab%E8%8E%B7%E5%8F%96%E5%BD%93%E5%89%8D%E6%96%87%E4%BB%B6%E5%90%8D%E5%8F%8A%E8%B7%AF%E5%BE%84/","content":"在 jupyter notebook &#x2F; lab 里面，使用 file 获取不了文件名。\n可以通过如下方法获取当前文件名和路径。\n安装 ipynbname\npip install ipynbname\n\nipynbname.name()ipynbname.path()\n","categories":["Python"],"tags":["jupyter lab"]},{"title":"Mac Vmware Fusion安装windows虚拟机黑屏","url":"/2022/07/Mac-Vmware-Fusion%E5%AE%89%E8%A3%85windows%E8%99%9A%E6%8B%9F%E6%9C%BA%E9%BB%91%E5%B1%8F/","content":"Mac OS 安装了 Vmware Fusion，在里面安装了 Windows 10 虚拟机，这个虚拟机，有时候有时候黑屏，看不到任何界面。\n解决办法：\n\n重启mac，并在启动时一直按着 Command + R，进入恢复模式。\n\n屏幕顶上，选择实用工具 -&gt; 终端。\n\n执行命令关闭SIP\n\n\ncsrutil disable\n\n\n重启电脑，进入 mac os 系统后，执行如下命令。\n\ntccutil reset All com.vmware.fusion sudo sqlite3 &quot;/Library/Application Support/com.apple.TCC/TCC.db&quot; &#x27;insert into access values (&quot;kTCCServiceScreenCapture&quot;, &quot;com.vmware.fusion&quot;, 0, 1, 1, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;UNUSED&quot;, &quot;&quot;, 0,1565595574)&#x27; sudo sqlite3 &quot;/Library/Application Support/com.apple.TCC/TCC.db&quot; &#x27;insert into access values (&quot;kTCCServiceListenEvent&quot;, &quot;com.vmware.fusion&quot;, 0, 1, 1, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;UNUSED&quot;, &quot;&quot;, 0,1565595574)&#x27; sudo sqlite3 &quot;/Library/Application Support/com.apple.TCC/TCC.db&quot; &#x27;insert into access values (&quot;kTCCServicePostEvent&quot;, &quot;com.vmware.fusion&quot;, 0, 1, 1, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;UNUSED&quot;, &quot;&quot;, 0,1565595574)&#x27;\n\n\n再次重启电脑，并进入 Command + R，进入恢复模式。\n\n并开启SIP\ncsrutil enable\n\n\n最后重启电脑\n\n进入以后，关掉 Vmware Fusion 软件。\n进入安全性和隐私，分别在项目辅助功能、输入监视、屏幕录制中，把 Vmware Fusion 复选框选中。\n再次启动 Vmware Fusion 中的 Windows 虚拟机，屏幕不会黑屏了。\n","categories":["Tools"],"tags":["vmware"]},{"title":"MacOS Vmware Fusion安装虚拟机CPU占用率高发烫","url":"/2022/07/MacOS-Vmware-Fusion%E5%AE%89%E8%A3%85%E8%99%9A%E6%8B%9F%E6%9C%BACPU%E5%8D%A0%E7%94%A8%E7%8E%87%E9%AB%98%E5%8F%91%E7%83%AB/","content":"mac系统基于vmware安装虚拟机，然后主机CPU占用率飙升，风扇狂转，发烫的现象，好几年都没有解决。\n网上很多人说限制CPU的功率，减少虚拟机CPU个数，这些都没有办法。\n目前发现一个方法，效率好了很多。就是关掉虚拟机3D图像展示，以OpenGL来替代。\n在宿主机系统中，编辑虚拟机.vmx文件，在文件末尾添加如下内容。\nmks.enableMTLRenderer = &quot;FALSE&quot;mks.enableGLRenderer = &quot;TRUE&quot;\n\n保存后重启虚拟机，CPU占用率的确变小了，也没有那么烫了。\n","categories":["Tools"],"tags":["vmware","cpu"]},{"title":"Ubuntu上安装和使用Miniconda","url":"/2022/07/Ubuntu%E4%B8%8A%E5%AE%89%E8%A3%85%E5%92%8C%E4%BD%BF%E7%94%A8Miniconda/","content":"安装miniconda下载\n$ wget https://repo.anaconda.com/miniconda/Miniconda3-py39_4.9.2-Linux-x86_64.sh\n\n$ sudo chmod +x Miniconda3-py39_4.9.2-Linux-x86_64.sh\n\n安装\n$ sudo ./Miniconda3-py39_4.9.2-Linux-x86_64.sh\n\n选择一个安装位置，比如/usr/local/miniconda3\n选择yes，即可安装成功。\n软件自动修改了~/.bashrc文件，把miniconda3的添加到PATH中了。\n关闭窗口，再次打开时，命令行就进入默认的base环境了。\n创建环境旧的环境如果要删除\n$ conda env remove -n py310\n\n指定环境位置可以用prefix参数，此时，name参数就不可用了，路径最后面的字符串即为环境名。\n$ conda create --prefix=~/development/python/envs/py310 python=3.10\n\n","categories":["Python"],"tags":["miniconda"]},{"title":"Ubuntu给用户赋予root权限","url":"/2022/07/Ubuntu%E7%BB%99%E7%94%A8%E6%88%B7%E8%B5%8B%E4%BA%88root%E6%9D%83%E9%99%90/","content":"给一个新用户添加一个附加组。\nusermod -aG group user\n\na 是追加G 附加组\n移除这个附加组，相当于复原。\ngpasswd -d user group\n\n或者是编辑suoders文件\nsudo visudo\n\n模仿root那行，再添加一行内容\nroot    ALL=(ALL:ALL) ALLuser ALL=(ALL:ALL) ALL\n\n这个时候user就能获取到root权限了。\n但是每次使用命令时都得加前缀sudo。\n可以通过命令sudo -s进入到root的shell，s代表shell。\n网上很多说修改/etc/passwd，把user的uid和gid修改为0，和root一样，这样会出很多问题的，强烈不建议这样。\n","categories":["Tools"]},{"title":"Windows10和Mac设置Miniconda启动环境","url":"/2022/07/Windows10%E5%92%8CMac%E8%AE%BE%E7%BD%AEMiniconda%E5%90%AF%E5%8A%A8%E7%8E%AF%E5%A2%83/","content":"Windows10启动Miniconda Prompt命令提示行时，默认会进入到默认的base环境中。\n要切换到用户使用的环境，都需要人工去切换一下，比较麻烦。\n按照如下方法，设置成用户想要的环境。\nconda env list# conda environments:#base                  *  C:\\ProgramData\\Miniconda3py310                    C:\\ProgramData\\Miniconda3\\envs\\py310\n\n然后右击Miniconda Prompt，进入到如下图片所示\n\n\n在目标位置，把后面的参数改为用户想设置的环境的具体位置C:\\ProgramData\\Miniconda3\\envs\\py310。\nMac如果是mac系统，则是编译bash_profile配置文件。\n$ vi ~/.bash_profile\n\n移动到最下面，添加如下内容：\ncd ~/Development/python/conda activate py310\n\nUbuntu$ vi ~/.bashrc\n\n内容修改为：\nconda activate py310\n","categories":["Python"],"tags":["conda","miniconda"]},{"title":"java根据对象属性排序","url":"/2022/07/java%E6%A0%B9%E6%8D%AE%E5%AF%B9%E8%B1%A1%E5%B1%9E%E6%80%A7%E6%8E%92%E5%BA%8F/","content":"Collections.sort() Comparable对象public class User implements Comparable&lt;User&gt; &#123;    &#125;\n\n排序\nCollections.sort(users);Collections.reverse(users);\n\nCollections.sort() 传入Comparator比较器Collections.sort(users, new Comparator&lt;User&gt;() &#123;  @Override  public int compare(User u1, User u2) &#123;    return u1.getCreatedOn().compareTo(u2.getCreatedOn());  &#125;&#125;);\n\nList接口的sortusers.sort(Comparator.comparing(User::getCreatedOn));users.sort(Comparator.comparing(User::getCreatedOn).reversed());\n\nStream接口的sorted [Java8]List&lt;User&gt; sortedUsers = users.stream()  .sorted(Comparator.comparing(User::getCreatedOn))  .collect(Collectors.toList());List&lt;User&gt; sortedUsers = users.stream()  .sorted(Comparator.comparing(User::getCreatedOn).reversed())  .collect(Collectors.toList());\n\n","categories":["Java"],"tags":["sort"]},{"title":"Vmware主机和虚拟机之间拖拽复制文件","url":"/2022/07/Vmware%E4%B8%BB%E6%9C%BA%E5%92%8C%E8%99%9A%E6%8B%9F%E6%9C%BA%E4%B9%8B%E9%97%B4%E6%8B%96%E6%8B%BD%E5%A4%8D%E5%88%B6%E6%96%87%E4%BB%B6/","content":"主机Windows10，虚拟机装的是Ubuntu。\n在Vmware上面设置了文件共享后，在Ubuntu上面看不到。如何看到，网上可以查查。\n我主要是想解决从Windows拖拽文件到Ubuntu虚拟机里面。\n执行下列命令就可以了。\nsudo apt-get install open-vm-toolssudo apt-get install open-vm-tools-desktop\n","categories":["Tools"],"tags":["vmware"]},{"title":"Jupyter lab外部访问配置","url":"/2025/01/Jupyter-lab%E5%A4%96%E9%83%A8%E8%AE%BF%E9%97%AE%E9%85%8D%E7%BD%AE/","content":"设置外部可以访问，即非本地访问。\n设置启动时不默认打开浏览器。\n设置jupyter打开哪个文件夹。\njupyter lab --no-browser --ip &quot;*&quot; \\            --notebook-dir /burg/glab/users/luke","categories":["Python"],"tags":["jupyter lab"]},{"title":"Python Pandas数据选择df[], df.loc[], df.iloc区别","url":"/2025/01/Python-Pandas%E6%95%B0%E6%8D%AE%E9%80%89%E6%8B%A9df-df-loc-df-iloc%E5%8C%BA%E5%88%AB/","content":"刚入门 pandas 的同学，对 DataFrame 也用了很久，但对 df[], df.loc[] 和 df.iloc[] 的认识总是不够清晰，什么时候该用哪个，不能很好很简便的进行选择。\n他们的区别主要在于：\ndf[] 是对单行或单列进行选取，也就是说只能单维度的选取，设置筛选条件时，也只能是对一个维度设置筛选条件。\ndf.loc[] 和 df.iloc[] 则是对区域的选择，可以对两个维度设置筛选条件。\n我们先创建一个 DataFrame。\ndf[] 行或列单维选取","categories":["Python","Pandas"],"tags":["python","pandas"]},{"title":"Windows安装Qlib","url":"/2025/01/Windows%E5%AE%89%E8%A3%85Qlib/","content":"安装异常\nFailed to build pyqlibERROR: Could not build wheels for pyqlib, which is required to install pyproject.toml-based projects\n\nC:\\Users\\simon\\AppData\\Local\\Temp\\pip-build-env-uuivrj87\\overlay\\Lib\\site-packages\\Cython\\Compiler\\Main.py:381: FutureWarning: Cython directive &#x27;language_level&#x27; not set, using &#x27;3str&#x27; for now (Py3). This has changed from earlier releases! File: D:\\development\\python\\qlib\\qlib\\data\\_libs\\rolling.pyx       tree = Parsing.p_module(s, pxd, full_module_name)     error: Microsoft Visual C++ 14.0 or greater is required. Get it with &quot;Microsoft C++ Build Tools&quot;: https://visualstudio.microsoft.com/visual-cpp-build-tools/     [end of output] note: This error originates from a subprocess, and is likely not a problem with pip. ERROR: Failed building wheel for pyqlib\n\n需要安装”Microsoft C++ Build Tools”。\n需要更新安装 cython\npip install cython --upgrade","tags":["vnpy","qlib"]},{"title":"java单元测试jacoco插件","url":"/2025/01/java%E5%8D%95%E5%85%83%E6%B5%8B%E8%AF%95jacoco%E6%8F%92%E4%BB%B6/","content":"java springboot 项目，配置单元测试，并要求到达一定的代码覆盖率。\n在 maven pom.xml 文件中，需要注意以下几点。\n跳过测试阶段改为false。\n&lt;properties&gt;    &lt;maven.test.skip&gt;false&lt;/maven.test.skip&gt;&lt;/properties&gt;\n\nbuild 阶段，skip 要改为 false。\n如果只要单元测试指定类，就在 include 标签里面指定。\n&lt;build&gt;    &lt;plugins&gt;        &lt;plugin&gt;            &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;            &lt;artifactId&gt;maven-surefire-plugin&lt;/artifactId&gt;            &lt;version&gt;2.22.0&lt;/version&gt;            &lt;configuration&gt;                &lt;skip&gt;false&lt;/skip&gt;                &lt;includes&gt;                    &lt;include&gt;**/XXXTest&lt;/include&gt; &lt;!-- 替换为您的测试类路径 --&gt;                &lt;/includes&gt;            &lt;/configuration&gt;        &lt;/plugin&gt;    &lt;/plugins&gt;&lt;/build&gt;\n\n配置单元测试依赖，mockito-core 如果配置成4版本会有其他问题。\n&lt;!-- 代码覆盖率 --&gt;&lt;dependency&gt;    &lt;groupId&gt;org.jacoco&lt;/groupId&gt;    &lt;artifactId&gt;jacoco-maven-plugin&lt;/artifactId&gt;    &lt;version&gt;0.8.12&lt;/version&gt;    &lt;scope&gt;test&lt;/scope&gt;&lt;/dependency&gt;&lt;dependency&gt;    &lt;groupId&gt;junit&lt;/groupId&gt;    &lt;artifactId&gt;junit&lt;/artifactId&gt;    &lt;version&gt;4.13.2&lt;/version&gt;    &lt;scope&gt;test&lt;/scope&gt;&lt;/dependency&gt;&lt;dependency&gt;    &lt;groupId&gt;org.mockito&lt;/groupId&gt;    &lt;artifactId&gt;mockito-core&lt;/artifactId&gt;    &lt;version&gt;3.12.4&lt;/version&gt;    &lt;scope&gt;test&lt;/scope&gt;&lt;/dependency&gt;\n\n&lt;build&gt;    &lt;pluginManagement&gt;        &lt;plugins&gt;            &lt;plugin&gt;                &lt;groupId&gt;org.jacoco&lt;/groupId&gt;                &lt;artifactId&gt;jacoco-maven-plugin&lt;/artifactId&gt;                &lt;version&gt;0.8.12&lt;/version&gt;            &lt;/plugin&gt;        &lt;/plugins&gt;    &lt;/pluginManagement&gt;    &lt;plugins&gt;        &lt;!-- jacoco --&gt;        &lt;plugin&gt;            &lt;groupId&gt;org.jacoco&lt;/groupId&gt;            &lt;artifactId&gt;jacoco-maven-plugin&lt;/artifactId&gt;            &lt;executions&gt;                &lt;execution&gt;                    &lt;goals&gt;                        &lt;goal&gt;prepare-agent&lt;/goal&gt;                    &lt;/goals&gt;                &lt;/execution&gt;                &lt;execution&gt;                    &lt;id&gt;report&lt;/id&gt;                    &lt;phase&gt;test&lt;/phase&gt;                    &lt;goals&gt;                        &lt;goal&gt;report&lt;/goal&gt;                    &lt;/goals&gt;                &lt;/execution&gt;            &lt;/executions&gt;        &lt;/plugin&gt;    &lt;/plugins&gt;&lt;/build&gt;\n\n配置结束后，点击 Lifecycle 下面的 test，通过html文件 \\target\\site\\jacoco\\index.html 就可以看到覆盖率的页面了\n","tags":["java","jacoco"]},{"title":"conda相关的坑","url":"/2025/01/conda%E7%9B%B8%E5%85%B3%E7%9A%84%E5%9D%91/","content":"查看 conda 的版本：conda -V\nconda 4.x 版本，提示要升级到 23.x 版本，使用 conda update conda 失效，提示很多包冲突。\n使用 conda list -r，然后回退到前面的版本。conda install --revision 0，也不行。\n遇到这种情况，是conda的版本太低了，直播删除软件，重新安装 anaconda 就行了。\n使用 conda env list 出现一些没有名字的环境，直播删除文件夹，如果还不行，修改用户目录下面的 .conda\\environments.txt 文件，把不要环境的内容删除就行了。(windows环境)\n环境创建好以后，在这个环境里面会默认安装 pip 的，通过 pip -V 可以查看版本及安装路径信息。\n","tags":["conda"]},{"title":"facebook audioseal踩坑指南","url":"/2025/01/facebook-audioseal%E8%B8%A9%E5%9D%91%E6%8C%87%E5%8D%97/","content":"mamba install 需要指定渠道 -c pytorch\n在 jupyter lab 中加载 pytorch 相关包时，控制台报错：\nOMP: Error #15: Initializing libiomp5md.dll, but found libiomp5md.dll already initialized.OMP: Hint This means that multiple copies of the OpenMP runtime have been linked into the program. That is dangerous, since it can degrade performance or cause incorrect results. The best thing to do is to ensure that only a single OpenMP runtime is linked into the process, e.g. by avoiding static linking of the OpenMP runtime in any library. As an unsafe, unsupported, undocumented workaround you can set the environment variable KMP_DUPLICATE_LIB_OK=TRUE to allow the program to continue to execute, but that may cause crashes or silently produce incorrect results. For more information, please see http://www.intel.com/software/products/support/.\n\n在虚拟环境下面的 \\Library\\bin 下面，找到 libiomp5md.dll 文件，改个名字就可以了。\n"},{"title":"python进阶","url":"/2025/01/python%E8%BF%9B%E9%98%B6/","content":"可以使用 for 循环的，就叫做可迭代对象，叫Iterable。\n使用next()的，叫Iterator。\n生成器，只是在next()时，再生成数据，不过早的占用内存。\n装饰器的实现方案。\n输入函数，返回函数。\n不带参数的，带参数的，带返回值的，在装饰器上传入参数的。\n","categories":["Python"],"tags":["python"]},{"title":"通过python plotly画图","url":"/2025/01/%E9%80%9A%E8%BF%87python-plotly%E7%94%BB%E5%9B%BE/","content":"之前一直是通过matplotlib来画图的，数据量大了以后，画图的速度较慢。\n如果使用plotly，效率较高，主要是通过浏览器来渲染图片的，通过js效果，还可以随意放大缩小查看细节。\n基本绘图折线图import plotly.express as pxfig1 = px.line(df[&quot;balance&quot;])fig1.show()\n\n或者更简便一些\nimport plotly.express as pxpx.line(df[&quot;balance&quot;])\n\n柱状图px.bar(df[&quot;pnl&quot;])\n\n散点图px.scatter(df[&quot;drawdown&quot;])\n\n对象方式绘图\n\n创建绘图区域import plotly.graph_objects as gofrom plotly.subplots import make_subplots# 创建绘图区域, 4行1列fig = make_subplots(    rows=4,    cols=1,    subplot_titles=[&quot;累计盈亏&quot;, &quot;净值回撤&quot;, &quot;交易盈亏&quot;, &quot;盈亏分布&quot;],    vertical_spacing=0.06)\n\n创建四幅子图Scatter取代了Line\n# 绘制资金曲线balance_line = go.Scatter(    x=df.index,    y=df[&quot;balance&quot;],    mode=&quot;lines&quot;,    name=&quot;累计盈亏&quot;)highlevel_scatter = go.Scatter(x=df.index, y=df[&quot;highlevel&quot;], name=&quot;高水位&quot;)\n\n这个有填充的效果\n# 绘制回撤区域drawdown_scatter = go.Scatter(    x=df.index,    y=df[&quot;drawdown&quot;],    fillcolor=&quot;red&quot;,    fill=&#x27;tozeroy&#x27;,    mode=&quot;lines&quot;,    name=&quot;回撤&quot;)\n\n# 绘制交易盈亏pnl_bar = go.Bar(y=df[&quot;pnl&quot;], name=&quot;交易盈亏&quot;)\n\n# 绘制盈亏分布pnl_histogram = go.Histogram(x=df[&quot;pnl&quot;], nbinsx=100, name=&quot;盈亏分布&quot;)\n\n把子图添加到画布上面\n# 绘制图表fig.add_trace(balance_line, row=1, col=1)fig.add_trace(highlevel_scatter, row=1, col=1)fig.add_trace(drawdown_scatter, row=2, col=1)fig.add_trace(pnl_bar, row=3, col=1)fig.add_trace(pnl_histogram, row=4, col=1)fig.update_layout(height=1000, width=1000)fig.show() # 可以省略\n","categories":["Python","Plotly"],"tags":["plotly"]}]